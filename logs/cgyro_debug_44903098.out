[build_datasets] windows: total=815  train=489  val=163  test=163

[train] dataset:
  y_mean: [[[125.73204 192.64935  74.71944]]]
  y_std : [[[68.6831  91.0152  41.72855]]]
  sample Y: min=-2.033e+00, max=1.878e+00, mean=-4.217e-01, std=1.053e+00

[val] dataset:
  y_mean: [[[125.73204 192.64935  74.71944]]]
  y_std : [[[68.6831  91.0152  41.72855]]]
  sample Y: min=-1.667e+00, max=3.134e+00, mean=2.253e-01, std=1.508e+00

[test] dataset:
  y_mean: [[[125.73204 192.64935  74.71944]]]
  y_std : [[[68.6831  91.0152  41.72855]]]
  sample Y: min=-1.564e+00, max=3.575e+00, mean=2.695e-01, std=1.442e+00
âœ… Saved flux histograms in ./mnt/data/myrun_logs_deep_debug_ordered_t
ðŸŸ¦ Starting epoch 1/20 (train steps â‰ˆ 62)
[1561120] Î¦2FluxDeep forward: input (8, 32, 2, 324, 1, 16)
  ... step 0/62  loss=1.1317  (2.3s since last print)
  â†˜ grad L2 norm â‰ˆ 1.228e+01 (batch 0)
  ... step 2/62  loss=1.0835  (0.8s since last print)
  â†˜ grad L2 norm â‰ˆ 1.777e+01 (batch 2)
  ... step 4/62  loss=1.1743  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 1.468e+01 (batch 4)
  ... step 6/62  loss=0.5762  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 8.602e+00 (batch 6)
  ... step 8/62  loss=0.8417  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 1.200e+01 (batch 8)
  ... step 10/62  loss=0.8215  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 1.176e+01 (batch 10)
  ... step 12/62  loss=1.0282  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 1.068e+01 (batch 12)
  ... step 14/62  loss=0.5361  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 7.835e+00 (batch 14)
  ... step 16/62  loss=0.9798  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 1.366e+01 (batch 16)
  ... step 18/62  loss=1.3951  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 1.450e+01 (batch 18)
  ... step 20/62  loss=1.1885  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 1.382e+01 (batch 20)
  ... step 22/62  loss=0.9587  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 1.030e+01 (batch 22)
  ... step 24/62  loss=0.8955  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 8.915e+00 (batch 24)
  ... step 26/62  loss=1.1413  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 1.069e+01 (batch 26)
  ... step 28/62  loss=1.3519  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 1.293e+01 (batch 28)
  ... step 30/62  loss=0.6325  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 8.459e+00 (batch 30)
  ... step 32/62  loss=1.1002  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 1.062e+01 (batch 32)
  ... step 34/62  loss=1.1322  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 1.106e+01 (batch 34)
  ... step 36/62  loss=1.3685  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 1.173e+01 (batch 36)
  ... step 38/62  loss=0.5846  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 9.005e+00 (batch 38)
  ... step 40/62  loss=0.9453  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 1.033e+01 (batch 40)
  ... step 42/62  loss=1.2140  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 1.124e+01 (batch 42)
  ... step 44/62  loss=1.0480  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 9.309e+00 (batch 44)
  ... step 46/62  loss=1.2578  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 1.065e+01 (batch 46)
  ... step 48/62  loss=0.4639  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 7.146e+00 (batch 48)
  ... step 50/62  loss=0.6190  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 7.673e+00 (batch 50)
  ... step 52/62  loss=0.6804  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 7.953e+00 (batch 52)
  ... step 54/62  loss=0.6198  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 8.409e+00 (batch 54)
  ... step 56/62  loss=0.9635  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 9.192e+00 (batch 56)
  ... step 58/62  loss=0.7137  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 9.393e+00 (batch 58)
  ... step 60/62  loss=1.0336  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 1.009e+01 (batch 60)
âœ… epoch 1 forward/backward done in 36.9s
  ðŸ”Ž val step 0: batch (8, 32, 2, 324, 1, 16) loss=7.1481
[01] train=0.9504  val=3.1870  RMSE(std)=[Qi:1.789, Qe:1.744, Î“:1.822]  RMSE(phys)=[Qi:122.864, Qe:158.698, Î“:76.042]  (2.3s)
âœ… Saved unified NN â†’ ./mnt/data/bin.cgyro.nn
ðŸŸ¦ Starting epoch 2/20 (train steps â‰ˆ 62)
  ... step 0/62  loss=0.8262  (0.1s since last print)
  â†˜ grad L2 norm â‰ˆ 8.289e+00 (batch 0)
  ... step 2/62  loss=1.2007  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 1.314e+01 (batch 2)
  ... step 4/62  loss=0.6247  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 8.633e+00 (batch 4)
  ... step 6/62  loss=0.8129  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 1.141e+01 (batch 6)
  ... step 8/62  loss=0.6789  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 7.500e+00 (batch 8)
  ... step 10/62  loss=0.4974  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 7.396e+00 (batch 10)
  ... step 12/62  loss=0.8133  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 9.419e+00 (batch 12)
  ... step 14/62  loss=0.7089  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 1.291e+01 (batch 14)
  ... step 16/62  loss=1.2704  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 1.201e+01 (batch 16)
  ... step 18/62  loss=0.8530  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 9.007e+00 (batch 18)
  ... step 20/62  loss=0.7881  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 1.028e+01 (batch 20)
  ... step 22/62  loss=0.2686  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 6.336e+00 (batch 22)
  ... step 24/62  loss=0.6700  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 9.310e+00 (batch 24)
  ... step 26/62  loss=0.4221  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 6.597e+00 (batch 26)
  ... step 28/62  loss=0.6232  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 8.611e+00 (batch 28)
  ... step 30/62  loss=0.3669  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 5.710e+00 (batch 30)
  ... step 32/62  loss=0.3095  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 9.286e+00 (batch 32)
  ... step 34/62  loss=0.8964  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 1.529e+01 (batch 34)
  ... step 36/62  loss=0.6620  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 9.017e+00 (batch 36)
  ... step 38/62  loss=0.7280  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 1.299e+01 (batch 38)
  ... step 40/62  loss=0.4014  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 9.388e+00 (batch 40)
  ... step 42/62  loss=0.7317  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 1.072e+01 (batch 42)
  ... step 44/62  loss=0.4961  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 1.269e+01 (batch 44)
  ... step 46/62  loss=0.5685  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 1.094e+01 (batch 46)
  ... step 48/62  loss=0.4537  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 7.848e+00 (batch 48)
  ... step 50/62  loss=0.4627  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 8.753e+00 (batch 50)
  ... step 52/62  loss=0.4626  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 7.572e+00 (batch 52)
  ... step 54/62  loss=0.7377  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 1.048e+01 (batch 54)
  ... step 56/62  loss=0.6788  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 8.674e+00 (batch 56)
  ... step 58/62  loss=0.7031  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 1.269e+01 (batch 58)
  ... step 60/62  loss=0.5802  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 9.108e+00 (batch 60)
âœ… epoch 2 forward/backward done in 35.0s
  ðŸ”Ž val step 0: batch (8, 32, 2, 324, 1, 16) loss=1.0526
[02] train=0.6933  val=3.8808  RMSE(std)=[Qi:2.003, Qe:1.990, Î“:1.916]  RMSE(phys)=[Qi:137.571, Qe:181.109, Î“:79.950]  (2.3s)
âœ… Saved unified NN â†’ ./mnt/data/bin.cgyro.nn
ðŸŸ¦ Starting epoch 3/20 (train steps â‰ˆ 62)
  ... step 0/62  loss=0.9633  (0.1s since last print)
  â†˜ grad L2 norm â‰ˆ 1.438e+01 (batch 0)
  ... step 2/62  loss=0.7421  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 7.788e+00 (batch 2)
  ... step 4/62  loss=0.2487  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 5.472e+00 (batch 4)
  ... step 6/62  loss=0.4546  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 9.779e+00 (batch 6)
  ... step 8/62  loss=0.6284  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 8.109e+00 (batch 8)
  ... step 10/62  loss=0.5906  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 7.185e+00 (batch 10)
  ... step 12/62  loss=0.4458  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 8.140e+00 (batch 12)
  ... step 14/62  loss=0.5495  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 1.169e+01 (batch 14)
  ... step 16/62  loss=0.7055  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 1.039e+01 (batch 16)
  ... step 18/62  loss=0.6398  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 8.094e+00 (batch 18)
  ... step 20/62  loss=0.3527  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 1.008e+01 (batch 20)
  ... step 22/62  loss=0.8060  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 1.709e+01 (batch 22)
  ... step 24/62  loss=0.2923  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 8.004e+00 (batch 24)
  ... step 26/62  loss=0.8251  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 9.071e+00 (batch 26)
  ... step 28/62  loss=0.5918  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 9.875e+00 (batch 28)
  ... step 30/62  loss=0.7962  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 1.183e+01 (batch 30)
  ... step 32/62  loss=0.6579  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 9.022e+00 (batch 32)
  ... step 34/62  loss=0.9245  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 7.980e+00 (batch 34)
  ... step 36/62  loss=0.6556  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 1.119e+01 (batch 36)
  ... step 38/62  loss=0.5242  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 8.016e+00 (batch 38)
  ... step 40/62  loss=0.2415  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 6.912e+00 (batch 40)
  ... step 42/62  loss=0.5994  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 1.216e+01 (batch 42)
  ... step 44/62  loss=0.3520  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 7.295e+00 (batch 44)
  ... step 46/62  loss=0.9484  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 1.067e+01 (batch 46)
  ... step 48/62  loss=0.6475  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 9.491e+00 (batch 48)
  ... step 50/62  loss=0.5214  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 1.113e+01 (batch 50)
  ... step 52/62  loss=0.4298  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 7.595e+00 (batch 52)
  ... step 54/62  loss=0.4182  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 8.892e+00 (batch 54)
  ... step 56/62  loss=0.9287  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 1.070e+01 (batch 56)
  ... step 58/62  loss=0.9209  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 1.569e+01 (batch 58)
  ... step 60/62  loss=0.5472  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 7.112e+00 (batch 60)
âœ… epoch 3 forward/backward done in 35.0s
  ðŸ”Ž val step 0: batch (8, 32, 2, 324, 1, 16) loss=1.1366
[03] train=0.6005  val=5.6504  RMSE(std)=[Qi:2.407, Qe:2.369, Î“:2.355]  RMSE(phys)=[Qi:165.327, Qe:215.627, Î“:98.257]  (2.3s)
âœ… Saved unified NN â†’ ./mnt/data/bin.cgyro.nn
ðŸŸ¦ Starting epoch 4/20 (train steps â‰ˆ 62)
  ... step 0/62  loss=1.3501  (0.1s since last print)
  â†˜ grad L2 norm â‰ˆ 1.614e+01 (batch 0)
  ... step 2/62  loss=1.0255  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 1.597e+01 (batch 2)
  ... step 4/62  loss=0.5338  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 7.633e+00 (batch 4)
  ... step 6/62  loss=0.4887  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 6.357e+00 (batch 6)
  ... step 8/62  loss=0.6450  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 6.248e+00 (batch 8)
  ... step 10/62  loss=0.5061  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 7.138e+00 (batch 10)
  ... step 12/62  loss=0.8270  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 9.799e+00 (batch 12)
  ... step 14/62  loss=0.6688  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 8.926e+00 (batch 14)
  ... step 16/62  loss=0.4660  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 7.668e+00 (batch 16)
  ... step 18/62  loss=0.3917  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 6.499e+00 (batch 18)
  ... step 20/62  loss=0.6858  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 1.119e+01 (batch 20)
  ... step 22/62  loss=0.4407  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 5.512e+00 (batch 22)
  ... step 24/62  loss=0.8425  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 1.213e+01 (batch 24)
  ... step 26/62  loss=0.4895  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 8.023e+00 (batch 26)
  ... step 28/62  loss=0.5969  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 1.008e+01 (batch 28)
  ... step 30/62  loss=1.1424  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 9.406e+00 (batch 30)
  ... step 32/62  loss=0.6014  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 7.580e+00 (batch 32)
  ... step 34/62  loss=0.3325  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 6.543e+00 (batch 34)
  ... step 36/62  loss=0.6049  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 7.208e+00 (batch 36)
  ... step 38/62  loss=0.5527  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 7.684e+00 (batch 38)
  ... step 40/62  loss=0.3933  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 7.232e+00 (batch 40)
  ... step 42/62  loss=0.8770  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 1.050e+01 (batch 42)
  ... step 44/62  loss=0.5495  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 7.344e+00 (batch 44)
  ... step 46/62  loss=0.4954  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 7.387e+00 (batch 46)
  ... step 48/62  loss=0.6140  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 6.978e+00 (batch 48)
  ... step 50/62  loss=0.4129  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 8.953e+00 (batch 50)
  ... step 52/62  loss=0.2949  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 4.598e+00 (batch 52)
  ... step 54/62  loss=0.6090  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 8.233e+00 (batch 54)
  ... step 56/62  loss=0.5087  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 5.332e+00 (batch 56)
  ... step 58/62  loss=0.5169  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 6.439e+00 (batch 58)
  ... step 60/62  loss=0.7588  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 7.813e+00 (batch 60)
âœ… epoch 4 forward/backward done in 35.0s
  ðŸ”Ž val step 0: batch (8, 32, 2, 324, 1, 16) loss=1.2488
[04] train=0.5965  val=5.9359  RMSE(std)=[Qi:2.602, Qe:2.354, Î“:2.344]  RMSE(phys)=[Qi:178.737, Qe:214.281, Î“:97.796]  (2.3s)
âœ… Saved unified NN â†’ ./mnt/data/bin.cgyro.nn
ðŸŸ¦ Starting epoch 5/20 (train steps â‰ˆ 62)
  ... step 0/62  loss=0.6806  (0.1s since last print)
  â†˜ grad L2 norm â‰ˆ 6.585e+00 (batch 0)
  ... step 2/62  loss=0.5639  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 7.318e+00 (batch 2)
  ... step 4/62  loss=0.6111  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 8.868e+00 (batch 4)
  ... step 6/62  loss=0.5616  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 9.454e+00 (batch 6)
  ... step 8/62  loss=0.5471  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 7.454e+00 (batch 8)
  ... step 10/62  loss=0.9964  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 1.517e+01 (batch 10)
  ... step 12/62  loss=0.3690  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 4.823e+00 (batch 12)
  ... step 14/62  loss=0.6393  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 8.439e+00 (batch 14)
  ... step 16/62  loss=0.4260  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 4.756e+00 (batch 16)
  ... step 18/62  loss=0.5410  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 6.661e+00 (batch 18)
  ... step 20/62  loss=0.6853  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 7.508e+00 (batch 20)
  ... step 22/62  loss=0.5166  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 8.326e+00 (batch 22)
  ... step 24/62  loss=0.6447  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 9.005e+00 (batch 24)
  ... step 26/62  loss=0.5958  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 1.126e+01 (batch 26)
  ... step 28/62  loss=0.5586  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 7.239e+00 (batch 28)
  ... step 30/62  loss=0.4705  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 7.333e+00 (batch 30)
  ... step 32/62  loss=0.3591  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 6.613e+00 (batch 32)
  ... step 34/62  loss=0.3513  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 4.153e+00 (batch 34)
  ... step 36/62  loss=0.5631  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 8.205e+00 (batch 36)
  ... step 38/62  loss=0.4158  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 4.889e+00 (batch 38)
  ... step 40/62  loss=0.6585  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 6.355e+00 (batch 40)
  ... step 42/62  loss=0.3658  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 7.760e+00 (batch 42)
  ... step 44/62  loss=0.4678  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 7.325e+00 (batch 44)
  ... step 46/62  loss=0.7536  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 8.722e+00 (batch 46)
  ... step 48/62  loss=0.5870  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 8.408e+00 (batch 48)
  ... step 50/62  loss=0.9733  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 8.086e+00 (batch 50)
  ... step 52/62  loss=0.5236  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 7.591e+00 (batch 52)
  ... step 54/62  loss=0.7696  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 8.979e+00 (batch 54)
  ... step 56/62  loss=0.5747  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 7.316e+00 (batch 56)
  ... step 58/62  loss=0.7367  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 8.307e+00 (batch 58)
  ... step 60/62  loss=0.3614  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 5.445e+00 (batch 60)
âœ… epoch 5 forward/backward done in 35.0s
  ðŸ”Ž val step 0: batch (8, 32, 2, 324, 1, 16) loss=1.7407
[05] train=0.5884  val=1.6604  RMSE(std)=[Qi:1.301, Qe:1.286, Î“:1.279]  RMSE(phys)=[Qi:89.330, Qe:117.008, Î“:53.387]  (2.3s)
âœ… Saved unified NN â†’ ./mnt/data/bin.cgyro.nn
ðŸŸ¦ Starting epoch 6/20 (train steps â‰ˆ 62)
  ... step 0/62  loss=0.2913  (0.1s since last print)
  â†˜ grad L2 norm â‰ˆ 4.557e+00 (batch 0)
  ... step 2/62  loss=0.5265  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 9.073e+00 (batch 2)
  ... step 4/62  loss=0.6810  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 6.645e+00 (batch 4)
  ... step 6/62  loss=0.7421  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 7.028e+00 (batch 6)
  ... step 8/62  loss=0.9071  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 8.213e+00 (batch 8)
  ... step 10/62  loss=0.4381  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 5.661e+00 (batch 10)
  ... step 12/62  loss=0.6298  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 8.616e+00 (batch 12)
  ... step 14/62  loss=0.7344  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 7.997e+00 (batch 14)
  ... step 16/62  loss=0.4511  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 5.148e+00 (batch 16)
  ... step 18/62  loss=0.5505  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 6.834e+00 (batch 18)
  ... step 20/62  loss=0.8093  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 8.698e+00 (batch 20)
  ... step 22/62  loss=0.8225  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 8.534e+00 (batch 22)
  ... step 24/62  loss=0.8419  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 1.042e+01 (batch 24)
  ... step 26/62  loss=0.6427  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 5.770e+00 (batch 26)
  ... step 28/62  loss=0.3328  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 3.967e+00 (batch 28)
  ... step 30/62  loss=0.3856  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 5.165e+00 (batch 30)
  ... step 32/62  loss=0.3877  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 5.135e+00 (batch 32)
  ... step 34/62  loss=0.8397  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 1.033e+01 (batch 34)
  ... step 36/62  loss=0.2981  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 4.544e+00 (batch 36)
  ... step 38/62  loss=0.8251  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 1.083e+01 (batch 38)
  ... step 40/62  loss=0.8220  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 7.244e+00 (batch 40)
  ... step 42/62  loss=0.6544  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 6.807e+00 (batch 42)
  ... step 44/62  loss=0.4744  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 7.278e+00 (batch 44)
  ... step 46/62  loss=0.6526  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 8.228e+00 (batch 46)
  ... step 48/62  loss=0.4036  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 5.412e+00 (batch 48)
  ... step 50/62  loss=0.3936  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 4.994e+00 (batch 50)
  ... step 52/62  loss=0.3651  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 6.668e+00 (batch 52)
  ... step 54/62  loss=0.2748  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 5.114e+00 (batch 54)
  ... step 56/62  loss=0.4395  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 5.103e+00 (batch 56)
  ... step 58/62  loss=0.7522  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 8.141e+00 (batch 58)
  ... step 60/62  loss=0.7169  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 8.069e+00 (batch 60)
âœ… epoch 6 forward/backward done in 35.0s
  ðŸ”Ž val step 0: batch (8, 32, 2, 324, 1, 16) loss=4.7572
[06] train=0.5353  val=1.7671  RMSE(std)=[Qi:1.327, Qe:1.327, Î“:1.334]  RMSE(phys)=[Qi:91.141, Qe:120.797, Î“:55.658]  (2.3s)
âœ… Saved unified NN â†’ ./mnt/data/bin.cgyro.nn
ðŸŸ¦ Starting epoch 7/20 (train steps â‰ˆ 62)
  ... step 0/62  loss=0.4145  (0.1s since last print)
  â†˜ grad L2 norm â‰ˆ 5.464e+00 (batch 0)
  ... step 2/62  loss=0.9971  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 1.034e+01 (batch 2)
  ... step 4/62  loss=0.5125  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 5.754e+00 (batch 4)
  ... step 6/62  loss=0.5678  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 8.333e+00 (batch 6)
  ... step 8/62  loss=0.3818  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 6.913e+00 (batch 8)
  ... step 10/62  loss=0.5171  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 6.568e+00 (batch 10)
  ... step 12/62  loss=0.6250  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 6.372e+00 (batch 12)
  ... step 14/62  loss=0.2526  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 4.661e+00 (batch 14)
  ... step 16/62  loss=0.2751  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 3.557e+00 (batch 16)
  ... step 18/62  loss=0.6112  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 7.413e+00 (batch 18)
  ... step 20/62  loss=0.2629  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 4.220e+00 (batch 20)
  ... step 22/62  loss=0.3162  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 5.374e+00 (batch 22)
  ... step 24/62  loss=0.3969  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 6.789e+00 (batch 24)
  ... step 26/62  loss=0.5707  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 7.099e+00 (batch 26)
  ... step 28/62  loss=0.3352  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 4.961e+00 (batch 28)
  ... step 30/62  loss=0.3518  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 5.340e+00 (batch 30)
  ... step 32/62  loss=0.4753  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 7.376e+00 (batch 32)
  ... step 34/62  loss=0.8485  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 1.297e+01 (batch 34)
  ... step 36/62  loss=0.9396  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 8.666e+00 (batch 36)
  ... step 38/62  loss=0.2546  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 5.212e+00 (batch 38)
  ... step 40/62  loss=0.3039  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 4.691e+00 (batch 40)
  ... step 42/62  loss=0.6962  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 8.614e+00 (batch 42)
  ... step 44/62  loss=0.6852  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 9.715e+00 (batch 44)
  ... step 46/62  loss=1.0118  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 8.954e+00 (batch 46)
  ... step 48/62  loss=0.4999  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 7.066e+00 (batch 48)
  ... step 50/62  loss=0.3778  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 5.316e+00 (batch 50)
  ... step 52/62  loss=0.4029  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 5.522e+00 (batch 52)
  ... step 54/62  loss=0.6478  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 9.852e+00 (batch 54)
  ... step 56/62  loss=0.3046  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 5.144e+00 (batch 56)
  ... step 58/62  loss=0.6091  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 5.711e+00 (batch 58)
  ... step 60/62  loss=0.3224  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 5.728e+00 (batch 60)
âœ… epoch 7 forward/backward done in 35.0s
  ðŸ”Ž val step 0: batch (8, 32, 2, 324, 1, 16) loss=4.6668
[07] train=0.5116  val=1.8078  RMSE(std)=[Qi:1.346, Qe:1.337, Î“:1.351]  RMSE(phys)=[Qi:92.423, Qe:121.714, Î“:56.360]  (2.3s)
âœ… Saved unified NN â†’ ./mnt/data/bin.cgyro.nn
ðŸŸ¦ Starting epoch 8/20 (train steps â‰ˆ 62)
  ... step 0/62  loss=0.4533  (0.1s since last print)
  â†˜ grad L2 norm â‰ˆ 5.890e+00 (batch 0)
  ... step 2/62  loss=0.5337  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 7.415e+00 (batch 2)
  ... step 4/62  loss=0.8060  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 8.667e+00 (batch 4)
  ... step 6/62  loss=0.5085  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 8.391e+00 (batch 6)
  ... step 8/62  loss=0.9014  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 9.255e+00 (batch 8)
  ... step 10/62  loss=0.3218  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 4.229e+00 (batch 10)
  ... step 12/62  loss=0.7940  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 6.480e+00 (batch 12)
  ... step 14/62  loss=0.5237  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 6.679e+00 (batch 14)
  ... step 16/62  loss=0.6068  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 7.385e+00 (batch 16)
  ... step 18/62  loss=0.5045  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 8.965e+00 (batch 18)
  ... step 20/62  loss=0.3863  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 5.591e+00 (batch 20)
  ... step 22/62  loss=0.4114  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 4.884e+00 (batch 22)
  ... step 24/62  loss=0.6813  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 7.883e+00 (batch 24)
  ... step 26/62  loss=0.5494  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 8.052e+00 (batch 26)
  ... step 28/62  loss=0.3854  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 4.466e+00 (batch 28)
  ... step 30/62  loss=0.2892  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 4.151e+00 (batch 30)
  ... step 32/62  loss=0.4739  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 5.271e+00 (batch 32)
  ... step 34/62  loss=0.4531  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 7.067e+00 (batch 34)
  ... step 36/62  loss=0.3004  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 4.164e+00 (batch 36)
  ... step 38/62  loss=0.2305  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 3.935e+00 (batch 38)
  ... step 40/62  loss=0.4194  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 7.446e+00 (batch 40)
  ... step 42/62  loss=0.1916  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 3.296e+00 (batch 42)
  ... step 44/62  loss=1.0088  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 8.228e+00 (batch 44)
  ... step 46/62  loss=0.5914  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 7.073e+00 (batch 46)
  ... step 48/62  loss=0.2060  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 3.124e+00 (batch 48)
  ... step 50/62  loss=0.5212  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 6.610e+00 (batch 50)
  ... step 52/62  loss=0.4163  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 6.248e+00 (batch 52)
  ... step 54/62  loss=0.5780  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 5.418e+00 (batch 54)
  ... step 56/62  loss=0.4279  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 6.374e+00 (batch 56)
  ... step 58/62  loss=0.5435  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 6.331e+00 (batch 58)
  ... step 60/62  loss=0.4361  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 5.667e+00 (batch 60)
âœ… epoch 8 forward/backward done in 35.0s
  ðŸ”Ž val step 0: batch (8, 32, 2, 324, 1, 16) loss=3.3757
[08] train=0.4842  val=1.4827  RMSE(std)=[Qi:1.214, Qe:1.214, Î“:1.224]  RMSE(phys)=[Qi:83.407, Qe:110.508, Î“:51.094]  (2.3s)
âœ… Saved unified NN â†’ ./mnt/data/bin.cgyro.nn
ðŸŸ¦ Starting epoch 9/20 (train steps â‰ˆ 62)
  ... step 0/62  loss=0.2619  (0.1s since last print)
  â†˜ grad L2 norm â‰ˆ 3.570e+00 (batch 0)
  ... step 2/62  loss=0.2674  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 3.972e+00 (batch 2)
  ... step 4/62  loss=0.5038  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 5.403e+00 (batch 4)
  ... step 6/62  loss=0.4555  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 4.661e+00 (batch 6)
  ... step 8/62  loss=0.7659  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 7.460e+00 (batch 8)
  ... step 10/62  loss=0.4964  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 7.093e+00 (batch 10)
  ... step 12/62  loss=0.4006  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 5.320e+00 (batch 12)
  ... step 14/62  loss=0.3407  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 4.515e+00 (batch 14)
  ... step 16/62  loss=0.3091  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 4.285e+00 (batch 16)
  ... step 18/62  loss=0.4283  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 7.497e+00 (batch 18)
  ... step 20/62  loss=0.4235  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 6.718e+00 (batch 20)
  ... step 22/62  loss=0.5018  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 6.573e+00 (batch 22)
  ... step 24/62  loss=0.4531  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 5.183e+00 (batch 24)
  ... step 26/62  loss=0.9814  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 1.357e+01 (batch 26)
  ... step 28/62  loss=0.4189  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 4.321e+00 (batch 28)
  ... step 30/62  loss=0.3493  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 6.182e+00 (batch 30)
  ... step 32/62  loss=0.4309  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 5.845e+00 (batch 32)
  ... step 34/62  loss=0.4126  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 4.748e+00 (batch 34)
  ... step 36/62  loss=0.6293  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 6.520e+00 (batch 36)
  ... step 38/62  loss=0.6822  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 7.729e+00 (batch 38)
  ... step 40/62  loss=0.4647  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 6.101e+00 (batch 40)
  ... step 42/62  loss=0.3727  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 4.155e+00 (batch 42)
  ... step 44/62  loss=0.3355  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 6.287e+00 (batch 44)
  ... step 46/62  loss=0.3976  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 4.825e+00 (batch 46)
  ... step 48/62  loss=0.3718  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 4.632e+00 (batch 48)
  ... step 50/62  loss=0.6906  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 6.981e+00 (batch 50)
  ... step 52/62  loss=0.4075  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 6.050e+00 (batch 52)
  ... step 54/62  loss=0.2664  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 4.065e+00 (batch 54)
  ... step 56/62  loss=0.4123  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 5.040e+00 (batch 56)
  ... step 58/62  loss=0.9154  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 8.871e+00 (batch 58)
  ... step 60/62  loss=0.4767  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 6.452e+00 (batch 60)
âœ… epoch 9 forward/backward done in 35.0s
  ðŸ”Ž val step 0: batch (8, 32, 2, 324, 1, 16) loss=5.1811
[09] train=0.4790  val=1.7183  RMSE(std)=[Qi:1.317, Qe:1.305, Î“:1.310]  RMSE(phys)=[Qi:90.460, Qe:118.790, Î“:54.677]  (2.3s)
âœ… Saved unified NN â†’ ./mnt/data/bin.cgyro.nn
ðŸŸ¦ Starting epoch 10/20 (train steps â‰ˆ 62)
  ... step 0/62  loss=0.4696  (0.1s since last print)
  â†˜ grad L2 norm â‰ˆ 5.777e+00 (batch 0)
  ... step 2/62  loss=0.6477  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 7.083e+00 (batch 2)
  ... step 4/62  loss=0.8958  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 1.015e+01 (batch 4)
  ... step 6/62  loss=0.4295  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 5.968e+00 (batch 6)
  ... step 8/62  loss=0.7068  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 9.048e+00 (batch 8)
  ... step 10/62  loss=0.1983  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 3.316e+00 (batch 10)
  ... step 12/62  loss=0.6845  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 8.785e+00 (batch 12)
  ... step 14/62  loss=0.6295  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 7.754e+00 (batch 14)
  ... step 16/62  loss=0.3299  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 6.860e+00 (batch 16)
  ... step 18/62  loss=0.5890  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 7.362e+00 (batch 18)
  ... step 20/62  loss=0.4632  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 5.088e+00 (batch 20)
  ... step 22/62  loss=0.2702  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 5.179e+00 (batch 22)
  ... step 24/62  loss=0.3767  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 5.074e+00 (batch 24)
  ... step 26/62  loss=0.6285  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 9.090e+00 (batch 26)
  ... step 28/62  loss=0.5353  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 8.568e+00 (batch 28)
  ... step 30/62  loss=0.4772  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 6.352e+00 (batch 30)
  ... step 32/62  loss=0.6240  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 7.364e+00 (batch 32)
  ... step 34/62  loss=0.3139  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 4.729e+00 (batch 34)
  ... step 36/62  loss=0.4257  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 4.798e+00 (batch 36)
  ... step 38/62  loss=0.5392  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 6.929e+00 (batch 38)
  ... step 40/62  loss=0.1961  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 2.852e+00 (batch 40)
  ... step 42/62  loss=0.2489  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 3.891e+00 (batch 42)
  ... step 44/62  loss=0.6367  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 6.974e+00 (batch 44)
  ... step 46/62  loss=0.2644  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 4.040e+00 (batch 46)
  ... step 48/62  loss=0.7048  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 8.030e+00 (batch 48)
  ... step 50/62  loss=0.4028  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 4.982e+00 (batch 50)
  ... step 52/62  loss=0.6338  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 5.158e+00 (batch 52)
  ... step 54/62  loss=0.3265  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 5.240e+00 (batch 54)
  ... step 56/62  loss=0.6450  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 6.909e+00 (batch 56)
  ... step 58/62  loss=0.3048  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 6.304e+00 (batch 58)
  ... step 60/62  loss=0.6984  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 7.015e+00 (batch 60)
âœ… epoch 10 forward/backward done in 35.0s
  ðŸ”Ž val step 0: batch (8, 32, 2, 324, 1, 16) loss=5.3110
[10] train=0.4797  val=1.8211  RMSE(std)=[Qi:1.356, Qe:1.338, Î“:1.355]  RMSE(phys)=[Qi:93.102, Qe:121.783, Î“:56.534]  (2.3s)
âœ… Saved unified NN â†’ ./mnt/data/bin.cgyro.nn
ðŸŸ¦ Starting epoch 11/20 (train steps â‰ˆ 62)
  ... step 0/62  loss=0.4576  (0.1s since last print)
  â†˜ grad L2 norm â‰ˆ 5.055e+00 (batch 0)
  ... step 2/62  loss=0.3268  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 3.723e+00 (batch 2)
  ... step 4/62  loss=0.5169  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 5.738e+00 (batch 4)
  ... step 6/62  loss=0.7642  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 7.753e+00 (batch 6)
  ... step 8/62  loss=0.4441  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 5.723e+00 (batch 8)
  ... step 10/62  loss=0.3295  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 4.655e+00 (batch 10)
  ... step 12/62  loss=0.2707  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 4.336e+00 (batch 12)
  ... step 14/62  loss=0.5725  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 6.160e+00 (batch 14)
  ... step 16/62  loss=0.2752  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 4.327e+00 (batch 16)
  ... step 18/62  loss=0.7177  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 1.048e+01 (batch 18)
  ... step 20/62  loss=0.4406  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 5.734e+00 (batch 20)
  ... step 22/62  loss=0.4815  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 3.424e+00 (batch 22)
  ... step 24/62  loss=0.2577  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 3.919e+00 (batch 24)
  ... step 26/62  loss=0.4027  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 5.118e+00 (batch 26)
  ... step 28/62  loss=0.2036  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 3.046e+00 (batch 28)
  ... step 30/62  loss=0.5679  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 6.018e+00 (batch 30)
  ... step 32/62  loss=0.7516  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 7.990e+00 (batch 32)
  ... step 34/62  loss=0.2912  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 4.067e+00 (batch 34)
  ... step 36/62  loss=0.4141  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 4.164e+00 (batch 36)
  ... step 38/62  loss=0.4645  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 4.858e+00 (batch 38)
  ... step 40/62  loss=0.5291  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 5.938e+00 (batch 40)
  ... step 42/62  loss=0.3593  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 4.379e+00 (batch 42)
  ... step 44/62  loss=0.5970  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 6.741e+00 (batch 44)
  ... step 46/62  loss=0.6302  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 5.125e+00 (batch 46)
  ... step 48/62  loss=0.4474  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 5.275e+00 (batch 48)
  ... step 50/62  loss=0.3848  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 6.397e+00 (batch 50)
  ... step 52/62  loss=0.7320  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 5.169e+00 (batch 52)
  ... step 54/62  loss=0.6057  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 7.160e+00 (batch 54)
  ... step 56/62  loss=0.2776  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 4.539e+00 (batch 56)
  ... step 58/62  loss=0.8261  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 6.735e+00 (batch 58)
  ... step 60/62  loss=0.4375  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 4.423e+00 (batch 60)
âœ… epoch 11 forward/backward done in 35.0s
  ðŸ”Ž val step 0: batch (8, 32, 2, 324, 1, 16) loss=4.7766
[11] train=0.4721  val=1.7929  RMSE(std)=[Qi:1.345, Qe:1.330, Î“:1.342]  RMSE(phys)=[Qi:92.371, Qe:121.039, Î“:56.004]  (2.3s)
âœ… Saved unified NN â†’ ./mnt/data/bin.cgyro.nn
ðŸŸ¦ Starting epoch 12/20 (train steps â‰ˆ 62)
  ... step 0/62  loss=0.3453  (0.1s since last print)
  â†˜ grad L2 norm â‰ˆ 5.045e+00 (batch 0)
  ... step 2/62  loss=0.4346  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 4.581e+00 (batch 2)
  ... step 4/62  loss=0.4518  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 4.811e+00 (batch 4)
  ... step 6/62  loss=0.6293  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 5.670e+00 (batch 6)
  ... step 8/62  loss=0.9332  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 6.830e+00 (batch 8)
  ... step 10/62  loss=0.5835  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 5.259e+00 (batch 10)
  ... step 12/62  loss=0.2007  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 2.677e+00 (batch 12)
  ... step 14/62  loss=0.6932  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 7.106e+00 (batch 14)
  ... step 16/62  loss=0.4971  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 6.980e+00 (batch 16)
  ... step 18/62  loss=0.4592  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 6.485e+00 (batch 18)
  ... step 20/62  loss=0.8237  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 8.265e+00 (batch 20)
  ... step 22/62  loss=0.4384  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 5.416e+00 (batch 22)
  ... step 24/62  loss=0.3854  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 4.405e+00 (batch 24)
  ... step 26/62  loss=0.4561  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 6.058e+00 (batch 26)
  ... step 28/62  loss=0.3074  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 3.965e+00 (batch 28)
  ... step 30/62  loss=0.4145  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 5.078e+00 (batch 30)
  ... step 32/62  loss=0.2687  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 3.904e+00 (batch 32)
  ... step 34/62  loss=0.2891  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 3.545e+00 (batch 34)
  ... step 36/62  loss=0.3291  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 3.654e+00 (batch 36)
  ... step 38/62  loss=0.2517  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 3.927e+00 (batch 38)
  ... step 40/62  loss=0.2624  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 3.807e+00 (batch 40)
  ... step 42/62  loss=0.4209  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 4.950e+00 (batch 42)
  ... step 44/62  loss=0.4084  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 4.156e+00 (batch 44)
  ... step 46/62  loss=0.5135  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 4.493e+00 (batch 46)
  ... step 48/62  loss=0.3490  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 4.851e+00 (batch 48)
  ... step 50/62  loss=0.5135  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 6.314e+00 (batch 50)
  ... step 52/62  loss=0.4112  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 4.522e+00 (batch 52)
  ... step 54/62  loss=0.3282  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 5.154e+00 (batch 54)
  ... step 56/62  loss=0.4841  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 5.234e+00 (batch 56)
  ... step 58/62  loss=0.5210  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 6.825e+00 (batch 58)
  ... step 60/62  loss=0.3677  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 4.582e+00 (batch 60)
âœ… epoch 12 forward/backward done in 35.0s
  ðŸ”Ž val step 0: batch (8, 32, 2, 324, 1, 16) loss=5.5065
[12] train=0.4499  val=1.8612  RMSE(std)=[Qi:1.370, Qe:1.355, Î“:1.368]  RMSE(phys)=[Qi:94.072, Qe:123.356, Î“:57.073]  (2.3s)
âœ… Saved unified NN â†’ ./mnt/data/bin.cgyro.nn
ðŸŸ¦ Starting epoch 13/20 (train steps â‰ˆ 62)
  ... step 0/62  loss=0.3501  (0.1s since last print)
  â†˜ grad L2 norm â‰ˆ 3.840e+00 (batch 0)
  ... step 2/62  loss=0.2728  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 4.518e+00 (batch 2)
  ... step 4/62  loss=0.3509  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 4.455e+00 (batch 4)
  ... step 6/62  loss=0.4525  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 4.431e+00 (batch 6)
  ... step 8/62  loss=0.4321  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 6.677e+00 (batch 8)
  ... step 10/62  loss=0.5352  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 7.605e+00 (batch 10)
  ... step 12/62  loss=0.5508  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 8.035e+00 (batch 12)
  ... step 14/62  loss=0.4928  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 5.207e+00 (batch 14)
  ... step 16/62  loss=0.4473  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 7.894e+00 (batch 16)
  ... step 18/62  loss=0.3097  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 4.054e+00 (batch 18)
  ... step 20/62  loss=0.4368  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 4.635e+00 (batch 20)
  ... step 22/62  loss=0.4890  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 5.364e+00 (batch 22)
  ... step 24/62  loss=0.2885  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 4.957e+00 (batch 24)
  ... step 26/62  loss=0.2588  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 3.445e+00 (batch 26)
  ... step 28/62  loss=0.3313  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 3.903e+00 (batch 28)
  ... step 30/62  loss=0.4731  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 7.114e+00 (batch 30)
  ... step 32/62  loss=0.3381  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 4.895e+00 (batch 32)
  ... step 34/62  loss=0.6166  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 6.401e+00 (batch 34)
  ... step 36/62  loss=0.1588  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 3.458e+00 (batch 36)
  ... step 38/62  loss=0.7097  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 7.426e+00 (batch 38)
  ... step 40/62  loss=0.6001  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 6.488e+00 (batch 40)
  ... step 42/62  loss=0.3200  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 4.328e+00 (batch 42)
  ... step 44/62  loss=0.4815  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 5.916e+00 (batch 44)
  ... step 46/62  loss=1.0794  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 5.914e+00 (batch 46)
  ... step 48/62  loss=0.4207  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 5.385e+00 (batch 48)
  ... step 50/62  loss=0.3679  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 4.522e+00 (batch 50)
  ... step 52/62  loss=0.4732  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 5.312e+00 (batch 52)
  ... step 54/62  loss=0.5539  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 6.653e+00 (batch 54)
  ... step 56/62  loss=0.4776  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 3.976e+00 (batch 56)
  ... step 58/62  loss=0.4043  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 6.070e+00 (batch 58)
  ... step 60/62  loss=0.4568  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 4.000e+00 (batch 60)
âœ… epoch 13 forward/backward done in 35.0s
  ðŸ”Ž val step 0: batch (8, 32, 2, 324, 1, 16) loss=5.9173
[13] train=0.4490  val=2.1154  RMSE(std)=[Qi:1.461, Qe:1.443, Î“:1.459]  RMSE(phys)=[Qi:100.352, Qe:131.323, Î“:60.893]  (2.3s)
âœ… Saved unified NN â†’ ./mnt/data/bin.cgyro.nn
ðŸŸ¦ Starting epoch 14/20 (train steps â‰ˆ 62)
  ... step 0/62  loss=0.3946  (0.1s since last print)
  â†˜ grad L2 norm â‰ˆ 4.816e+00 (batch 0)
  ... step 2/62  loss=0.3499  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 4.547e+00 (batch 2)
  ... step 4/62  loss=0.5311  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 5.825e+00 (batch 4)
  ... step 6/62  loss=0.4612  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 5.065e+00 (batch 6)
  ... step 8/62  loss=0.2502  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 3.536e+00 (batch 8)
  ... step 10/62  loss=0.3672  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 3.897e+00 (batch 10)
  ... step 12/62  loss=0.6759  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 7.981e+00 (batch 12)
  ... step 14/62  loss=0.3327  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 3.639e+00 (batch 14)
  ... step 16/62  loss=0.6804  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 7.201e+00 (batch 16)
  ... step 18/62  loss=0.3101  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 3.899e+00 (batch 18)
  ... step 20/62  loss=0.6865  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 8.129e+00 (batch 20)
  ... step 22/62  loss=0.5309  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 5.974e+00 (batch 22)
  ... step 24/62  loss=0.2936  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 4.301e+00 (batch 24)
  ... step 26/62  loss=0.5091  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 6.527e+00 (batch 26)
  ... step 28/62  loss=0.4213  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 4.738e+00 (batch 28)
  ... step 30/62  loss=0.4403  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 6.787e+00 (batch 30)
  ... step 32/62  loss=0.9726  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 9.281e+00 (batch 32)
  ... step 34/62  loss=0.5167  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 5.031e+00 (batch 34)
  ... step 36/62  loss=0.2438  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 4.137e+00 (batch 36)
  ... step 38/62  loss=0.4324  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 3.919e+00 (batch 38)
  ... step 40/62  loss=0.4032  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 4.942e+00 (batch 40)
  ... step 42/62  loss=0.2143  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 3.269e+00 (batch 42)
  ... step 44/62  loss=0.2561  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 3.583e+00 (batch 44)
  ... step 46/62  loss=0.2219  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 2.947e+00 (batch 46)
  ... step 48/62  loss=0.6088  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 5.715e+00 (batch 48)
  ... step 50/62  loss=0.7688  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 6.035e+00 (batch 50)
  ... step 52/62  loss=0.2761  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 3.360e+00 (batch 52)
  ... step 54/62  loss=0.2133  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 3.163e+00 (batch 54)
  ... step 56/62  loss=0.5140  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 4.931e+00 (batch 56)
  ... step 58/62  loss=0.2485  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 2.355e+00 (batch 58)
  ... step 60/62  loss=0.2106  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 2.893e+00 (batch 60)
âœ… epoch 14 forward/backward done in 35.0s
  ðŸ”Ž val step 0: batch (8, 32, 2, 324, 1, 16) loss=5.3772
[14] train=0.4396  val=1.9693  RMSE(std)=[Qi:1.407, Qe:1.395, Î“:1.407]  RMSE(phys)=[Qi:96.637, Qe:127.011, Î“:58.730]  (2.3s)
âœ… Saved unified NN â†’ ./mnt/data/bin.cgyro.nn
ðŸŸ¦ Starting epoch 15/20 (train steps â‰ˆ 62)
  ... step 0/62  loss=0.4355  (0.1s since last print)
  â†˜ grad L2 norm â‰ˆ 5.681e+00 (batch 0)
  ... step 2/62  loss=0.4152  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 4.036e+00 (batch 2)
  ... step 4/62  loss=0.7114  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 5.774e+00 (batch 4)
  ... step 6/62  loss=0.3855  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 5.682e+00 (batch 6)
  ... step 8/62  loss=0.2337  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 2.867e+00 (batch 8)
  ... step 10/62  loss=0.4653  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 5.197e+00 (batch 10)
  ... step 12/62  loss=0.5063  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 5.892e+00 (batch 12)
  ... step 14/62  loss=0.6421  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 5.303e+00 (batch 14)
  ... step 16/62  loss=0.5738  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 5.206e+00 (batch 16)
  ... step 18/62  loss=0.2111  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 3.288e+00 (batch 18)
  ... step 20/62  loss=0.2672  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 3.342e+00 (batch 20)
  ... step 22/62  loss=0.3253  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 4.359e+00 (batch 22)
  ... step 24/62  loss=0.2663  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 3.532e+00 (batch 24)
  ... step 26/62  loss=0.4595  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 4.607e+00 (batch 26)
  ... step 28/62  loss=0.6008  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 7.125e+00 (batch 28)
  ... step 30/62  loss=0.2890  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 4.909e+00 (batch 30)
  ... step 32/62  loss=0.1816  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 2.599e+00 (batch 32)
  ... step 34/62  loss=0.2683  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 3.526e+00 (batch 34)
  ... step 36/62  loss=0.2145  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 2.632e+00 (batch 36)
  ... step 38/62  loss=0.3444  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 4.902e+00 (batch 38)
  ... step 40/62  loss=0.7630  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 7.916e+00 (batch 40)
  ... step 42/62  loss=0.2423  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 3.270e+00 (batch 42)
  ... step 44/62  loss=0.2174  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 3.458e+00 (batch 44)
  ... step 46/62  loss=0.5549  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 4.280e+00 (batch 46)
  ... step 48/62  loss=0.5967  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 6.406e+00 (batch 48)
  ... step 50/62  loss=0.3463  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 3.574e+00 (batch 50)
  ... step 52/62  loss=0.6527  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 5.608e+00 (batch 52)
  ... step 54/62  loss=0.4223  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 4.806e+00 (batch 54)
  ... step 56/62  loss=0.3899  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 6.436e+00 (batch 56)
  ... step 58/62  loss=0.8036  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 7.099e+00 (batch 58)
  ... step 60/62  loss=0.6924  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 6.086e+00 (batch 60)
âœ… epoch 15 forward/backward done in 35.0s
  ðŸ”Ž val step 0: batch (8, 32, 2, 324, 1, 16) loss=5.5911
[15] train=0.4521  val=1.8829  RMSE(std)=[Qi:1.377, Qe:1.364, Î“:1.375]  RMSE(phys)=[Qi:94.610, Qe:124.131, Î“:57.385]  (2.3s)
âœ… Saved unified NN â†’ ./mnt/data/bin.cgyro.nn
ðŸŸ¦ Starting epoch 16/20 (train steps â‰ˆ 62)
  ... step 0/62  loss=0.4475  (0.1s since last print)
  â†˜ grad L2 norm â‰ˆ 5.039e+00 (batch 0)
  ... step 2/62  loss=0.3754  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 5.373e+00 (batch 2)
  ... step 4/62  loss=0.2386  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 4.353e+00 (batch 4)
  ... step 6/62  loss=0.4348  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 6.196e+00 (batch 6)
  ... step 8/62  loss=0.4640  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 7.286e+00 (batch 8)
  ... step 10/62  loss=0.6224  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 7.331e+00 (batch 10)
  ... step 12/62  loss=0.6395  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 8.290e+00 (batch 12)
  ... step 14/62  loss=0.4968  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 6.108e+00 (batch 14)
  ... step 16/62  loss=0.8450  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 5.959e+00 (batch 16)
  ... step 18/62  loss=0.6264  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 8.057e+00 (batch 18)
  ... step 20/62  loss=0.3599  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 4.049e+00 (batch 20)
  ... step 22/62  loss=0.6310  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 7.897e+00 (batch 22)
  ... step 24/62  loss=0.3172  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 4.309e+00 (batch 24)
  ... step 26/62  loss=0.3786  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 4.918e+00 (batch 26)
  ... step 28/62  loss=0.4122  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 5.114e+00 (batch 28)
  ... step 30/62  loss=0.6451  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 8.045e+00 (batch 30)
  ... step 32/62  loss=0.8047  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 6.319e+00 (batch 32)
  ... step 34/62  loss=0.2941  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 3.409e+00 (batch 34)
  ... step 36/62  loss=0.2923  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 3.732e+00 (batch 36)
  ... step 38/62  loss=0.3914  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 5.849e+00 (batch 38)
  ... step 40/62  loss=0.3525  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 3.955e+00 (batch 40)
  ... step 42/62  loss=0.4656  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 5.642e+00 (batch 42)
  ... step 44/62  loss=0.3217  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 3.807e+00 (batch 44)
  ... step 46/62  loss=0.4173  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 4.690e+00 (batch 46)
  ... step 48/62  loss=0.3460  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 4.353e+00 (batch 48)
  ... step 50/62  loss=0.5009  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 5.248e+00 (batch 50)
  ... step 52/62  loss=0.5288  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 7.274e+00 (batch 52)
  ... step 54/62  loss=0.3920  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 5.032e+00 (batch 54)
  ... step 56/62  loss=0.3009  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 3.108e+00 (batch 56)
  ... step 58/62  loss=0.4203  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 4.217e+00 (batch 58)
  ... step 60/62  loss=0.2358  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 2.864e+00 (batch 60)
âœ… epoch 16 forward/backward done in 35.0s
  ðŸ”Ž val step 0: batch (8, 32, 2, 324, 1, 16) loss=6.0380
[16] train=0.4615  val=2.1819  RMSE(std)=[Qi:1.481, Qe:1.464, Î“:1.486]  RMSE(phys)=[Qi:101.721, Qe:133.255, Î“:62.015]  (2.3s)
âœ… Saved unified NN â†’ ./mnt/data/bin.cgyro.nn
ðŸŸ¦ Starting epoch 17/20 (train steps â‰ˆ 62)
  ... step 0/62  loss=0.3175  (0.1s since last print)
  â†˜ grad L2 norm â‰ˆ 3.021e+00 (batch 0)
  ... step 2/62  loss=0.5581  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 5.133e+00 (batch 2)
  ... step 4/62  loss=0.2253  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 4.161e+00 (batch 4)
  ... step 6/62  loss=0.3616  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 5.058e+00 (batch 6)
  ... step 8/62  loss=0.3081  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 4.286e+00 (batch 8)
  ... step 10/62  loss=0.2895  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 4.218e+00 (batch 10)
  ... step 12/62  loss=0.3269  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 4.318e+00 (batch 12)
  ... step 14/62  loss=0.2993  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 4.080e+00 (batch 14)
  ... step 16/62  loss=0.5033  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 5.794e+00 (batch 16)
  ... step 18/62  loss=0.3824  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 5.095e+00 (batch 18)
  ... step 20/62  loss=0.5650  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 5.365e+00 (batch 20)
  ... step 22/62  loss=0.6946  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 8.421e+00 (batch 22)
  ... step 24/62  loss=0.3493  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 4.993e+00 (batch 24)
  ... step 26/62  loss=0.3869  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 5.308e+00 (batch 26)
  ... step 28/62  loss=0.2417  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 3.375e+00 (batch 28)
  ... step 30/62  loss=0.9606  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 8.379e+00 (batch 30)
  ... step 32/62  loss=0.3737  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 5.117e+00 (batch 32)
  ... step 34/62  loss=0.6204  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 8.842e+00 (batch 34)
  ... step 36/62  loss=0.4375  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 5.690e+00 (batch 36)
  ... step 38/62  loss=0.4868  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 5.729e+00 (batch 38)
  ... step 40/62  loss=0.2498  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 3.712e+00 (batch 40)
  ... step 42/62  loss=0.7237  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 7.012e+00 (batch 42)
  ... step 44/62  loss=0.5840  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 5.214e+00 (batch 44)
  ... step 46/62  loss=0.7483  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 8.480e+00 (batch 46)
  ... step 48/62  loss=0.4582  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 6.156e+00 (batch 48)
  ... step 50/62  loss=0.3015  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 4.817e+00 (batch 50)
  ... step 52/62  loss=0.2787  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 3.756e+00 (batch 52)
  ... step 54/62  loss=0.2339  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 3.871e+00 (batch 54)
  ... step 56/62  loss=0.2624  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 4.112e+00 (batch 56)
  ... step 58/62  loss=0.4995  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 4.522e+00 (batch 58)
  ... step 60/62  loss=0.4273  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 6.529e+00 (batch 60)
âœ… epoch 17 forward/backward done in 35.0s
  ðŸ”Ž val step 0: batch (8, 32, 2, 324, 1, 16) loss=4.3432
[17] train=0.4446  val=1.4952  RMSE(std)=[Qi:1.218, Qe:1.226, Î“:1.224]  RMSE(phys)=[Qi:83.672, Qe:111.553, Î“:51.094]  (2.3s)
âœ… Saved unified NN â†’ ./mnt/data/bin.cgyro.nn
ðŸŸ¦ Starting epoch 18/20 (train steps â‰ˆ 62)
  ... step 0/62  loss=0.1674  (0.1s since last print)
  â†˜ grad L2 norm â‰ˆ 3.100e+00 (batch 0)
  ... step 2/62  loss=0.5188  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 5.508e+00 (batch 2)
  ... step 4/62  loss=0.4142  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 4.221e+00 (batch 4)
  ... step 6/62  loss=0.2085  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 3.490e+00 (batch 6)
  ... step 8/62  loss=0.6528  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 6.084e+00 (batch 8)
  ... step 10/62  loss=0.3330  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 3.718e+00 (batch 10)
  ... step 12/62  loss=0.4313  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 5.647e+00 (batch 12)
  ... step 14/62  loss=0.6744  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 5.125e+00 (batch 14)
  ... step 16/62  loss=0.6522  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 4.932e+00 (batch 16)
  ... step 18/62  loss=0.4503  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 6.474e+00 (batch 18)
  ... step 20/62  loss=0.2072  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 2.424e+00 (batch 20)
  ... step 22/62  loss=0.7120  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 4.270e+00 (batch 22)
  ... step 24/62  loss=0.2607  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 2.544e+00 (batch 24)
  ... step 26/62  loss=0.6187  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 5.000e+00 (batch 26)
  ... step 28/62  loss=0.3467  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 4.097e+00 (batch 28)
  ... step 30/62  loss=0.3645  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 3.443e+00 (batch 30)
  ... step 32/62  loss=0.5812  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 4.204e+00 (batch 32)
  ... step 34/62  loss=0.5138  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 5.218e+00 (batch 34)
  ... step 36/62  loss=0.3892  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 4.656e+00 (batch 36)
  ... step 38/62  loss=0.2938  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 3.775e+00 (batch 38)
  ... step 40/62  loss=0.2901  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 3.445e+00 (batch 40)
  ... step 42/62  loss=0.2194  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 2.624e+00 (batch 42)
  ... step 44/62  loss=0.2278  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 2.735e+00 (batch 44)
  ... step 46/62  loss=0.4842  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 3.452e+00 (batch 46)
  ... step 48/62  loss=0.5129  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 5.448e+00 (batch 48)
  ... step 50/62  loss=0.4249  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 3.711e+00 (batch 50)
  ... step 52/62  loss=0.3871  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 4.405e+00 (batch 52)
  ... step 54/62  loss=0.2903  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 3.477e+00 (batch 54)
  ... step 56/62  loss=0.3459  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 3.332e+00 (batch 56)
  ... step 58/62  loss=0.4029  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 3.733e+00 (batch 58)
  ... step 60/62  loss=0.3517  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 3.631e+00 (batch 60)
âœ… epoch 18 forward/backward done in 35.0s
  ðŸ”Ž val step 0: batch (8, 32, 2, 324, 1, 16) loss=5.7937
[18] train=0.4364  val=2.0420  RMSE(std)=[Qi:1.435, Qe:1.420, Î“:1.432]  RMSE(phys)=[Qi:98.547, Qe:129.249, Î“:59.759]  (2.3s)
âœ… Saved unified NN â†’ ./mnt/data/bin.cgyro.nn
ðŸŸ¦ Starting epoch 19/20 (train steps â‰ˆ 62)
  ... step 0/62  loss=0.3963  (0.1s since last print)
  â†˜ grad L2 norm â‰ˆ 3.151e+00 (batch 0)
  ... step 2/62  loss=0.4844  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 4.134e+00 (batch 2)
  ... step 4/62  loss=0.2296  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 2.862e+00 (batch 4)
  ... step 6/62  loss=0.1750  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 2.770e+00 (batch 6)
  ... step 8/62  loss=0.3657  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 3.932e+00 (batch 8)
  ... step 10/62  loss=0.1470  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 2.498e+00 (batch 10)
  ... step 12/62  loss=0.7331  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 5.243e+00 (batch 12)
  ... step 14/62  loss=0.4192  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 4.399e+00 (batch 14)
  ... step 16/62  loss=0.2053  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 2.392e+00 (batch 16)
  ... step 18/62  loss=0.5605  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 6.279e+00 (batch 18)
  ... step 20/62  loss=0.3371  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 3.398e+00 (batch 20)
  ... step 22/62  loss=0.2689  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 4.473e+00 (batch 22)
  ... step 24/62  loss=0.4405  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 4.969e+00 (batch 24)
  ... step 26/62  loss=0.4316  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 5.320e+00 (batch 26)
  ... step 28/62  loss=0.3972  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 4.471e+00 (batch 28)
  ... step 30/62  loss=0.3582  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 4.859e+00 (batch 30)
  ... step 32/62  loss=0.5634  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 4.982e+00 (batch 32)
  ... step 34/62  loss=0.3441  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 4.840e+00 (batch 34)
  ... step 36/62  loss=0.4062  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 3.865e+00 (batch 36)
  ... step 38/62  loss=0.2990  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 4.542e+00 (batch 38)
  ... step 40/62  loss=0.5819  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 6.640e+00 (batch 40)
  ... step 42/62  loss=0.1520  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 2.213e+00 (batch 42)
  ... step 44/62  loss=0.4033  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 4.096e+00 (batch 44)
  ... step 46/62  loss=0.2729  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 3.220e+00 (batch 46)
  ... step 48/62  loss=0.4130  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 3.606e+00 (batch 48)
  ... step 50/62  loss=0.1968  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 2.949e+00 (batch 50)
  ... step 52/62  loss=0.5306  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 6.597e+00 (batch 52)
  ... step 54/62  loss=0.4069  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 5.848e+00 (batch 54)
  ... step 56/62  loss=0.3945  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 3.709e+00 (batch 56)
  ... step 58/62  loss=0.4612  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 8.459e+00 (batch 58)
  ... step 60/62  loss=0.4288  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 3.833e+00 (batch 60)
âœ… epoch 19 forward/backward done in 35.0s
  ðŸ”Ž val step 0: batch (8, 32, 2, 324, 1, 16) loss=6.5158
[19] train=0.4144  val=2.3323  RMSE(std)=[Qi:1.539, Qe:1.507, Î“:1.535]  RMSE(phys)=[Qi:105.711, Qe:137.192, Î“:64.051]  (2.3s)
âœ… Saved unified NN â†’ ./mnt/data/bin.cgyro.nn
ðŸŸ¦ Starting epoch 20/20 (train steps â‰ˆ 62)
  ... step 0/62  loss=0.3592  (0.1s since last print)
  â†˜ grad L2 norm â‰ˆ 4.626e+00 (batch 0)
  ... step 2/62  loss=0.3376  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 3.688e+00 (batch 2)
  ... step 4/62  loss=0.2588  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 4.270e+00 (batch 4)
  ... step 6/62  loss=0.3349  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 4.743e+00 (batch 6)
  ... step 8/62  loss=0.3804  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 4.583e+00 (batch 8)
  ... step 10/62  loss=0.7074  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 5.605e+00 (batch 10)
  ... step 12/62  loss=0.5623  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 5.045e+00 (batch 12)
  ... step 14/62  loss=0.2652  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 3.731e+00 (batch 14)
  ... step 16/62  loss=0.4604  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 6.810e+00 (batch 16)
  ... step 18/62  loss=0.5308  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 6.563e+00 (batch 18)
  ... step 20/62  loss=0.8777  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 1.045e+01 (batch 20)
  ... step 22/62  loss=0.1649  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 2.613e+00 (batch 22)
  ... step 24/62  loss=0.3690  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 4.436e+00 (batch 24)
  ... step 26/62  loss=0.1823  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 2.823e+00 (batch 26)
  ... step 28/62  loss=0.1826  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 3.347e+00 (batch 28)
  ... step 30/62  loss=0.2687  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 2.957e+00 (batch 30)
  ... step 32/62  loss=0.5681  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 5.274e+00 (batch 32)
  ... step 34/62  loss=0.7867  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 6.395e+00 (batch 34)
  ... step 36/62  loss=0.8132  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 5.522e+00 (batch 36)
  ... step 38/62  loss=0.4363  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 4.257e+00 (batch 38)
  ... step 40/62  loss=0.2810  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 3.992e+00 (batch 40)
  ... step 42/62  loss=0.3989  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 3.549e+00 (batch 42)
  ... step 44/62  loss=0.4415  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 5.500e+00 (batch 44)
  ... step 46/62  loss=0.2851  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 3.350e+00 (batch 46)
  ... step 48/62  loss=0.2975  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 3.554e+00 (batch 48)
  ... step 50/62  loss=0.1943  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 3.450e+00 (batch 50)
  ... step 52/62  loss=0.4290  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 3.820e+00 (batch 52)
  ... step 54/62  loss=0.1893  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 2.863e+00 (batch 54)
  ... step 56/62  loss=0.4082  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 5.465e+00 (batch 56)
  ... step 58/62  loss=0.2468  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 4.955e+00 (batch 58)
  ... step 60/62  loss=0.3629  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 5.528e+00 (batch 60)
âœ… epoch 20 forward/backward done in 35.0s
  ðŸ”Ž val step 0: batch (8, 32, 2, 324, 1, 16) loss=6.9689
[20] train=0.3891  val=2.4430  RMSE(std)=[Qi:1.578, Qe:1.541, Î“:1.570]  RMSE(phys)=[Qi:108.356, Qe:140.285, Î“:65.507]  (2.3s)
âœ… Saved unified NN â†’ ./mnt/data/bin.cgyro.nn
Saved metrics â†’ ./mnt/data/myrun_logs_deep_debug_ordered_t/metrics.csv
âœ… Final unified NN â†’ ./mnt/data/bin.cgyro.nn
âœ… Training complete. Artifacts in: ./mnt/data/myrun_logs_deep_debug_ordered_t
