[build_datasets] windows: total=615  train=369  val=123  test=123

[train] dataset:
  y_mean: [[[149.46204 223.82538  88.89333]]]
  y_std : [[[72.62504 92.54556 44.15767]]]
  sample Y: min=-1.324e+00, max=2.054e+00, mean=-2.283e-01, std=7.447e-01

[val] dataset:
  y_mean: [[[149.46204 223.82538  88.89333]]]
  y_std : [[[72.62504 92.54556 44.15767]]]
  sample Y: min=-1.976e+00, max=2.718e+00, mean=-3.365e-01, std=1.145e+00

[test] dataset:
  y_mean: [[[149.46204 223.82538  88.89333]]]
  y_std : [[[72.62504 92.54556 44.15767]]]
  sample Y: min=-1.871e+00, max=3.058e+00, mean=-1.987e-02, std=1.562e+00
âœ… Saved flux histograms in ./mnt/data/myrun_logs_deep_debug_ordered_t
ðŸŸ¦ Starting epoch 1/30 (train steps â‰ˆ 93)
[2041298] Î¦2FluxDeep forward: input (4, 32, 2, 324, 1, 16)
  ... step 0/93  loss=1.2821  (3.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.427e+01 (batch 0)
  ... step 2/93  loss=0.6509  (0.6s since last print)
  â†˜ grad L2 norm â‰ˆ 1.496e+01 (batch 2)
  ... step 4/93  loss=1.8527  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.454e+01 (batch 4)
  ... step 6/93  loss=1.3128  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.030e+01 (batch 6)
  ... step 8/93  loss=1.1215  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.798e+01 (batch 8)
  ... step 10/93  loss=0.7003  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.708e+01 (batch 10)
  ... step 12/93  loss=1.4763  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.196e+01 (batch 12)
  ... step 14/93  loss=1.7271  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.263e+01 (batch 14)
  ... step 16/93  loss=1.3761  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.083e+01 (batch 16)
  ... step 18/93  loss=0.8333  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.999e+01 (batch 18)
  ... step 20/93  loss=0.7322  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.581e+01 (batch 20)
  ... step 22/93  loss=1.2738  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.599e+01 (batch 22)
  ... step 24/93  loss=0.4785  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.093e+01 (batch 24)
  ... step 26/93  loss=1.0779  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.629e+01 (batch 26)
  ... step 28/93  loss=1.2438  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.885e+01 (batch 28)
  ... step 30/93  loss=1.2719  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.604e+01 (batch 30)
  ... step 32/93  loss=0.7590  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.330e+01 (batch 32)
  ... step 34/93  loss=1.0310  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.851e+01 (batch 34)
  ... step 36/93  loss=1.2218  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.778e+01 (batch 36)
  ... step 38/93  loss=0.9221  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.432e+01 (batch 38)
  ... step 40/93  loss=0.6540  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.226e+01 (batch 40)
  ... step 42/93  loss=0.5385  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.111e+01 (batch 42)
  ... step 44/93  loss=0.3031  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.934e+00 (batch 44)
  ... step 46/93  loss=1.2750  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.953e+01 (batch 46)
  ... step 48/93  loss=0.4596  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.228e+01 (batch 48)
  ... step 50/93  loss=0.3955  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.945e+00 (batch 50)
  ... step 52/93  loss=2.2599  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.337e+01 (batch 52)
  ... step 54/93  loss=0.8968  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.094e+01 (batch 54)
  ... step 56/93  loss=1.2260  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.868e+01 (batch 56)
  ... step 58/93  loss=0.9530  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.093e+01 (batch 58)
  ... step 60/93  loss=0.8639  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.463e+01 (batch 60)
  ... step 62/93  loss=0.7811  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.128e+01 (batch 62)
  ... step 64/93  loss=0.7328  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.335e+01 (batch 64)
  ... step 66/93  loss=0.9469  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.136e+01 (batch 66)
  ... step 68/93  loss=1.0363  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.487e+01 (batch 68)
  ... step 70/93  loss=0.6841  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.196e+01 (batch 70)
  ... step 72/93  loss=1.1835  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.631e+01 (batch 72)
  ... step 74/93  loss=0.4831  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.560e+00 (batch 74)
  ... step 76/93  loss=1.0697  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.157e+01 (batch 76)
  ... step 78/93  loss=1.0976  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.406e+01 (batch 78)
  ... step 80/93  loss=1.0056  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.325e+01 (batch 80)
  ... step 82/93  loss=1.8967  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.835e+01 (batch 82)
  ... step 84/93  loss=0.6417  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.124e+01 (batch 84)
  ... step 86/93  loss=0.5214  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.039e+01 (batch 86)
  ... step 88/93  loss=2.1986  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.146e+01 (batch 88)
  ... step 90/93  loss=0.6123  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.733e+00 (batch 90)
  ... step 92/93  loss=1.8991  (0.7s since last print)
  â†˜ grad L2 norm â‰ˆ 3.592e+01 (batch 92)
âœ… epoch 1 forward/backward done in 31.0s
  ðŸ”Ž val step 0: batch (4, 32, 2, 324, 1, 16) loss=1.0272
[01] train=1.0485  val=1.3971  RMSE(std)=[Qi:1.165, Qe:1.218, Î“:1.162]  RMSE(phys)=[Qi:84.610, Qe:112.755, Î“:51.300]  (1.7s)
âœ… Saved unified NN â†’ ./mnt/data/bin.cgyro.nn
ðŸŸ¦ Starting epoch 2/30 (train steps â‰ˆ 93)
  ... step 0/93  loss=0.2701  (0.1s since last print)
  â†˜ grad L2 norm â‰ˆ 6.933e+00 (batch 0)
  ... step 2/93  loss=1.0901  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.590e+01 (batch 2)
  ... step 4/93  loss=1.0068  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.506e+01 (batch 4)
  ... step 6/93  loss=0.4373  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.414e+00 (batch 6)
  ... step 8/93  loss=1.8511  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.351e+01 (batch 8)
  ... step 10/93  loss=0.9027  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.139e+01 (batch 10)
  ... step 12/93  loss=1.6061  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.646e+01 (batch 12)
  ... step 14/93  loss=0.3549  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.246e+00 (batch 14)
  ... step 16/93  loss=0.6736  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.145e+01 (batch 16)
  ... step 18/93  loss=0.8407  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.394e+01 (batch 18)
  ... step 20/93  loss=0.2388  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.330e+00 (batch 20)
  ... step 22/93  loss=1.4007  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.330e+01 (batch 22)
  ... step 24/93  loss=1.4588  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.288e+01 (batch 24)
  ... step 26/93  loss=0.5448  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.198e+00 (batch 26)
  ... step 28/93  loss=0.4062  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.814e+00 (batch 28)
  ... step 30/93  loss=1.9005  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.210e+01 (batch 30)
  ... step 32/93  loss=0.4431  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.085e+01 (batch 32)
  ... step 34/93  loss=0.5106  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.201e+00 (batch 34)
  ... step 36/93  loss=0.5884  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.722e+00 (batch 36)
  ... step 38/93  loss=1.4205  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.849e+01 (batch 38)
  ... step 40/93  loss=1.4253  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.616e+01 (batch 40)
  ... step 42/93  loss=0.8781  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.141e+00 (batch 42)
  ... step 44/93  loss=0.3085  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.996e+00 (batch 44)
  ... step 46/93  loss=0.4248  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.864e+00 (batch 46)
  ... step 48/93  loss=0.4243  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.931e+00 (batch 48)
  ... step 50/93  loss=1.9156  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.707e+01 (batch 50)
  ... step 52/93  loss=1.3234  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.321e+01 (batch 52)
  ... step 54/93  loss=1.2737  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.363e+01 (batch 54)
  ... step 56/93  loss=0.9317  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.191e+01 (batch 56)
  ... step 58/93  loss=1.7310  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.770e+01 (batch 58)
  ... step 60/93  loss=0.2917  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.638e+00 (batch 60)
  ... step 62/93  loss=0.7972  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.010e+01 (batch 62)
  ... step 64/93  loss=0.4823  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.889e+00 (batch 64)
  ... step 66/93  loss=0.7747  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.099e+01 (batch 66)
  ... step 68/93  loss=0.8198  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.272e+01 (batch 68)
  ... step 70/93  loss=0.6111  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.030e+01 (batch 70)
  ... step 72/93  loss=0.6156  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.630e+00 (batch 72)
  ... step 74/93  loss=1.2385  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.238e+01 (batch 74)
  ... step 76/93  loss=1.0102  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.591e+01 (batch 76)
  ... step 78/93  loss=0.3650  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.663e+00 (batch 78)
  ... step 80/93  loss=0.5386  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.355e+00 (batch 80)
  ... step 82/93  loss=1.5553  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.582e+01 (batch 82)
  ... step 84/93  loss=0.4645  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.984e+00 (batch 84)
  ... step 86/93  loss=0.6903  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.066e+01 (batch 86)
  ... step 88/93  loss=1.0430  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.261e+01 (batch 88)
  ... step 90/93  loss=1.2040  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.376e+01 (batch 90)
  ... step 92/93  loss=0.1949  (0.3s since last print)
  â†˜ grad L2 norm â‰ˆ 1.024e+01 (batch 92)
âœ… epoch 2 forward/backward done in 27.3s
  ðŸ”Ž val step 0: batch (4, 32, 2, 324, 1, 16) loss=1.1334
[02] train=0.9724  val=1.2974  RMSE(std)=[Qi:1.121, Qe:1.189, Î“:1.105]  RMSE(phys)=[Qi:81.420, Qe:110.039, Î“:48.802]  (1.7s)
âœ… Saved unified NN â†’ ./mnt/data/bin.cgyro.nn
ðŸŸ¦ Starting epoch 3/30 (train steps â‰ˆ 93)
  ... step 0/93  loss=0.7433  (0.1s since last print)
  â†˜ grad L2 norm â‰ˆ 1.142e+01 (batch 0)
  ... step 2/93  loss=1.3235  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.582e+01 (batch 2)
  ... step 4/93  loss=0.4648  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.208e+00 (batch 4)
  ... step 6/93  loss=1.1783  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.605e+01 (batch 6)
  ... step 8/93  loss=0.4954  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.030e+00 (batch 8)
  ... step 10/93  loss=0.5532  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.376e+00 (batch 10)
  ... step 12/93  loss=0.6014  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.139e+00 (batch 12)
  ... step 14/93  loss=0.7157  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.356e+01 (batch 14)
  ... step 16/93  loss=0.5452  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.075e+01 (batch 16)
  ... step 18/93  loss=0.7591  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.391e+01 (batch 18)
  ... step 20/93  loss=0.3432  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.086e+01 (batch 20)
  ... step 22/93  loss=0.5354  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.791e+00 (batch 22)
  ... step 24/93  loss=0.5087  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.240e+01 (batch 24)
  ... step 26/93  loss=0.9475  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.452e+01 (batch 26)
  ... step 28/93  loss=1.1516  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.530e+01 (batch 28)
  ... step 30/93  loss=1.1263  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.799e+01 (batch 30)
  ... step 32/93  loss=0.7295  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.507e+01 (batch 32)
  ... step 34/93  loss=0.4780  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.224e+01 (batch 34)
  ... step 36/93  loss=1.7701  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.188e+01 (batch 36)
  ... step 38/93  loss=0.7269  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.303e+01 (batch 38)
  ... step 40/93  loss=1.2391  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.940e+01 (batch 40)
  ... step 42/93  loss=1.4107  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.197e+01 (batch 42)
  ... step 44/93  loss=1.7313  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.731e+01 (batch 44)
  ... step 46/93  loss=1.5236  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.135e+01 (batch 46)
  ... step 48/93  loss=0.7956  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.406e+01 (batch 48)
  ... step 50/93  loss=1.0816  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.319e+01 (batch 50)
  ... step 52/93  loss=1.9757  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.790e+01 (batch 52)
  ... step 54/93  loss=0.3456  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.930e+00 (batch 54)
  ... step 56/93  loss=0.8464  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.238e+01 (batch 56)
  ... step 58/93  loss=0.6213  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.229e+00 (batch 58)
  ... step 60/93  loss=1.1769  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.064e+01 (batch 60)
  ... step 62/93  loss=1.7285  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.889e+01 (batch 62)
  ... step 64/93  loss=1.2190  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.801e+01 (batch 64)
  ... step 66/93  loss=1.0102  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.229e+01 (batch 66)
  ... step 68/93  loss=1.4131  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.895e+01 (batch 68)
  ... step 70/93  loss=0.7328  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.889e+01 (batch 70)
  ... step 72/93  loss=0.4923  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.335e+00 (batch 72)
  ... step 74/93  loss=0.3887  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.323e+01 (batch 74)
  ... step 76/93  loss=1.2010  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.788e+01 (batch 76)
  ... step 78/93  loss=0.2337  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.141e+00 (batch 78)
  ... step 80/93  loss=1.0684  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.303e+01 (batch 80)
  ... step 82/93  loss=0.8503  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.573e+00 (batch 82)
  ... step 84/93  loss=0.5549  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.225e+01 (batch 84)
  ... step 86/93  loss=0.9664  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.061e+01 (batch 86)
  ... step 88/93  loss=0.8035  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.145e+01 (batch 88)
  ... step 90/93  loss=1.3176  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.791e+01 (batch 90)
  ... step 92/93  loss=0.3705  (0.3s since last print)
  â†˜ grad L2 norm â‰ˆ 1.500e+01 (batch 92)
âœ… epoch 3 forward/backward done in 27.3s
  ðŸ”Ž val step 0: batch (4, 32, 2, 324, 1, 16) loss=34.5131
[03] train=0.8789  val=39.7222  RMSE(std)=[Qi:6.677, Qe:6.453, Î“:5.740]  RMSE(phys)=[Qi:484.894, Qe:597.163, Î“:253.482]  (1.7s)
âœ… Saved unified NN â†’ ./mnt/data/bin.cgyro.nn
ðŸŸ¦ Starting epoch 4/30 (train steps â‰ˆ 93)
  ... step 0/93  loss=0.1644  (0.1s since last print)
  â†˜ grad L2 norm â‰ˆ 5.164e+00 (batch 0)
  ... step 2/93  loss=1.0680  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.566e+01 (batch 2)
  ... step 4/93  loss=0.1995  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.861e+00 (batch 4)
  ... step 6/93  loss=0.6333  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.034e+01 (batch 6)
  ... step 8/93  loss=0.6014  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.808e+01 (batch 8)
  ... step 10/93  loss=0.6235  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.150e+00 (batch 10)
  ... step 12/93  loss=0.5596  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.064e+01 (batch 12)
  ... step 14/93  loss=0.5553  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.346e+01 (batch 14)
  ... step 16/93  loss=1.4916  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.722e+01 (batch 16)
  ... step 18/93  loss=0.4038  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.559e+00 (batch 18)
  ... step 20/93  loss=0.7160  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.232e+01 (batch 20)
  ... step 22/93  loss=1.4712  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.834e+01 (batch 22)
  ... step 24/93  loss=0.2583  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.357e+00 (batch 24)
  ... step 26/93  loss=0.4924  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.407e+00 (batch 26)
  ... step 28/93  loss=0.8528  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.455e+00 (batch 28)
  ... step 30/93  loss=0.3345  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.983e+00 (batch 30)
  ... step 32/93  loss=1.3622  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.537e+01 (batch 32)
  ... step 34/93  loss=1.1449  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.529e+01 (batch 34)
  ... step 36/93  loss=0.7260  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.317e+00 (batch 36)
  ... step 38/93  loss=0.5350  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.266e+00 (batch 38)
  ... step 40/93  loss=1.3959  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.764e+01 (batch 40)
  ... step 42/93  loss=0.4826  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.949e+00 (batch 42)
  ... step 44/93  loss=1.1973  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.346e+01 (batch 44)
  ... step 46/93  loss=1.3110  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.989e+01 (batch 46)
  ... step 48/93  loss=1.2823  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.532e+01 (batch 48)
  ... step 50/93  loss=0.6297  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.117e+01 (batch 50)
  ... step 52/93  loss=0.4175  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.995e+00 (batch 52)
  ... step 54/93  loss=0.7935  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.248e+01 (batch 54)
  ... step 56/93  loss=1.2699  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.630e+01 (batch 56)
  ... step 58/93  loss=0.9196  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.629e+01 (batch 58)
  ... step 60/93  loss=0.6071  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.675e+01 (batch 60)
  ... step 62/93  loss=0.7893  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.696e+01 (batch 62)
  ... step 64/93  loss=0.8376  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.330e+01 (batch 64)
  ... step 66/93  loss=0.4399  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.183e+01 (batch 66)
  ... step 68/93  loss=1.1517  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.434e+01 (batch 68)
  ... step 70/93  loss=0.4607  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.000e+00 (batch 70)
  ... step 72/93  loss=0.4346  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.025e+00 (batch 72)
  ... step 74/93  loss=1.0067  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.295e+01 (batch 74)
  ... step 76/93  loss=0.6475  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.310e+01 (batch 76)
  ... step 78/93  loss=0.2606  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.886e+00 (batch 78)
  ... step 80/93  loss=0.4462  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.345e+00 (batch 80)
  ... step 82/93  loss=0.1954  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.000e+00 (batch 82)
  ... step 84/93  loss=1.3851  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.099e+01 (batch 84)
  ... step 86/93  loss=0.3100  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.292e+00 (batch 86)
  ... step 88/93  loss=0.2681  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.309e+00 (batch 88)
  ... step 90/93  loss=0.2037  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.429e+00 (batch 90)
  ... step 92/93  loss=0.4113  (0.3s since last print)
  â†˜ grad L2 norm â‰ˆ 1.904e+01 (batch 92)
âœ… epoch 4 forward/backward done in 27.3s
  ðŸ”Ž val step 0: batch (4, 32, 2, 324, 1, 16) loss=37.4006
[04] train=0.7440  val=41.0154  RMSE(std)=[Qi:6.588, Qe:6.498, Î“:6.117]  RMSE(phys)=[Qi:478.467, Qe:601.353, Î“:270.118]  (1.7s)
âœ… Saved unified NN â†’ ./mnt/data/bin.cgyro.nn
ðŸŸ¦ Starting epoch 5/30 (train steps â‰ˆ 93)
  ... step 0/93  loss=1.3270  (0.1s since last print)
  â†˜ grad L2 norm â‰ˆ 1.567e+01 (batch 0)
  ... step 2/93  loss=0.7086  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.306e+01 (batch 2)
  ... step 4/93  loss=0.2483  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.102e+00 (batch 4)
  ... step 6/93  loss=0.5161  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.040e+01 (batch 6)
  ... step 8/93  loss=1.9839  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.626e+01 (batch 8)
  ... step 10/93  loss=0.5481  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.441e+01 (batch 10)
  ... step 12/93  loss=0.9008  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.793e+01 (batch 12)
  ... step 14/93  loss=0.8847  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.546e+01 (batch 14)
  ... step 16/93  loss=0.7542  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.531e+01 (batch 16)
  ... step 18/93  loss=0.8701  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.393e+01 (batch 18)
  ... step 20/93  loss=1.2388  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.929e+01 (batch 20)
  ... step 22/93  loss=0.8286  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.498e+01 (batch 22)
  ... step 24/93  loss=0.3085  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.654e+00 (batch 24)
  ... step 26/93  loss=0.6021  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.825e+00 (batch 26)
  ... step 28/93  loss=0.5305  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.315e+01 (batch 28)
  ... step 30/93  loss=0.7798  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.172e+01 (batch 30)
  ... step 32/93  loss=0.7830  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.518e+01 (batch 32)
  ... step 34/93  loss=0.3987  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.393e+00 (batch 34)
  ... step 36/93  loss=0.5195  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.176e+01 (batch 36)
  ... step 38/93  loss=0.8798  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.314e+01 (batch 38)
  ... step 40/93  loss=0.6499  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.525e+01 (batch 40)
  ... step 42/93  loss=0.7270  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.036e+01 (batch 42)
  ... step 44/93  loss=0.2456  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.539e+00 (batch 44)
  ... step 46/93  loss=0.7064  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.442e+01 (batch 46)
  ... step 48/93  loss=0.5605  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.217e+01 (batch 48)
  ... step 50/93  loss=0.6137  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.007e+01 (batch 50)
  ... step 52/93  loss=0.3024  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.296e+00 (batch 52)
  ... step 54/93  loss=0.8940  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.702e+01 (batch 54)
  ... step 56/93  loss=0.7149  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.487e+01 (batch 56)
  ... step 58/93  loss=0.8405  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.847e+01 (batch 58)
  ... step 60/93  loss=0.4529  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.280e+01 (batch 60)
  ... step 62/93  loss=0.6669  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.670e+01 (batch 62)
  ... step 64/93  loss=0.4261  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.003e+01 (batch 64)
  ... step 66/93  loss=0.5443  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.405e+01 (batch 66)
  ... step 68/93  loss=1.3885  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.058e+01 (batch 68)
  ... step 70/93  loss=0.3498  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.146e+01 (batch 70)
  ... step 72/93  loss=0.7591  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.339e+01 (batch 72)
  ... step 74/93  loss=1.5269  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.220e+01 (batch 74)
  ... step 76/93  loss=0.8534  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.316e+01 (batch 76)
  ... step 78/93  loss=0.9893  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.844e+01 (batch 78)
  ... step 80/93  loss=0.6779  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.347e+01 (batch 80)
  ... step 82/93  loss=0.4529  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.754e+00 (batch 82)
  ... step 84/93  loss=0.5962  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.694e+00 (batch 84)
  ... step 86/93  loss=0.4106  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.594e+00 (batch 86)
  ... step 88/93  loss=0.4637  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.224e+00 (batch 88)
  ... step 90/93  loss=0.3488  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.190e+00 (batch 90)
  ... step 92/93  loss=0.2514  (0.3s since last print)
  â†˜ grad L2 norm â‰ˆ 1.211e+01 (batch 92)
âœ… epoch 5 forward/backward done in 27.3s
  ðŸ”Ž val step 0: batch (4, 32, 2, 324, 1, 16) loss=34.6600
[05] train=0.7147  val=38.1689  RMSE(std)=[Qi:5.880, Qe:6.315, Î“:6.328]  RMSE(phys)=[Qi:427.031, Qe:584.469, Î“:279.444]  (1.7s)
âœ… Saved unified NN â†’ ./mnt/data/bin.cgyro.nn
ðŸŸ¦ Starting epoch 6/30 (train steps â‰ˆ 93)
  ... step 0/93  loss=0.7088  (0.1s since last print)
  â†˜ grad L2 norm â‰ˆ 1.050e+01 (batch 0)
  ... step 2/93  loss=0.8047  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.241e+01 (batch 2)
  ... step 4/93  loss=0.7082  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.426e+01 (batch 4)
  ... step 6/93  loss=0.4858  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.519e+00 (batch 6)
  ... step 8/93  loss=0.6647  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.169e+00 (batch 8)
  ... step 10/93  loss=0.5059  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.290e+00 (batch 10)
  ... step 12/93  loss=0.3917  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.504e+00 (batch 12)
  ... step 14/93  loss=0.6279  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.178e+00 (batch 14)
  ... step 16/93  loss=1.1274  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.395e+01 (batch 16)
  ... step 18/93  loss=0.3805  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.734e+00 (batch 18)
  ... step 20/93  loss=0.5871  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.052e+01 (batch 20)
  ... step 22/93  loss=0.8554  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.859e+01 (batch 22)
  ... step 24/93  loss=0.9124  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.019e+01 (batch 24)
  ... step 26/93  loss=1.1921  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.212e+01 (batch 26)
  ... step 28/93  loss=0.2843  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.823e+00 (batch 28)
  ... step 30/93  loss=0.1754  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.720e+00 (batch 30)
  ... step 32/93  loss=0.4968  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.559e+00 (batch 32)
  ... step 34/93  loss=0.8598  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.096e+01 (batch 34)
  ... step 36/93  loss=0.5085  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.129e+01 (batch 36)
  ... step 38/93  loss=0.7575  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.182e+01 (batch 38)
  ... step 40/93  loss=0.4605  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.620e+00 (batch 40)
  ... step 42/93  loss=0.8502  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.234e+01 (batch 42)
  ... step 44/93  loss=1.2899  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.548e+01 (batch 44)
  ... step 46/93  loss=1.8818  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.116e+01 (batch 46)
  ... step 48/93  loss=0.5983  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.133e+01 (batch 48)
  ... step 50/93  loss=0.6621  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.617e+01 (batch 50)
  ... step 52/93  loss=0.9717  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.529e+01 (batch 52)
  ... step 54/93  loss=0.5274  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.056e+01 (batch 54)
  ... step 56/93  loss=1.1207  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.132e+01 (batch 56)
  ... step 58/93  loss=0.4226  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.100e+00 (batch 58)
  ... step 60/93  loss=0.6537  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.119e+01 (batch 60)
  ... step 62/93  loss=0.7000  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.235e+01 (batch 62)
  ... step 64/93  loss=0.6185  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.480e+00 (batch 64)
  ... step 66/93  loss=0.5151  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.538e+00 (batch 66)
  ... step 68/93  loss=0.4277  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.517e+00 (batch 68)
  ... step 70/93  loss=0.5694  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.213e+00 (batch 70)
  ... step 72/93  loss=0.4037  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.470e+00 (batch 72)
  ... step 74/93  loss=0.5318  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.954e+00 (batch 74)
  ... step 76/93  loss=0.2016  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.731e+00 (batch 76)
  ... step 78/93  loss=0.7533  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.003e+01 (batch 78)
  ... step 80/93  loss=0.5274  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.198e+01 (batch 80)
  ... step 82/93  loss=0.2244  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.322e+00 (batch 82)
  ... step 84/93  loss=0.9514  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.257e+01 (batch 84)
  ... step 86/93  loss=0.3793  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.266e+00 (batch 86)
  ... step 88/93  loss=0.4332  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.546e+00 (batch 88)
  ... step 90/93  loss=0.4150  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.551e+00 (batch 90)
  ... step 92/93  loss=0.3200  (0.3s since last print)
  â†˜ grad L2 norm â‰ˆ 1.628e+01 (batch 92)
âœ… epoch 6 forward/backward done in 27.3s
  ðŸ”Ž val step 0: batch (4, 32, 2, 324, 1, 16) loss=6.2472
[06] train=0.6721  val=4.2258  RMSE(std)=[Qi:2.003, Qe:2.199, Î“:1.957]  RMSE(phys)=[Qi:145.464, Qe:203.476, Î“:86.435]  (1.7s)
âœ… Saved unified NN â†’ ./mnt/data/bin.cgyro.nn
ðŸŸ¦ Starting epoch 7/30 (train steps â‰ˆ 93)
  ... step 0/93  loss=0.5726  (0.1s since last print)
  â†˜ grad L2 norm â‰ˆ 7.426e+00 (batch 0)
  ... step 2/93  loss=0.5566  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.283e+00 (batch 2)
  ... step 4/93  loss=0.7729  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.300e+01 (batch 4)
  ... step 6/93  loss=0.9652  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.225e+01 (batch 6)
  ... step 8/93  loss=0.4106  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.701e+00 (batch 8)
  ... step 10/93  loss=0.1896  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.663e+00 (batch 10)
  ... step 12/93  loss=0.6224  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.943e+00 (batch 12)
  ... step 14/93  loss=0.4492  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.072e+00 (batch 14)
  ... step 16/93  loss=0.5495  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.257e+01 (batch 16)
  ... step 18/93  loss=0.6647  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.197e+01 (batch 18)
  ... step 20/93  loss=0.4475  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.475e+00 (batch 20)
  ... step 22/93  loss=0.5749  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.147e+01 (batch 22)
  ... step 24/93  loss=0.8959  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.310e+01 (batch 24)
  ... step 26/93  loss=2.2454  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.539e+01 (batch 26)
  ... step 28/93  loss=0.8276  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.619e+01 (batch 28)
  ... step 30/93  loss=0.7570  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.314e+01 (batch 30)
  ... step 32/93  loss=1.0647  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.337e+01 (batch 32)
  ... step 34/93  loss=0.2305  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.566e+00 (batch 34)
  ... step 36/93  loss=0.4437  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.018e+01 (batch 36)
  ... step 38/93  loss=1.1103  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.722e+01 (batch 38)
  ... step 40/93  loss=0.5971  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.327e+01 (batch 40)
  ... step 42/93  loss=0.9259  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.145e+01 (batch 42)
  ... step 44/93  loss=0.2692  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.371e+00 (batch 44)
  ... step 46/93  loss=0.3735  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.076e+00 (batch 46)
  ... step 48/93  loss=0.6200  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.001e+01 (batch 48)
  ... step 50/93  loss=0.4853  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.012e+01 (batch 50)
  ... step 52/93  loss=0.1428  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.016e+00 (batch 52)
  ... step 54/93  loss=0.7109  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.206e+01 (batch 54)
  ... step 56/93  loss=0.4082  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.880e+00 (batch 56)
  ... step 58/93  loss=0.5210  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.239e+00 (batch 58)
  ... step 60/93  loss=0.7697  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.018e+01 (batch 60)
  ... step 62/93  loss=0.5914  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.369e+01 (batch 62)
  ... step 64/93  loss=1.0337  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.507e+01 (batch 64)
  ... step 66/93  loss=0.3312  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.093e+00 (batch 66)
  ... step 68/93  loss=0.7366  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.144e+00 (batch 68)
  ... step 70/93  loss=0.9312  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.275e+01 (batch 70)
  ... step 72/93  loss=0.2156  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.828e+00 (batch 72)
  ... step 74/93  loss=0.7133  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.124e+01 (batch 74)
  ... step 76/93  loss=0.7484  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.488e+01 (batch 76)
  ... step 78/93  loss=0.4983  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.314e+00 (batch 78)
  ... step 80/93  loss=0.8452  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.065e+01 (batch 80)
  ... step 82/93  loss=0.3619  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.433e+00 (batch 82)
  ... step 84/93  loss=0.7965  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.184e+00 (batch 84)
  ... step 86/93  loss=0.6095  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.166e+01 (batch 86)
  ... step 88/93  loss=0.3017  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.234e+00 (batch 88)
  ... step 90/93  loss=1.4044  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.249e+01 (batch 90)
  ... step 92/93  loss=0.5439  (0.3s since last print)
  â†˜ grad L2 norm â‰ˆ 2.180e+01 (batch 92)
âœ… epoch 7 forward/backward done in 27.3s
  ðŸ”Ž val step 0: batch (4, 32, 2, 324, 1, 16) loss=1.9054
[07] train=0.6618  val=0.9116  RMSE(std)=[Qi:0.922, Qe:1.007, Î“:0.933]  RMSE(phys)=[Qi:66.954, Qe:93.175, Î“:41.216]  (1.7s)
âœ… Saved unified NN â†’ ./mnt/data/bin.cgyro.nn
ðŸŸ¦ Starting epoch 8/30 (train steps â‰ˆ 93)
  ... step 0/93  loss=0.6727  (0.1s since last print)
  â†˜ grad L2 norm â‰ˆ 7.158e+00 (batch 0)
  ... step 2/93  loss=0.2420  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.983e+00 (batch 2)
  ... step 4/93  loss=0.4499  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.094e+00 (batch 4)
  ... step 6/93  loss=0.9798  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.402e+01 (batch 6)
  ... step 8/93  loss=0.6291  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.403e+00 (batch 8)
  ... step 10/93  loss=0.2151  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.838e+00 (batch 10)
  ... step 12/93  loss=0.6080  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.478e+00 (batch 12)
  ... step 14/93  loss=1.1138  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.224e+01 (batch 14)
  ... step 16/93  loss=0.9929  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.929e+00 (batch 16)
  ... step 18/93  loss=0.5757  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.811e+00 (batch 18)
  ... step 20/93  loss=0.4048  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.116e+00 (batch 20)
  ... step 22/93  loss=0.3725  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.917e+00 (batch 22)
  ... step 24/93  loss=1.1475  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.255e+01 (batch 24)
  ... step 26/93  loss=0.9283  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.051e+01 (batch 26)
  ... step 28/93  loss=0.5079  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.437e+00 (batch 28)
  ... step 30/93  loss=1.0316  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.723e+01 (batch 30)
  ... step 32/93  loss=0.7359  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.797e+00 (batch 32)
  ... step 34/93  loss=0.2516  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.522e+00 (batch 34)
  ... step 36/93  loss=0.5156  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.884e+00 (batch 36)
  ... step 38/93  loss=0.6626  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.511e+00 (batch 38)
  ... step 40/93  loss=0.2522  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.894e+00 (batch 40)
  ... step 42/93  loss=0.6178  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.114e+01 (batch 42)
  ... step 44/93  loss=0.9985  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.121e+01 (batch 44)
  ... step 46/93  loss=0.2039  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.147e+00 (batch 46)
  ... step 48/93  loss=0.1592  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.285e+00 (batch 48)
  ... step 50/93  loss=0.2568  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.042e+00 (batch 50)
  ... step 52/93  loss=0.3759  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.526e+00 (batch 52)
  ... step 54/93  loss=0.5458  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.071e+01 (batch 54)
  ... step 56/93  loss=0.8815  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.355e+01 (batch 56)
  ... step 58/93  loss=0.7933  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.988e+01 (batch 58)
  ... step 60/93  loss=0.4384  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.709e+00 (batch 60)
  ... step 62/93  loss=0.8223  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.014e+01 (batch 62)
  ... step 64/93  loss=0.9647  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.268e+01 (batch 64)
  ... step 66/93  loss=0.3664  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.854e+00 (batch 66)
  ... step 68/93  loss=0.5122  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.314e+00 (batch 68)
  ... step 70/93  loss=0.2151  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.059e+00 (batch 70)
  ... step 72/93  loss=0.4381  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.287e+00 (batch 72)
  ... step 74/93  loss=0.3686  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.277e+00 (batch 74)
  ... step 76/93  loss=0.4127  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.745e+00 (batch 76)
  ... step 78/93  loss=1.3333  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.542e+01 (batch 78)
  ... step 80/93  loss=0.4970  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.017e+01 (batch 80)
  ... step 82/93  loss=0.2699  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.507e+00 (batch 82)
  ... step 84/93  loss=0.8838  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.542e+01 (batch 84)
  ... step 86/93  loss=1.0015  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.007e+01 (batch 86)
  ... step 88/93  loss=0.8420  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.256e+01 (batch 88)
  ... step 90/93  loss=0.2567  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.312e+00 (batch 90)
  ... step 92/93  loss=0.0721  (0.3s since last print)
  â†˜ grad L2 norm â‰ˆ 3.935e+00 (batch 92)
âœ… epoch 8 forward/backward done in 27.3s
  ðŸ”Ž val step 0: batch (4, 32, 2, 324, 1, 16) loss=2.9815
[08] train=0.6025  val=1.2468  RMSE(std)=[Qi:1.109, Qe:1.180, Î“:1.057]  RMSE(phys)=[Qi:80.570, Qe:109.230, Î“:46.663]  (1.7s)
âœ… Saved unified NN â†’ ./mnt/data/bin.cgyro.nn
ðŸŸ¦ Starting epoch 9/30 (train steps â‰ˆ 93)
  ... step 0/93  loss=0.3087  (0.1s since last print)
  â†˜ grad L2 norm â‰ˆ 4.855e+00 (batch 0)
  ... step 2/93  loss=0.9941  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.215e+01 (batch 2)
  ... step 4/93  loss=0.4395  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.324e+00 (batch 4)
  ... step 6/93  loss=0.6776  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.190e+01 (batch 6)
  ... step 8/93  loss=0.3356  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.591e+00 (batch 8)
  ... step 10/93  loss=0.5519  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.207e+00 (batch 10)
  ... step 12/93  loss=0.5393  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.899e+00 (batch 12)
  ... step 14/93  loss=0.5471  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.040e+01 (batch 14)
  ... step 16/93  loss=0.6043  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.083e+00 (batch 16)
  ... step 18/93  loss=0.1459  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.165e+00 (batch 18)
  ... step 20/93  loss=0.6938  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.669e+00 (batch 20)
  ... step 22/93  loss=0.6004  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.158e+01 (batch 22)
  ... step 24/93  loss=0.5897  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.133e+01 (batch 24)
  ... step 26/93  loss=0.6057  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.445e+00 (batch 26)
  ... step 28/93  loss=0.8483  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.111e+01 (batch 28)
  ... step 30/93  loss=1.7376  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.338e+01 (batch 30)
  ... step 32/93  loss=0.4294  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.345e+01 (batch 32)
  ... step 34/93  loss=0.9202  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.262e+01 (batch 34)
  ... step 36/93  loss=0.4000  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.930e+00 (batch 36)
  ... step 38/93  loss=0.4518  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.431e+00 (batch 38)
  ... step 40/93  loss=0.8473  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.035e+01 (batch 40)
  ... step 42/93  loss=0.2958  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.377e+00 (batch 42)
  ... step 44/93  loss=0.5763  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.678e+00 (batch 44)
  ... step 46/93  loss=0.3618  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.346e+00 (batch 46)
  ... step 48/93  loss=0.7607  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.059e+00 (batch 48)
  ... step 50/93  loss=0.5370  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.613e+00 (batch 50)
  ... step 52/93  loss=0.6273  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.773e+00 (batch 52)
  ... step 54/93  loss=0.7483  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.607e+00 (batch 54)
  ... step 56/93  loss=0.1543  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.845e+00 (batch 56)
  ... step 58/93  loss=0.1753  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.685e+00 (batch 58)
  ... step 60/93  loss=1.0183  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.230e+01 (batch 60)
  ... step 62/93  loss=1.3384  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.555e+01 (batch 62)
  ... step 64/93  loss=0.3401  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.478e+00 (batch 64)
  ... step 66/93  loss=0.2648  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.956e+00 (batch 66)
  ... step 68/93  loss=0.5371  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.016e+01 (batch 68)
  ... step 70/93  loss=0.7246  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.078e+00 (batch 70)
  ... step 72/93  loss=0.2097  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.823e+00 (batch 72)
  ... step 74/93  loss=0.4864  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.442e+00 (batch 74)
  ... step 76/93  loss=0.5740  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.667e+00 (batch 76)
  ... step 78/93  loss=0.2127  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.313e+00 (batch 78)
  ... step 80/93  loss=0.5041  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.095e+01 (batch 80)
  ... step 82/93  loss=0.6178  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.005e+00 (batch 82)
  ... step 84/93  loss=0.1687  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.843e+00 (batch 84)
  ... step 86/93  loss=0.4394  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.101e+00 (batch 86)
  ... step 88/93  loss=1.7128  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.768e+01 (batch 88)
  ... step 90/93  loss=0.5172  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.492e+00 (batch 90)
  ... step 92/93  loss=0.3637  (0.3s since last print)
  â†˜ grad L2 norm â‰ˆ 1.773e+01 (batch 92)
âœ… epoch 9 forward/backward done in 27.3s
  ðŸ”Ž val step 0: batch (4, 32, 2, 324, 1, 16) loss=4.3713
[09] train=0.6135  val=1.9480  RMSE(std)=[Qi:1.380, Qe:1.435, Î“:1.371]  RMSE(phys)=[Qi:100.197, Qe:132.815, Î“:60.560]  (1.7s)
âœ… Saved unified NN â†’ ./mnt/data/bin.cgyro.nn
ðŸŸ¦ Starting epoch 10/30 (train steps â‰ˆ 93)
  ... step 0/93  loss=0.4182  (0.1s since last print)
  â†˜ grad L2 norm â‰ˆ 9.603e+00 (batch 0)
  ... step 2/93  loss=0.5966  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.850e+00 (batch 2)
  ... step 4/93  loss=0.6601  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.371e+00 (batch 4)
  ... step 6/93  loss=0.4112  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.711e+00 (batch 6)
  ... step 8/93  loss=0.3281  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.774e+00 (batch 8)
  ... step 10/93  loss=0.7587  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.399e+01 (batch 10)
  ... step 12/93  loss=0.6388  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.361e+00 (batch 12)
  ... step 14/93  loss=0.8794  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.345e+01 (batch 14)
  ... step 16/93  loss=0.7851  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.130e+01 (batch 16)
  ... step 18/93  loss=0.7276  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.515e+00 (batch 18)
  ... step 20/93  loss=0.6579  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.060e+01 (batch 20)
  ... step 22/93  loss=0.5863  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.069e+01 (batch 22)
  ... step 24/93  loss=0.6803  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.063e+01 (batch 24)
  ... step 26/93  loss=1.0063  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.792e+00 (batch 26)
  ... step 28/93  loss=0.8724  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.433e+00 (batch 28)
  ... step 30/93  loss=0.6191  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.886e+00 (batch 30)
  ... step 32/93  loss=0.4172  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.321e+00 (batch 32)
  ... step 34/93  loss=0.4308  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.638e+00 (batch 34)
  ... step 36/93  loss=0.1904  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.721e+00 (batch 36)
  ... step 38/93  loss=0.9508  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.331e+01 (batch 38)
  ... step 40/93  loss=0.4998  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.628e+00 (batch 40)
  ... step 42/93  loss=0.3189  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.955e+00 (batch 42)
  ... step 44/93  loss=0.3314  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.021e+00 (batch 44)
  ... step 46/93  loss=0.4194  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.174e+00 (batch 46)
  ... step 48/93  loss=0.6455  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.126e+01 (batch 48)
  ... step 50/93  loss=0.6262  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.700e+00 (batch 50)
  ... step 52/93  loss=0.7702  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.499e+00 (batch 52)
  ... step 54/93  loss=0.6903  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.550e+00 (batch 54)
  ... step 56/93  loss=0.1730  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.284e+00 (batch 56)
  ... step 58/93  loss=0.5279  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.611e+00 (batch 58)
  ... step 60/93  loss=0.2245  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.842e+00 (batch 60)
  ... step 62/93  loss=0.8118  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.174e+01 (batch 62)
  ... step 64/93  loss=0.4078  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.427e+00 (batch 64)
  ... step 66/93  loss=0.3806  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.968e+00 (batch 66)
  ... step 68/93  loss=0.4834  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.829e+00 (batch 68)
  ... step 70/93  loss=0.2692  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.408e+00 (batch 70)
  ... step 72/93  loss=0.4974  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.128e+00 (batch 72)
  ... step 74/93  loss=0.2463  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.761e+00 (batch 74)
  ... step 76/93  loss=1.2241  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.300e+01 (batch 76)
  ... step 78/93  loss=0.9355  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.012e+01 (batch 78)
  ... step 80/93  loss=0.6662  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.035e+01 (batch 80)
  ... step 82/93  loss=0.7694  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.018e+00 (batch 82)
  ... step 84/93  loss=0.5035  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.758e+00 (batch 84)
  ... step 86/93  loss=0.4841  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.178e+01 (batch 86)
  ... step 88/93  loss=0.9935  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.038e+01 (batch 88)
  ... step 90/93  loss=1.0063  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.439e+01 (batch 90)
  ... step 92/93  loss=0.6950  (0.3s since last print)
  â†˜ grad L2 norm â‰ˆ 1.453e+01 (batch 92)
âœ… epoch 10 forward/backward done in 27.3s
  ðŸ”Ž val step 0: batch (4, 32, 2, 324, 1, 16) loss=1.6931
[10] train=0.6330  val=0.8412  RMSE(std)=[Qi:0.889, Qe:0.953, Î“:0.909]  RMSE(phys)=[Qi:64.531, Qe:88.216, Î“:40.121]  (1.7s)
âœ… Saved unified NN â†’ ./mnt/data/bin.cgyro.nn
ðŸŸ¦ Starting epoch 11/30 (train steps â‰ˆ 93)
  ... step 0/93  loss=0.9514  (0.1s since last print)
  â†˜ grad L2 norm â‰ˆ 1.247e+01 (batch 0)
  ... step 2/93  loss=0.2621  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.154e+00 (batch 2)
  ... step 4/93  loss=0.4223  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.257e+01 (batch 4)
  ... step 6/93  loss=0.5449  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.116e+01 (batch 6)
  ... step 8/93  loss=0.4319  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.586e+00 (batch 8)
  ... step 10/93  loss=0.6973  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.003e+01 (batch 10)
  ... step 12/93  loss=0.2899  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.483e+00 (batch 12)
  ... step 14/93  loss=0.3938  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.627e+00 (batch 14)
  ... step 16/93  loss=0.6664  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.062e+01 (batch 16)
  ... step 18/93  loss=1.1825  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.236e+01 (batch 18)
  ... step 20/93  loss=0.3625  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.487e+00 (batch 20)
  ... step 22/93  loss=0.8804  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.905e+00 (batch 22)
  ... step 24/93  loss=0.5908  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.086e+00 (batch 24)
  ... step 26/93  loss=0.6838  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.755e+00 (batch 26)
  ... step 28/93  loss=0.6726  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.417e+00 (batch 28)
  ... step 30/93  loss=0.3364  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.587e+00 (batch 30)
  ... step 32/93  loss=0.5203  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.642e+00 (batch 32)
  ... step 34/93  loss=0.3218  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.479e+00 (batch 34)
  ... step 36/93  loss=0.4873  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.969e+00 (batch 36)
  ... step 38/93  loss=0.6447  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.570e+00 (batch 38)
  ... step 40/93  loss=0.5583  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.472e+00 (batch 40)
  ... step 42/93  loss=0.4509  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.721e+00 (batch 42)
  ... step 44/93  loss=0.9071  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.225e+01 (batch 44)
  ... step 46/93  loss=1.4162  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.423e+01 (batch 46)
  ... step 48/93  loss=0.6230  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.212e+00 (batch 48)
  ... step 50/93  loss=0.7853  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.140e+01 (batch 50)
  ... step 52/93  loss=2.1547  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.291e+01 (batch 52)
  ... step 54/93  loss=0.7394  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.040e+01 (batch 54)
  ... step 56/93  loss=0.4314  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.158e+01 (batch 56)
  ... step 58/93  loss=0.2324  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.723e+00 (batch 58)
  ... step 60/93  loss=0.1882  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.116e+00 (batch 60)
  ... step 62/93  loss=0.9706  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.477e+00 (batch 62)
  ... step 64/93  loss=0.2283  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.070e+00 (batch 64)
  ... step 66/93  loss=0.2631  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.902e+00 (batch 66)
  ... step 68/93  loss=1.0962  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.179e+01 (batch 68)
  ... step 70/93  loss=1.9198  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.557e+01 (batch 70)
  ... step 72/93  loss=1.6586  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.178e+01 (batch 72)
  ... step 74/93  loss=0.9334  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.193e+00 (batch 74)
  ... step 76/93  loss=0.2151  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.227e+00 (batch 76)
  ... step 78/93  loss=0.2391  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.008e+00 (batch 78)
  ... step 80/93  loss=1.0221  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.582e+01 (batch 80)
  ... step 82/93  loss=0.6723  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.861e+00 (batch 82)
  ... step 84/93  loss=0.2158  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.684e+00 (batch 84)
  ... step 86/93  loss=0.5547  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.122e+00 (batch 86)
  ... step 88/93  loss=0.2038  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.665e+00 (batch 88)
  ... step 90/93  loss=0.7098  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.769e+00 (batch 90)
  ... step 92/93  loss=0.4831  (0.3s since last print)
  â†˜ grad L2 norm â‰ˆ 2.141e+01 (batch 92)
âœ… epoch 11 forward/backward done in 27.3s
  ðŸ”Ž val step 0: batch (4, 32, 2, 324, 1, 16) loss=1.1127
[11] train=0.6201  val=0.7474  RMSE(std)=[Qi:0.835, Qe:0.911, Î“:0.845]  RMSE(phys)=[Qi:60.671, Qe:84.307, Î“:37.321]  (1.7s)
âœ… Saved unified NN â†’ ./mnt/data/bin.cgyro.nn
ðŸŸ¦ Starting epoch 12/30 (train steps â‰ˆ 93)
  ... step 0/93  loss=0.3528  (0.1s since last print)
  â†˜ grad L2 norm â‰ˆ 5.655e+00 (batch 0)
  ... step 2/93  loss=0.8300  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.829e+00 (batch 2)
  ... step 4/93  loss=0.5006  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.085e+00 (batch 4)
  ... step 6/93  loss=0.4893  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.149e+00 (batch 6)
  ... step 8/93  loss=0.2519  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.766e+00 (batch 8)
  ... step 10/93  loss=0.3430  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.374e+00 (batch 10)
  ... step 12/93  loss=0.3600  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.098e+00 (batch 12)
  ... step 14/93  loss=1.1116  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.393e+01 (batch 14)
  ... step 16/93  loss=0.8855  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.015e+01 (batch 16)
  ... step 18/93  loss=0.6787  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.826e+00 (batch 18)
  ... step 20/93  loss=0.4664  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.963e+00 (batch 20)
  ... step 22/93  loss=0.2390  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.937e+00 (batch 22)
  ... step 24/93  loss=0.1876  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.581e+00 (batch 24)
  ... step 26/93  loss=0.1655  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.479e+00 (batch 26)
  ... step 28/93  loss=0.8811  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.874e+00 (batch 28)
  ... step 30/93  loss=0.4994  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.366e+00 (batch 30)
  ... step 32/93  loss=0.5803  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.714e+00 (batch 32)
  ... step 34/93  loss=0.1379  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.747e+00 (batch 34)
  ... step 36/93  loss=0.7436  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.141e+01 (batch 36)
  ... step 38/93  loss=0.3439  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.508e+00 (batch 38)
  ... step 40/93  loss=0.6165  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.078e+00 (batch 40)
  ... step 42/93  loss=0.3310  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.144e+00 (batch 42)
  ... step 44/93  loss=0.3985  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.429e+00 (batch 44)
  ... step 46/93  loss=0.1361  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.150e+00 (batch 46)
  ... step 48/93  loss=0.3741  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.415e+00 (batch 48)
  ... step 50/93  loss=0.6758  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.048e+01 (batch 50)
  ... step 52/93  loss=0.1624  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.212e+00 (batch 52)
  ... step 54/93  loss=1.0942  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.732e+00 (batch 54)
  ... step 56/93  loss=0.2067  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.193e+00 (batch 56)
  ... step 58/93  loss=0.3611  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.992e+00 (batch 58)
  ... step 60/93  loss=0.6204  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.455e+00 (batch 60)
  ... step 62/93  loss=0.5080  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.612e+00 (batch 62)
  ... step 64/93  loss=0.4279  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.273e+00 (batch 64)
  ... step 66/93  loss=0.3580  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.226e+00 (batch 66)
  ... step 68/93  loss=1.0545  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.149e+01 (batch 68)
  ... step 70/93  loss=0.3760  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.582e+00 (batch 70)
  ... step 72/93  loss=0.3747  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.436e+00 (batch 72)
  ... step 74/93  loss=0.7079  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.368e+00 (batch 74)
  ... step 76/93  loss=0.3544  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.443e+00 (batch 76)
  ... step 78/93  loss=0.6598  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.006e+01 (batch 78)
  ... step 80/93  loss=0.8742  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.431e+01 (batch 80)
  ... step 82/93  loss=0.8159  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.585e+00 (batch 82)
  ... step 84/93  loss=1.1107  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.242e+01 (batch 84)
  ... step 86/93  loss=0.5432  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.662e+00 (batch 86)
  ... step 88/93  loss=1.0724  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.098e+01 (batch 88)
  ... step 90/93  loss=0.1501  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.458e+00 (batch 90)
  ... step 92/93  loss=0.8699  (0.3s since last print)
  â†˜ grad L2 norm â‰ˆ 1.785e+01 (batch 92)
âœ… epoch 12 forward/backward done in 27.3s
  ðŸ”Ž val step 0: batch (4, 32, 2, 324, 1, 16) loss=1.0601
[12] train=0.5574  val=0.7283  RMSE(std)=[Qi:0.827, Qe:0.898, Î“:0.833]  RMSE(phys)=[Qi:60.071, Qe:83.093, Î“:36.802]  (1.7s)
âœ… Saved unified NN â†’ ./mnt/data/bin.cgyro.nn
ðŸŸ¦ Starting epoch 13/30 (train steps â‰ˆ 93)
  ... step 0/93  loss=0.4799  (0.1s since last print)
  â†˜ grad L2 norm â‰ˆ 7.786e+00 (batch 0)
  ... step 2/93  loss=0.2590  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.848e+00 (batch 2)
  ... step 4/93  loss=0.8853  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.315e+00 (batch 4)
  ... step 6/93  loss=0.3060  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.414e+00 (batch 6)
  ... step 8/93  loss=0.5701  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.431e+00 (batch 8)
  ... step 10/93  loss=0.3305  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.961e+00 (batch 10)
  ... step 12/93  loss=0.3820  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.348e+00 (batch 12)
  ... step 14/93  loss=1.3698  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.818e+01 (batch 14)
  ... step 16/93  loss=1.7173  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.511e+01 (batch 16)
  ... step 18/93  loss=0.5141  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.097e+00 (batch 18)
  ... step 20/93  loss=0.4559  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.581e+00 (batch 20)
  ... step 22/93  loss=0.4603  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.011e+00 (batch 22)
  ... step 24/93  loss=0.1185  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.286e+00 (batch 24)
  ... step 26/93  loss=0.2207  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.459e+00 (batch 26)
  ... step 28/93  loss=0.8910  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.420e+01 (batch 28)
  ... step 30/93  loss=0.5427  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.194e+01 (batch 30)
  ... step 32/93  loss=0.6357  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.240e+00 (batch 32)
  ... step 34/93  loss=0.8753  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.435e+01 (batch 34)
  ... step 36/93  loss=0.4956  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.224e+00 (batch 36)
  ... step 38/93  loss=0.4956  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.064e+00 (batch 38)
  ... step 40/93  loss=0.6145  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.085e+01 (batch 40)
  ... step 42/93  loss=0.2872  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.310e+00 (batch 42)
  ... step 44/93  loss=0.4491  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.099e+01 (batch 44)
  ... step 46/93  loss=0.6054  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.309e+00 (batch 46)
  ... step 48/93  loss=1.0732  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.030e+01 (batch 48)
  ... step 50/93  loss=0.7123  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.895e+00 (batch 50)
  ... step 52/93  loss=0.7032  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.887e+00 (batch 52)
  ... step 54/93  loss=0.3339  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.341e+00 (batch 54)
  ... step 56/93  loss=0.5445  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.020e+00 (batch 56)
  ... step 58/93  loss=0.5807  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.025e+00 (batch 58)
  ... step 60/93  loss=0.2040  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.907e+00 (batch 60)
  ... step 62/93  loss=0.3879  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.275e+00 (batch 62)
  ... step 64/93  loss=0.4553  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.144e+01 (batch 64)
  ... step 66/93  loss=0.2296  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.712e+00 (batch 66)
  ... step 68/93  loss=0.7652  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.428e+01 (batch 68)
  ... step 70/93  loss=0.6835  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.044e+01 (batch 70)
  ... step 72/93  loss=0.9302  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.277e+01 (batch 72)
  ... step 74/93  loss=1.0801  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.391e+01 (batch 74)
  ... step 76/93  loss=1.2264  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.111e+01 (batch 76)
  ... step 78/93  loss=0.7166  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.686e+00 (batch 78)
  ... step 80/93  loss=0.1663  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.518e+00 (batch 80)
  ... step 82/93  loss=0.6924  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.090e+01 (batch 82)
  ... step 84/93  loss=0.9683  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.207e+01 (batch 84)
  ... step 86/93  loss=0.3615  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.649e+00 (batch 86)
  ... step 88/93  loss=0.8207  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.166e+01 (batch 88)
  ... step 90/93  loss=0.1830  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.356e+00 (batch 90)
  ... step 92/93  loss=0.4271  (0.3s since last print)
  â†˜ grad L2 norm â‰ˆ 1.589e+01 (batch 92)
âœ… epoch 13 forward/backward done in 27.3s
  ðŸ”Ž val step 0: batch (4, 32, 2, 324, 1, 16) loss=1.1304
[13] train=0.5667  val=0.7273  RMSE(std)=[Qi:0.829, Qe:0.896, Î“:0.831]  RMSE(phys)=[Qi:60.226, Qe:82.933, Î“:36.713]  (1.7s)
âœ… Saved unified NN â†’ ./mnt/data/bin.cgyro.nn
ðŸŸ¦ Starting epoch 14/30 (train steps â‰ˆ 93)
  ... step 0/93  loss=0.3231  (0.1s since last print)
  â†˜ grad L2 norm â‰ˆ 5.627e+00 (batch 0)
  ... step 2/93  loss=0.4548  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.991e+00 (batch 2)
  ... step 4/93  loss=0.6992  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.092e+00 (batch 4)
  ... step 6/93  loss=0.7097  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.683e+00 (batch 6)
  ... step 8/93  loss=0.4934  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.473e+00 (batch 8)
  ... step 10/93  loss=0.2483  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.305e+00 (batch 10)
  ... step 12/93  loss=0.2612  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.299e+00 (batch 12)
  ... step 14/93  loss=0.3003  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.234e+00 (batch 14)
  ... step 16/93  loss=0.5963  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.542e+00 (batch 16)
  ... step 18/93  loss=0.4320  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.371e+00 (batch 18)
  ... step 20/93  loss=0.6927  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.977e+00 (batch 20)
  ... step 22/93  loss=0.6482  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.427e+00 (batch 22)
  ... step 24/93  loss=0.1982  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.632e+00 (batch 24)
  ... step 26/93  loss=0.4914  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.190e+00 (batch 26)
  ... step 28/93  loss=1.7998  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.838e+01 (batch 28)
  ... step 30/93  loss=0.3271  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.533e+00 (batch 30)
  ... step 32/93  loss=1.0610  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.480e+01 (batch 32)
  ... step 34/93  loss=0.2020  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.075e+00 (batch 34)
  ... step 36/93  loss=0.7212  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.242e+00 (batch 36)
  ... step 38/93  loss=0.6084  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.841e+00 (batch 38)
  ... step 40/93  loss=0.7276  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.074e+01 (batch 40)
  ... step 42/93  loss=0.3477  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.675e+00 (batch 42)
  ... step 44/93  loss=0.2526  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.432e+00 (batch 44)
  ... step 46/93  loss=0.3408  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.391e+00 (batch 46)
  ... step 48/93  loss=0.3033  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.581e+00 (batch 48)
  ... step 50/93  loss=0.7073  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.237e+00 (batch 50)
  ... step 52/93  loss=0.1424  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.520e+00 (batch 52)
  ... step 54/93  loss=0.5392  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.289e+00 (batch 54)
  ... step 56/93  loss=0.1213  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.197e+00 (batch 56)
  ... step 58/93  loss=1.1838  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.343e+01 (batch 58)
  ... step 60/93  loss=0.5029  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.105e+00 (batch 60)
  ... step 62/93  loss=0.4684  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.125e+00 (batch 62)
  ... step 64/93  loss=0.4123  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.568e+00 (batch 64)
  ... step 66/93  loss=0.3033  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.338e+00 (batch 66)
  ... step 68/93  loss=0.4656  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.882e+00 (batch 68)
  ... step 70/93  loss=0.2423  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.980e+00 (batch 70)
  ... step 72/93  loss=0.2609  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.608e+00 (batch 72)
  ... step 74/93  loss=0.3292  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.922e+00 (batch 74)
  ... step 76/93  loss=0.2044  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.538e+00 (batch 76)
  ... step 78/93  loss=0.4409  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.390e+00 (batch 78)
  ... step 80/93  loss=0.3141  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.413e+00 (batch 80)
  ... step 82/93  loss=1.2344  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.253e+01 (batch 82)
  ... step 84/93  loss=0.2038  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.121e+00 (batch 84)
  ... step 86/93  loss=0.3426  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.575e+00 (batch 86)
  ... step 88/93  loss=0.6108  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.985e+00 (batch 88)
  ... step 90/93  loss=0.2200  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.917e+00 (batch 90)
  ... step 92/93  loss=1.5030  (0.3s since last print)
  â†˜ grad L2 norm â‰ˆ 1.751e+01 (batch 92)
âœ… epoch 14 forward/backward done in 27.3s
  ðŸ”Ž val step 0: batch (4, 32, 2, 324, 1, 16) loss=0.5966
[14] train=0.5396  val=0.8064  RMSE(std)=[Qi:0.868, Qe:0.933, Î“:0.892]  RMSE(phys)=[Qi:63.009, Qe:86.357, Î“:39.389]  (1.7s)
âœ… Saved unified NN â†’ ./mnt/data/bin.cgyro.nn
ðŸŸ¦ Starting epoch 15/30 (train steps â‰ˆ 93)
  ... step 0/93  loss=0.4235  (0.1s since last print)
  â†˜ grad L2 norm â‰ˆ 6.017e+00 (batch 0)
  ... step 2/93  loss=0.6411  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.157e+00 (batch 2)
  ... step 4/93  loss=0.3334  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.836e+00 (batch 4)
  ... step 6/93  loss=0.5665  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.031e+00 (batch 6)
  ... step 8/93  loss=0.2544  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.153e+00 (batch 8)
  ... step 10/93  loss=0.6515  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.023e+00 (batch 10)
  ... step 12/93  loss=0.2178  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.617e+00 (batch 12)
  ... step 14/93  loss=0.5788  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.653e+00 (batch 14)
  ... step 16/93  loss=0.2093  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.494e+00 (batch 16)
  ... step 18/93  loss=0.7957  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.709e+00 (batch 18)
  ... step 20/93  loss=0.3220  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.021e+00 (batch 20)
  ... step 22/93  loss=0.6684  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.363e+00 (batch 22)
  ... step 24/93  loss=0.1547  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.094e+00 (batch 24)
  ... step 26/93  loss=0.4457  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.822e+00 (batch 26)
  ... step 28/93  loss=0.6703  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.048e+00 (batch 28)
  ... step 30/93  loss=0.7138  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.930e+00 (batch 30)
  ... step 32/93  loss=0.2290  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.552e+00 (batch 32)
  ... step 34/93  loss=0.6779  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.417e+00 (batch 34)
  ... step 36/93  loss=0.7346  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.926e+00 (batch 36)
  ... step 38/93  loss=0.1388  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.981e+00 (batch 38)
  ... step 40/93  loss=0.8625  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.232e+01 (batch 40)
  ... step 42/93  loss=0.4935  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.057e+01 (batch 42)
  ... step 44/93  loss=0.3897  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.518e+00 (batch 44)
  ... step 46/93  loss=0.3620  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.981e+00 (batch 46)
  ... step 48/93  loss=1.0773  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.399e+01 (batch 48)
  ... step 50/93  loss=0.3537  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.012e+01 (batch 50)
  ... step 52/93  loss=0.2407  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.433e+00 (batch 52)
  ... step 54/93  loss=0.5200  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.794e+00 (batch 54)
  ... step 56/93  loss=0.3994  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.259e+00 (batch 56)
  ... step 58/93  loss=0.1800  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.226e+00 (batch 58)
  ... step 60/93  loss=1.2636  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.249e+01 (batch 60)
  ... step 62/93  loss=0.1845  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.173e+00 (batch 62)
  ... step 64/93  loss=0.6861  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.053e+01 (batch 64)
  ... step 66/93  loss=0.4981  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.157e+00 (batch 66)
  ... step 68/93  loss=0.3010  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.606e+00 (batch 68)
  ... step 70/93  loss=0.2432  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.704e+00 (batch 70)
  ... step 72/93  loss=0.5258  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.022e+01 (batch 72)
  ... step 74/93  loss=0.3550  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.519e+00 (batch 74)
  ... step 76/93  loss=0.4878  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.290e+00 (batch 76)
  ... step 78/93  loss=1.1097  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.174e+00 (batch 78)
  ... step 80/93  loss=0.2876  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.317e+00 (batch 80)
  ... step 82/93  loss=0.5700  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.661e+00 (batch 82)
  ... step 84/93  loss=0.4991  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.113e+00 (batch 84)
  ... step 86/93  loss=0.5782  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.506e+00 (batch 86)
  ... step 88/93  loss=0.8235  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.974e+00 (batch 88)
  ... step 90/93  loss=1.2561  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.311e+01 (batch 90)
  ... step 92/93  loss=0.3796  (0.3s since last print)
  â†˜ grad L2 norm â‰ˆ 1.212e+01 (batch 92)
âœ… epoch 15 forward/backward done in 27.3s
  ðŸ”Ž val step 0: batch (4, 32, 2, 324, 1, 16) loss=0.7450
[15] train=0.5120  val=0.8279  RMSE(std)=[Qi:0.871, Qe:0.948, Î“:0.908]  RMSE(phys)=[Qi:63.281, Qe:87.756, Î“:40.116]  (1.7s)
âœ… Saved unified NN â†’ ./mnt/data/bin.cgyro.nn
ðŸŸ¦ Starting epoch 16/30 (train steps â‰ˆ 93)
  ... step 0/93  loss=0.4321  (0.1s since last print)
  â†˜ grad L2 norm â‰ˆ 9.226e+00 (batch 0)
  ... step 2/93  loss=0.2343  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.642e+00 (batch 2)
  ... step 4/93  loss=0.7845  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.769e+00 (batch 4)
  ... step 6/93  loss=0.2038  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.078e+00 (batch 6)
  ... step 8/93  loss=0.5840  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.627e+00 (batch 8)
  ... step 10/93  loss=0.7026  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.623e+00 (batch 10)
  ... step 12/93  loss=1.0891  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.096e+01 (batch 12)
  ... step 14/93  loss=0.3999  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.833e+00 (batch 14)
  ... step 16/93  loss=0.9787  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.127e+01 (batch 16)
  ... step 18/93  loss=0.1731  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.373e+00 (batch 18)
  ... step 20/93  loss=0.5186  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.448e+00 (batch 20)
  ... step 22/93  loss=0.5493  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.116e+00 (batch 22)
  ... step 24/93  loss=0.7320  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.018e+00 (batch 24)
  ... step 26/93  loss=0.6848  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.567e+00 (batch 26)
  ... step 28/93  loss=0.3163  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.252e+00 (batch 28)
  ... step 30/93  loss=0.3678  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.205e+00 (batch 30)
  ... step 32/93  loss=0.3956  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.579e+00 (batch 32)
  ... step 34/93  loss=0.8577  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.493e+01 (batch 34)
  ... step 36/93  loss=0.4766  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.492e+00 (batch 36)
  ... step 38/93  loss=0.6460  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.369e+00 (batch 38)
  ... step 40/93  loss=0.2768  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.799e+00 (batch 40)
  ... step 42/93  loss=0.5464  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.076e+00 (batch 42)
  ... step 44/93  loss=0.4712  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.506e+00 (batch 44)
  ... step 46/93  loss=0.2204  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.627e+00 (batch 46)
  ... step 48/93  loss=0.3390  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.741e+00 (batch 48)
  ... step 50/93  loss=0.2616  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.072e+00 (batch 50)
  ... step 52/93  loss=0.5617  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.495e+00 (batch 52)
  ... step 54/93  loss=0.1524  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.610e+00 (batch 54)
  ... step 56/93  loss=0.6367  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.008e+01 (batch 56)
  ... step 58/93  loss=0.4088  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.325e+00 (batch 58)
  ... step 60/93  loss=0.2421  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.864e+00 (batch 60)
  ... step 62/93  loss=0.2563  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.537e+00 (batch 62)
  ... step 64/93  loss=0.4878  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.766e+00 (batch 64)
  ... step 66/93  loss=0.5621  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.737e+00 (batch 66)
  ... step 68/93  loss=0.2490  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.018e+00 (batch 68)
  ... step 70/93  loss=0.6767  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.003e+01 (batch 70)
  ... step 72/93  loss=0.3107  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.682e+00 (batch 72)
  ... step 74/93  loss=0.3045  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.340e+00 (batch 74)
  ... step 76/93  loss=0.3502  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.632e+00 (batch 76)
  ... step 78/93  loss=0.9242  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.164e+01 (batch 78)
  ... step 80/93  loss=0.4809  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.026e+01 (batch 80)
  ... step 82/93  loss=0.2192  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.051e+00 (batch 82)
  ... step 84/93  loss=0.7857  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.069e+01 (batch 84)
  ... step 86/93  loss=0.2308  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.691e+00 (batch 86)
  ... step 88/93  loss=0.6983  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.074e+01 (batch 88)
  ... step 90/93  loss=0.3075  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.336e+00 (batch 90)
  ... step 92/93  loss=0.5972  (0.3s since last print)
  â†˜ grad L2 norm â‰ˆ 2.730e+01 (batch 92)
âœ… epoch 16 forward/backward done in 27.3s
  ðŸ”Ž val step 0: batch (4, 32, 2, 324, 1, 16) loss=0.5899
[16] train=0.4988  val=0.8801  RMSE(std)=[Qi:0.908, Qe:0.973, Î“:0.933]  RMSE(phys)=[Qi:65.918, Qe:90.049, Î“:41.184]  (1.7s)
âœ… Saved unified NN â†’ ./mnt/data/bin.cgyro.nn
ðŸŸ¦ Starting epoch 17/30 (train steps â‰ˆ 93)
  ... step 0/93  loss=0.4130  (0.1s since last print)
  â†˜ grad L2 norm â‰ˆ 8.640e+00 (batch 0)
  ... step 2/93  loss=0.3649  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.534e+00 (batch 2)
  ... step 4/93  loss=0.6551  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.368e+00 (batch 4)
  ... step 6/93  loss=0.2148  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.325e+00 (batch 6)
  ... step 8/93  loss=0.2547  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.143e+00 (batch 8)
  ... step 10/93  loss=0.4362  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.635e+00 (batch 10)
  ... step 12/93  loss=0.3442  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.882e+00 (batch 12)
  ... step 14/93  loss=1.0704  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.071e+01 (batch 14)
  ... step 16/93  loss=0.1653  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.090e+00 (batch 16)
  ... step 18/93  loss=0.5712  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.386e+00 (batch 18)
  ... step 20/93  loss=0.3988  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.082e+00 (batch 20)
  ... step 22/93  loss=0.4421  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.506e+00 (batch 22)
  ... step 24/93  loss=0.7173  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.109e+01 (batch 24)
  ... step 26/93  loss=0.3045  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.371e+00 (batch 26)
  ... step 28/93  loss=0.4012  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.171e+00 (batch 28)
  ... step 30/93  loss=0.2476  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.123e+00 (batch 30)
  ... step 32/93  loss=0.4553  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.996e+00 (batch 32)
  ... step 34/93  loss=0.5956  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.819e+00 (batch 34)
  ... step 36/93  loss=0.4552  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.028e+00 (batch 36)
  ... step 38/93  loss=0.4559  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.009e+01 (batch 38)
  ... step 40/93  loss=0.5961  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.132e+01 (batch 40)
  ... step 42/93  loss=0.8747  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.550e+01 (batch 42)
  ... step 44/93  loss=0.7035  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.473e+01 (batch 44)
  ... step 46/93  loss=0.8786  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.930e+00 (batch 46)
  ... step 48/93  loss=0.7897  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.173e+01 (batch 48)
  ... step 50/93  loss=0.1865  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.359e+00 (batch 50)
  ... step 52/93  loss=0.4816  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.025e+01 (batch 52)
  ... step 54/93  loss=0.2767  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.187e+00 (batch 54)
  ... step 56/93  loss=0.5878  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.131e+00 (batch 56)
  ... step 58/93  loss=0.3851  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.585e+00 (batch 58)
  ... step 60/93  loss=0.5073  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.950e+00 (batch 60)
  ... step 62/93  loss=0.3431  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.123e+00 (batch 62)
  ... step 64/93  loss=0.2408  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.658e+00 (batch 64)
  ... step 66/93  loss=0.7856  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.096e+01 (batch 66)
  ... step 68/93  loss=0.5177  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.470e+00 (batch 68)
  ... step 70/93  loss=0.6644  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.519e+00 (batch 70)
  ... step 72/93  loss=0.6211  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.188e+01 (batch 72)
  ... step 74/93  loss=0.2016  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.790e+00 (batch 74)
  ... step 76/93  loss=0.3334  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.083e+01 (batch 76)
  ... step 78/93  loss=0.5658  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.175e+01 (batch 78)
  ... step 80/93  loss=0.2413  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.856e+00 (batch 80)
  ... step 82/93  loss=0.6111  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.714e+00 (batch 82)
  ... step 84/93  loss=0.5495  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.744e+00 (batch 84)
  ... step 86/93  loss=0.3598  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.024e+00 (batch 86)
  ... step 88/93  loss=0.1920  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.911e+00 (batch 88)
  ... step 90/93  loss=0.5119  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.089e+01 (batch 90)
  ... step 92/93  loss=0.1800  (0.3s since last print)
  â†˜ grad L2 norm â‰ˆ 9.926e+00 (batch 92)
âœ… epoch 17 forward/backward done in 27.3s
  ðŸ”Ž val step 0: batch (4, 32, 2, 324, 1, 16) loss=0.8040
[17] train=0.5006  val=0.7634  RMSE(std)=[Qi:0.846, Qe:0.918, Î“:0.856]  RMSE(phys)=[Qi:61.470, Qe:84.919, Î“:37.777]  (1.7s)
âœ… Saved unified NN â†’ ./mnt/data/bin.cgyro.nn
ðŸŸ¦ Starting epoch 18/30 (train steps â‰ˆ 93)
  ... step 0/93  loss=0.2706  (0.1s since last print)
  â†˜ grad L2 norm â‰ˆ 4.651e+00 (batch 0)
  ... step 2/93  loss=0.2122  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.726e+00 (batch 2)
  ... step 4/93  loss=0.8132  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.629e+00 (batch 4)
  ... step 6/93  loss=0.7052  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.095e+01 (batch 6)
  ... step 8/93  loss=0.5869  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.204e+01 (batch 8)
  ... step 10/93  loss=0.2487  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.018e+00 (batch 10)
  ... step 12/93  loss=0.6976  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.604e+01 (batch 12)
  ... step 14/93  loss=0.4485  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.282e+01 (batch 14)
  ... step 16/93  loss=0.8603  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.318e+01 (batch 16)
  ... step 18/93  loss=0.2513  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.271e+00 (batch 18)
  ... step 20/93  loss=1.2168  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.241e+01 (batch 20)
  ... step 22/93  loss=0.5795  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.253e+00 (batch 22)
  ... step 24/93  loss=0.3118  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.643e+00 (batch 24)
  ... step 26/93  loss=0.3806  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.744e+00 (batch 26)
  ... step 28/93  loss=0.3208  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.173e+00 (batch 28)
  ... step 30/93  loss=0.7828  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.570e+01 (batch 30)
  ... step 32/93  loss=0.3999  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.091e+01 (batch 32)
  ... step 34/93  loss=0.2854  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.714e+00 (batch 34)
  ... step 36/93  loss=0.2825  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.271e+00 (batch 36)
  ... step 38/93  loss=0.1043  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.782e+00 (batch 38)
  ... step 40/93  loss=0.3391  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.907e+00 (batch 40)
  ... step 42/93  loss=0.2891  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.523e+00 (batch 42)
  ... step 44/93  loss=0.3696  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.244e+00 (batch 44)
  ... step 46/93  loss=0.2205  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.720e+00 (batch 46)
  ... step 48/93  loss=0.7769  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.039e+01 (batch 48)
  ... step 50/93  loss=0.2874  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.776e+00 (batch 50)
  ... step 52/93  loss=0.4933  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.608e+00 (batch 52)
  ... step 54/93  loss=0.2769  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.652e+00 (batch 54)
  ... step 56/93  loss=0.6377  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.348e+00 (batch 56)
  ... step 58/93  loss=0.1566  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.904e+00 (batch 58)
  ... step 60/93  loss=0.5649  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.656e+00 (batch 60)
  ... step 62/93  loss=0.8619  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.201e+01 (batch 62)
  ... step 64/93  loss=0.5964  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.095e+01 (batch 64)
  ... step 66/93  loss=0.5160  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.294e+00 (batch 66)
  ... step 68/93  loss=0.2043  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.180e+00 (batch 68)
  ... step 70/93  loss=0.4622  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.056e+01 (batch 70)
  ... step 72/93  loss=0.6759  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.856e+00 (batch 72)
  ... step 74/93  loss=0.2497  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.989e+00 (batch 74)
  ... step 76/93  loss=0.1648  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.530e+00 (batch 76)
  ... step 78/93  loss=0.6913  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.364e+00 (batch 78)
  ... step 80/93  loss=0.5605  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.950e+00 (batch 80)
  ... step 82/93  loss=0.2313  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.480e+00 (batch 82)
  ... step 84/93  loss=0.8437  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.016e+01 (batch 84)
  ... step 86/93  loss=0.5070  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.044e+01 (batch 86)
  ... step 88/93  loss=0.6228  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.327e+00 (batch 88)
  ... step 90/93  loss=0.2689  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.010e+00 (batch 90)
  ... step 92/93  loss=0.1337  (0.3s since last print)
  â†˜ grad L2 norm â‰ˆ 7.049e+00 (batch 92)
âœ… epoch 18 forward/backward done in 27.3s
  ðŸ”Ž val step 0: batch (4, 32, 2, 324, 1, 16) loss=0.3203
[18] train=0.4547  val=0.6668  RMSE(std)=[Qi:0.788, Qe:0.856, Î“:0.804]  RMSE(phys)=[Qi:57.207, Qe:79.239, Î“:35.517]  (1.7s)
âœ… Saved unified NN â†’ ./mnt/data/bin.cgyro.nn
ðŸŸ¦ Starting epoch 19/30 (train steps â‰ˆ 93)
  ... step 0/93  loss=0.4126  (0.1s since last print)
  â†˜ grad L2 norm â‰ˆ 8.220e+00 (batch 0)
  ... step 2/93  loss=0.5319  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.579e+00 (batch 2)
  ... step 4/93  loss=0.4784  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.116e+01 (batch 4)
  ... step 6/93  loss=0.4332  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.337e+01 (batch 6)
  ... step 8/93  loss=0.8798  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.294e+01 (batch 8)
  ... step 10/93  loss=0.2775  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.548e+00 (batch 10)
  ... step 12/93  loss=0.2151  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.553e+00 (batch 12)
  ... step 14/93  loss=0.2477  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.855e+00 (batch 14)
  ... step 16/93  loss=0.3303  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.622e+00 (batch 16)
  ... step 18/93  loss=1.0231  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.295e+00 (batch 18)
  ... step 20/93  loss=0.6645  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.867e+00 (batch 20)
  ... step 22/93  loss=0.4683  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.518e+00 (batch 22)
  ... step 24/93  loss=0.3930  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.788e+00 (batch 24)
  ... step 26/93  loss=0.1812  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.543e+00 (batch 26)
  ... step 28/93  loss=0.2832  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.338e+00 (batch 28)
  ... step 30/93  loss=0.3678  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.282e+00 (batch 30)
  ... step 32/93  loss=0.3399  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.595e+00 (batch 32)
  ... step 34/93  loss=0.2821  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.241e+00 (batch 34)
  ... step 36/93  loss=0.4018  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.791e+00 (batch 36)
  ... step 38/93  loss=0.2635  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.870e+00 (batch 38)
  ... step 40/93  loss=0.1557  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.192e+00 (batch 40)
  ... step 42/93  loss=0.8028  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.295e+01 (batch 42)
  ... step 44/93  loss=0.6668  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.087e+01 (batch 44)
  ... step 46/93  loss=0.6681  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.994e+00 (batch 46)
  ... step 48/93  loss=0.1057  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.049e+00 (batch 48)
  ... step 50/93  loss=1.1293  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.484e+01 (batch 50)
  ... step 52/93  loss=0.5885  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.002e+01 (batch 52)
  ... step 54/93  loss=0.3018  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.578e+00 (batch 54)
  ... step 56/93  loss=0.4085  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.210e+00 (batch 56)
  ... step 58/93  loss=0.3181  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.810e+00 (batch 58)
  ... step 60/93  loss=0.2228  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.164e+00 (batch 60)
  ... step 62/93  loss=0.3360  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.766e+00 (batch 62)
  ... step 64/93  loss=0.4843  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.228e+00 (batch 64)
  ... step 66/93  loss=0.4427  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.295e+00 (batch 66)
  ... step 68/93  loss=0.6485  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.708e+00 (batch 68)
  ... step 70/93  loss=0.4819  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.635e+00 (batch 70)
  ... step 72/93  loss=0.4223  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.003e+01 (batch 72)
  ... step 74/93  loss=0.7299  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.235e+01 (batch 74)
  ... step 76/93  loss=0.5626  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.379e+01 (batch 76)
  ... step 78/93  loss=0.5610  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.859e+00 (batch 78)
  ... step 80/93  loss=0.2565  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.372e+00 (batch 80)
  ... step 82/93  loss=0.4286  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.151e+00 (batch 82)
  ... step 84/93  loss=0.3316  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.205e+01 (batch 84)
  ... step 86/93  loss=0.5632  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.054e+01 (batch 86)
  ... step 88/93  loss=0.8885  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.163e+01 (batch 88)
  ... step 90/93  loss=0.2568  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.629e+00 (batch 90)
  ... step 92/93  loss=0.1040  (0.3s since last print)
  â†˜ grad L2 norm â‰ˆ 3.796e+00 (batch 92)
âœ… epoch 19 forward/backward done in 27.3s
  ðŸ”Ž val step 0: batch (4, 32, 2, 324, 1, 16) loss=0.8665
[19] train=0.4623  val=0.7653  RMSE(std)=[Qi:0.853, Qe:0.915, Î“:0.856]  RMSE(phys)=[Qi:61.939, Qe:84.643, Î“:37.780]  (1.7s)
âœ… Saved unified NN â†’ ./mnt/data/bin.cgyro.nn
ðŸŸ¦ Starting epoch 20/30 (train steps â‰ˆ 93)
  ... step 0/93  loss=0.3739  (0.1s since last print)
  â†˜ grad L2 norm â‰ˆ 7.123e+00 (batch 0)
  ... step 2/93  loss=0.1988  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.125e+00 (batch 2)
  ... step 4/93  loss=0.2025  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.488e+00 (batch 4)
  ... step 6/93  loss=0.2952  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.271e+00 (batch 6)
  ... step 8/93  loss=1.1563  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.296e+01 (batch 8)
  ... step 10/93  loss=0.3032  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.820e+00 (batch 10)
  ... step 12/93  loss=0.3973  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.991e+00 (batch 12)
  ... step 14/93  loss=0.4298  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.569e+00 (batch 14)
  ... step 16/93  loss=0.4475  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.128e+00 (batch 16)
  ... step 18/93  loss=0.3065  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.511e+00 (batch 18)
  ... step 20/93  loss=0.5772  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.236e+01 (batch 20)
  ... step 22/93  loss=0.7612  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.182e+01 (batch 22)
  ... step 24/93  loss=0.4447  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.000e+00 (batch 24)
  ... step 26/93  loss=0.2731  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.691e+00 (batch 26)
  ... step 28/93  loss=0.2886  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.550e+00 (batch 28)
  ... step 30/93  loss=0.3325  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.440e+00 (batch 30)
  ... step 32/93  loss=0.3206  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.667e+00 (batch 32)
  ... step 34/93  loss=0.5247  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.249e+01 (batch 34)
  ... step 36/93  loss=0.1708  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.728e+00 (batch 36)
  ... step 38/93  loss=0.2382  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.883e+00 (batch 38)
  ... step 40/93  loss=0.3742  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.217e+00 (batch 40)
  ... step 42/93  loss=0.2735  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.311e+00 (batch 42)
  ... step 44/93  loss=0.5110  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.472e+00 (batch 44)
  ... step 46/93  loss=0.1276  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.803e+00 (batch 46)
  ... step 48/93  loss=0.4289  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.331e+00 (batch 48)
  ... step 50/93  loss=0.5709  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.071e+01 (batch 50)
  ... step 52/93  loss=0.6463  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.165e+01 (batch 52)
  ... step 54/93  loss=0.3830  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.008e+00 (batch 54)
  ... step 56/93  loss=0.4259  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.722e+00 (batch 56)
  ... step 58/93  loss=0.2429  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.850e+00 (batch 58)
  ... step 60/93  loss=0.4981  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.113e+01 (batch 60)
  ... step 62/93  loss=0.6837  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.266e+01 (batch 62)
  ... step 64/93  loss=0.3463  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.042e+00 (batch 64)
  ... step 66/93  loss=0.2289  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.527e+00 (batch 66)
  ... step 68/93  loss=0.2583  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.581e+00 (batch 68)
  ... step 70/93  loss=0.7072  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.273e+01 (batch 70)
  ... step 72/93  loss=0.2994  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.769e+00 (batch 72)
  ... step 74/93  loss=0.6639  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.302e+01 (batch 74)
  ... step 76/93  loss=0.3863  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.182e+00 (batch 76)
  ... step 78/93  loss=0.2495  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.582e+00 (batch 78)
  ... step 80/93  loss=0.2572  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.685e+00 (batch 80)
  ... step 82/93  loss=0.4263  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.536e+00 (batch 82)
  ... step 84/93  loss=0.2332  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.403e+00 (batch 84)
  ... step 86/93  loss=0.3624  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.028e+01 (batch 86)
  ... step 88/93  loss=0.1794  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.602e+00 (batch 88)
  ... step 90/93  loss=0.3983  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.122e+00 (batch 90)
  ... step 92/93  loss=0.1586  (0.3s since last print)
  â†˜ grad L2 norm â‰ˆ 8.584e+00 (batch 92)
âœ… epoch 20 forward/backward done in 27.3s
  ðŸ”Ž val step 0: batch (4, 32, 2, 324, 1, 16) loss=0.4543
[20] train=0.4173  val=0.7203  RMSE(std)=[Qi:0.820, Qe:0.886, Î“:0.839]  RMSE(phys)=[Qi:59.529, Qe:81.985, Î“:37.055]  (1.7s)
âœ… Saved unified NN â†’ ./mnt/data/bin.cgyro.nn
ðŸŸ¦ Starting epoch 21/30 (train steps â‰ˆ 93)
  ... step 0/93  loss=0.3136  (0.1s since last print)
  â†˜ grad L2 norm â‰ˆ 6.592e+00 (batch 0)
  ... step 2/93  loss=0.2277  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.175e+00 (batch 2)
  ... step 4/93  loss=0.0731  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.486e+00 (batch 4)
  ... step 6/93  loss=0.3712  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.139e+00 (batch 6)
  ... step 8/93  loss=0.2377  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.805e+00 (batch 8)
  ... step 10/93  loss=0.2508  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.163e+00 (batch 10)
  ... step 12/93  loss=0.3480  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.006e+01 (batch 12)
  ... step 14/93  loss=0.1846  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.740e+00 (batch 14)
  ... step 16/93  loss=0.5350  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.039e+01 (batch 16)
  ... step 18/93  loss=0.1864  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.351e+00 (batch 18)
  ... step 20/93  loss=1.2751  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.236e+01 (batch 20)
  ... step 22/93  loss=0.4008  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.939e+00 (batch 22)
  ... step 24/93  loss=0.5317  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.169e+01 (batch 24)
  ... step 26/93  loss=0.3584  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.576e+00 (batch 26)
  ... step 28/93  loss=0.2631  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.239e+00 (batch 28)
  ... step 30/93  loss=0.6409  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.308e+01 (batch 30)
  ... step 32/93  loss=0.1620  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.712e+00 (batch 32)
  ... step 34/93  loss=0.1878  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.797e+00 (batch 34)
  ... step 36/93  loss=0.7420  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.328e+01 (batch 36)
  ... step 38/93  loss=0.1406  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.042e+00 (batch 38)
  ... step 40/93  loss=0.5807  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.358e+00 (batch 40)
  ... step 42/93  loss=0.2885  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.550e+00 (batch 42)
  ... step 44/93  loss=0.3565  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.697e+00 (batch 44)
  ... step 46/93  loss=0.1972  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.046e+00 (batch 46)
  ... step 48/93  loss=0.3610  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.132e+00 (batch 48)
  ... step 50/93  loss=1.0222  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.368e+01 (batch 50)
  ... step 52/93  loss=0.1262  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.765e+00 (batch 52)
  ... step 54/93  loss=0.4152  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.319e+00 (batch 54)
  ... step 56/93  loss=0.4185  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.738e+00 (batch 56)
  ... step 58/93  loss=0.2197  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.776e+00 (batch 58)
  ... step 60/93  loss=0.2841  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.508e+00 (batch 60)
  ... step 62/93  loss=0.4188  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.044e+00 (batch 62)
  ... step 64/93  loss=0.5612  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.710e+00 (batch 64)
  ... step 66/93  loss=0.2887  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.674e+00 (batch 66)
  ... step 68/93  loss=0.2797  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.417e+00 (batch 68)
  ... step 70/93  loss=0.3229  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.414e+00 (batch 70)
  ... step 72/93  loss=0.4525  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.356e+00 (batch 72)
  ... step 74/93  loss=1.0616  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.358e+01 (batch 74)
  ... step 76/93  loss=0.4282  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.256e+01 (batch 76)
  ... step 78/93  loss=0.1578  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.208e+00 (batch 78)
  ... step 80/93  loss=0.1207  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.142e+00 (batch 80)
  ... step 82/93  loss=0.3558  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.399e+00 (batch 82)
  ... step 84/93  loss=0.2423  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.122e+00 (batch 84)
  ... step 86/93  loss=0.3202  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.152e+00 (batch 86)
  ... step 88/93  loss=0.7678  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.808e+00 (batch 88)
  ... step 90/93  loss=0.7936  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.110e+01 (batch 90)
  ... step 92/93  loss=0.0408  (0.3s since last print)
  â†˜ grad L2 norm â‰ˆ 3.420e+00 (batch 92)
âœ… epoch 21 forward/backward done in 27.3s
  ðŸ”Ž val step 0: batch (4, 32, 2, 324, 1, 16) loss=0.3947
[21] train=0.4386  val=0.7139  RMSE(std)=[Qi:0.823, Qe:0.885, Î“:0.826]  RMSE(phys)=[Qi:59.776, Qe:81.875, Î“:36.458]  (1.7s)
âœ… Saved unified NN â†’ ./mnt/data/bin.cgyro.nn
ðŸŸ¦ Starting epoch 22/30 (train steps â‰ˆ 93)
  ... step 0/93  loss=0.3446  (0.1s since last print)
  â†˜ grad L2 norm â‰ˆ 6.734e+00 (batch 0)
  ... step 2/93  loss=0.3589  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.901e+00 (batch 2)
  ... step 4/93  loss=0.2073  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.364e+00 (batch 4)
  ... step 6/93  loss=0.4217  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.354e+00 (batch 6)
  ... step 8/93  loss=0.4825  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.024e+00 (batch 8)
  ... step 10/93  loss=0.4450  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.201e+01 (batch 10)
  ... step 12/93  loss=0.2903  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.761e+00 (batch 12)
  ... step 14/93  loss=0.7337  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.304e+01 (batch 14)
  ... step 16/93  loss=0.6963  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.113e+01 (batch 16)
  ... step 18/93  loss=0.1658  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.943e+00 (batch 18)
  ... step 20/93  loss=0.4348  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.212e+01 (batch 20)
  ... step 22/93  loss=0.3564  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.616e+00 (batch 22)
  ... step 24/93  loss=0.4071  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.333e+01 (batch 24)
  ... step 26/93  loss=0.2245  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.172e+00 (batch 26)
  ... step 28/93  loss=0.5571  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.495e+01 (batch 28)
  ... step 30/93  loss=0.1162  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.974e+00 (batch 30)
  ... step 32/93  loss=0.2427  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.100e+00 (batch 32)
  ... step 34/93  loss=0.2888  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.006e+01 (batch 34)
  ... step 36/93  loss=0.8990  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.200e+01 (batch 36)
  ... step 38/93  loss=0.3345  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.953e+00 (batch 38)
  ... step 40/93  loss=0.2616  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.589e+00 (batch 40)
  ... step 42/93  loss=0.0952  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.968e+00 (batch 42)
  ... step 44/93  loss=0.1621  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.413e+00 (batch 44)
  ... step 46/93  loss=0.2245  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.668e+00 (batch 46)
  ... step 48/93  loss=0.3189  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.862e+00 (batch 48)
  ... step 50/93  loss=0.3107  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.906e+00 (batch 50)
  ... step 52/93  loss=0.5482  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.417e+01 (batch 52)
  ... step 54/93  loss=0.5185  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.205e+01 (batch 54)
  ... step 56/93  loss=0.2743  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.131e+00 (batch 56)
  ... step 58/93  loss=0.4098  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.639e+00 (batch 58)
  ... step 60/93  loss=0.6414  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.274e+01 (batch 60)
  ... step 62/93  loss=0.2604  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.646e+00 (batch 62)
  ... step 64/93  loss=0.4883  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.408e+00 (batch 64)
  ... step 66/93  loss=0.4977  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.069e+01 (batch 66)
  ... step 68/93  loss=0.2970  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.396e+00 (batch 68)
  ... step 70/93  loss=0.6311  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.297e+01 (batch 70)
  ... step 72/93  loss=0.3063  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.695e+00 (batch 72)
  ... step 74/93  loss=0.5121  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.109e+01 (batch 74)
  ... step 76/93  loss=0.2109  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.909e+00 (batch 76)
  ... step 78/93  loss=0.2490  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.611e+00 (batch 78)
  ... step 80/93  loss=0.5225  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.255e+00 (batch 80)
  ... step 82/93  loss=0.1893  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.078e+00 (batch 82)
  ... step 84/93  loss=0.3659  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.308e+00 (batch 84)
  ... step 86/93  loss=0.4705  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.201e+00 (batch 86)
  ... step 88/93  loss=0.5865  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.085e+01 (batch 88)
  ... step 90/93  loss=0.6926  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.076e+01 (batch 90)
  ... step 92/93  loss=0.5154  (0.3s since last print)
  â†˜ grad L2 norm â‰ˆ 1.667e+01 (batch 92)
âœ… epoch 22 forward/backward done in 27.3s
  ðŸ”Ž val step 0: batch (4, 32, 2, 324, 1, 16) loss=0.3324
[22] train=0.4059  val=0.6578  RMSE(std)=[Qi:0.789, Qe:0.851, Î“:0.792]  RMSE(phys)=[Qi:57.297, Qe:78.754, Î“:34.958]  (1.7s)
âœ… Saved unified NN â†’ ./mnt/data/bin.cgyro.nn
ðŸŸ¦ Starting epoch 23/30 (train steps â‰ˆ 93)
  ... step 0/93  loss=0.1881  (0.1s since last print)
  â†˜ grad L2 norm â‰ˆ 5.117e+00 (batch 0)
  ... step 2/93  loss=0.3377  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.118e+00 (batch 2)
  ... step 4/93  loss=1.0666  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.709e+01 (batch 4)
  ... step 6/93  loss=0.4762  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.336e+01 (batch 6)
  ... step 8/93  loss=0.3914  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.978e+00 (batch 8)
  ... step 10/93  loss=0.4741  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.243e+01 (batch 10)
  ... step 12/93  loss=0.3679  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.218e+01 (batch 12)
  ... step 14/93  loss=0.3231  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.260e+00 (batch 14)
  ... step 16/93  loss=0.2225  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.972e+00 (batch 16)
  ... step 18/93  loss=0.1960  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.110e+00 (batch 18)
  ... step 20/93  loss=0.3502  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.901e+00 (batch 20)
  ... step 22/93  loss=0.2644  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.736e+00 (batch 22)
  ... step 24/93  loss=0.9382  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.752e+01 (batch 24)
  ... step 26/93  loss=0.3390  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.009e+01 (batch 26)
  ... step 28/93  loss=0.1714  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.971e+00 (batch 28)
  ... step 30/93  loss=0.1744  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.944e+00 (batch 30)
  ... step 32/93  loss=0.4433  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.755e+00 (batch 32)
  ... step 34/93  loss=0.8234  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.398e+00 (batch 34)
  ... step 36/93  loss=0.1429  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.059e+00 (batch 36)
  ... step 38/93  loss=0.2870  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.408e+00 (batch 38)
  ... step 40/93  loss=0.3865  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.213e+01 (batch 40)
  ... step 42/93  loss=0.3569  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.964e+00 (batch 42)
  ... step 44/93  loss=0.4662  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.508e+00 (batch 44)
  ... step 46/93  loss=0.2605  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.286e+00 (batch 46)
  ... step 48/93  loss=0.4480  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.606e+00 (batch 48)
  ... step 50/93  loss=0.5576  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.478e+00 (batch 50)
  ... step 52/93  loss=0.7707  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.572e+01 (batch 52)
  ... step 54/93  loss=0.2762  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.318e+00 (batch 54)
  ... step 56/93  loss=0.3409  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.647e+00 (batch 56)
  ... step 58/93  loss=0.6045  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.132e+01 (batch 58)
  ... step 60/93  loss=0.1723  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.015e+00 (batch 60)
  ... step 62/93  loss=0.3657  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.282e+00 (batch 62)
  ... step 64/93  loss=0.4673  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.694e+00 (batch 64)
  ... step 66/93  loss=0.1716  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.805e+00 (batch 66)
  ... step 68/93  loss=0.2347  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.799e+00 (batch 68)
  ... step 70/93  loss=0.1573  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.842e+00 (batch 70)
  ... step 72/93  loss=0.3331  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.063e+01 (batch 72)
  ... step 74/93  loss=0.2408  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.059e+00 (batch 74)
  ... step 76/93  loss=0.1536  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.555e+00 (batch 76)
  ... step 78/93  loss=0.4878  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.441e+00 (batch 78)
  ... step 80/93  loss=0.2652  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.906e+00 (batch 80)
  ... step 82/93  loss=0.2371  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.926e+00 (batch 82)
  ... step 84/93  loss=0.2261  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.096e+00 (batch 84)
  ... step 86/93  loss=0.4521  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.224e+00 (batch 86)
  ... step 88/93  loss=0.3191  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.873e+00 (batch 88)
  ... step 90/93  loss=0.1190  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.741e+00 (batch 90)
  ... step 92/93  loss=1.3501  (0.3s since last print)
  â†˜ grad L2 norm â‰ˆ 4.967e+01 (batch 92)
âœ… epoch 23 forward/backward done in 27.3s
  ðŸ”Ž val step 0: batch (4, 32, 2, 324, 1, 16) loss=0.4342
[23] train=0.3889  val=0.7372  RMSE(std)=[Qi:0.842, Qe:0.891, Î“:0.842]  RMSE(phys)=[Qi:61.172, Qe:82.422, Î“:37.179]  (1.7s)
âœ… Saved unified NN â†’ ./mnt/data/bin.cgyro.nn
ðŸŸ¦ Starting epoch 24/30 (train steps â‰ˆ 93)
  ... step 0/93  loss=0.2475  (0.1s since last print)
  â†˜ grad L2 norm â‰ˆ 6.714e+00 (batch 0)
  ... step 2/93  loss=0.2819  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.316e+00 (batch 2)
  ... step 4/93  loss=0.3075  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.827e+00 (batch 4)
  ... step 6/93  loss=0.7436  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.165e+00 (batch 6)
  ... step 8/93  loss=0.4981  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.168e+00 (batch 8)
  ... step 10/93  loss=0.5433  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.210e+01 (batch 10)
  ... step 12/93  loss=0.9938  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.357e+01 (batch 12)
  ... step 14/93  loss=0.2322  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.237e+00 (batch 14)
  ... step 16/93  loss=0.4511  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.477e+01 (batch 16)
  ... step 18/93  loss=0.3432  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.024e+01 (batch 18)
  ... step 20/93  loss=0.3809  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.323e+01 (batch 20)
  ... step 22/93  loss=0.4902  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.109e+01 (batch 22)
  ... step 24/93  loss=0.3995  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.277e+01 (batch 24)
  ... step 26/93  loss=0.2713  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.686e+00 (batch 26)
  ... step 28/93  loss=0.4214  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.890e+00 (batch 28)
  ... step 30/93  loss=0.1244  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.552e+00 (batch 30)
  ... step 32/93  loss=0.3199  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.043e+01 (batch 32)
  ... step 34/93  loss=0.2644  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.107e+00 (batch 34)
  ... step 36/93  loss=0.4684  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.343e+00 (batch 36)
  ... step 38/93  loss=0.3973  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.455e+00 (batch 38)
  ... step 40/93  loss=0.1535  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.434e+00 (batch 40)
  ... step 42/93  loss=0.2149  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.850e+00 (batch 42)
  ... step 44/93  loss=0.6912  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.195e+01 (batch 44)
  ... step 46/93  loss=0.1972  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.426e+00 (batch 46)
  ... step 48/93  loss=0.5802  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.194e+01 (batch 48)
  ... step 50/93  loss=0.2342  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.551e+00 (batch 50)
  ... step 52/93  loss=0.3477  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.417e+00 (batch 52)
  ... step 54/93  loss=0.3368  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.792e+00 (batch 54)
  ... step 56/93  loss=0.5848  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.163e+00 (batch 56)
  ... step 58/93  loss=0.2131  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.002e+00 (batch 58)
  ... step 60/93  loss=0.2057  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.843e+00 (batch 60)
  ... step 62/93  loss=0.2850  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.275e+00 (batch 62)
  ... step 64/93  loss=0.4538  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.973e+00 (batch 64)
  ... step 66/93  loss=0.1559  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.654e+00 (batch 66)
  ... step 68/93  loss=0.5033  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.257e+01 (batch 68)
  ... step 70/93  loss=0.2865  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.331e+00 (batch 70)
  ... step 72/93  loss=0.2755  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.294e+00 (batch 72)
  ... step 74/93  loss=0.9346  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.108e+01 (batch 74)
  ... step 76/93  loss=0.2683  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.909e+00 (batch 76)
  ... step 78/93  loss=0.1158  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.512e+00 (batch 78)
  ... step 80/93  loss=0.5851  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.260e+01 (batch 80)
  ... step 82/93  loss=0.2073  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.347e+00 (batch 82)
  ... step 84/93  loss=0.3252  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.778e+00 (batch 84)
  ... step 86/93  loss=0.3139  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.503e+00 (batch 86)
  ... step 88/93  loss=0.2316  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.720e+00 (batch 88)
  ... step 90/93  loss=0.3625  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.153e+01 (batch 90)
  ... step 92/93  loss=0.1331  (0.3s since last print)
  â†˜ grad L2 norm â‰ˆ 1.810e+01 (batch 92)
âœ… epoch 24 forward/backward done in 27.4s
  ðŸ”Ž val step 0: batch (4, 32, 2, 324, 1, 16) loss=0.3541
[24] train=0.3976  val=0.7299  RMSE(std)=[Qi:0.836, Qe:0.886, Î“:0.841]  RMSE(phys)=[Qi:60.683, Qe:81.953, Î“:37.135]  (1.7s)
âœ… Saved unified NN â†’ ./mnt/data/bin.cgyro.nn
ðŸŸ¦ Starting epoch 25/30 (train steps â‰ˆ 93)
  ... step 0/93  loss=0.7877  (0.1s since last print)
  â†˜ grad L2 norm â‰ˆ 1.868e+02 (batch 0)
  ... step 2/93  loss=0.2476  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.219e+00 (batch 2)
  ... step 4/93  loss=0.2239  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.634e+00 (batch 4)
  ... step 6/93  loss=0.1744  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.544e+00 (batch 6)
  ... step 8/93  loss=0.2786  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.172e+00 (batch 8)
  ... step 10/93  loss=0.2004  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.211e+01 (batch 10)
  ... step 12/93  loss=0.6002  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.882e+00 (batch 12)
  ... step 14/93  loss=0.5932  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.282e+00 (batch 14)
  ... step 16/93  loss=0.6382  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.276e+00 (batch 16)
  ... step 18/93  loss=0.4331  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.650e+00 (batch 18)
  ... step 20/93  loss=0.2935  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.925e+00 (batch 20)
  ... step 22/93  loss=0.3031  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.410e+00 (batch 22)
  ... step 24/93  loss=0.5633  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.279e+00 (batch 24)
  ... step 26/93  loss=0.4190  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.532e+00 (batch 26)
  ... step 28/93  loss=0.5862  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.044e+01 (batch 28)
  ... step 30/93  loss=0.3582  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.403e+00 (batch 30)
  ... step 32/93  loss=0.3473  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.243e+00 (batch 32)
  ... step 34/93  loss=0.3305  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.949e+00 (batch 34)
  ... step 36/93  loss=0.5682  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.045e+01 (batch 36)
  ... step 38/93  loss=0.3766  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.220e+01 (batch 38)
  ... step 40/93  loss=0.3245  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.192e+00 (batch 40)
  ... step 42/93  loss=0.2724  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.905e+00 (batch 42)
  ... step 44/93  loss=0.7563  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.687e+01 (batch 44)
  ... step 46/93  loss=0.2135  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.696e+00 (batch 46)
  ... step 48/93  loss=0.9371  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.146e+01 (batch 48)
  ... step 50/93  loss=0.2871  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.304e+00 (batch 50)
  ... step 52/93  loss=0.3669  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.821e+00 (batch 52)
  ... step 54/93  loss=0.2954  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.693e+00 (batch 54)
  ... step 56/93  loss=0.3981  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.396e+01 (batch 56)
  ... step 58/93  loss=0.1540  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.960e+00 (batch 58)
  ... step 60/93  loss=0.5383  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.059e+01 (batch 60)
  ... step 62/93  loss=0.3774  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.106e+01 (batch 62)
  ... step 64/93  loss=0.4942  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.895e+00 (batch 64)
  ... step 66/93  loss=0.3166  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.695e+00 (batch 66)
  ... step 68/93  loss=0.7338  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.047e+01 (batch 68)
  ... step 70/93  loss=0.6720  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.567e+00 (batch 70)
  ... step 72/93  loss=0.3414  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.970e+00 (batch 72)
  ... step 74/93  loss=0.6469  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.476e+00 (batch 74)
  ... step 76/93  loss=0.2331  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.086e+00 (batch 76)
  ... step 78/93  loss=0.7418  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.753e+01 (batch 78)
  ... step 80/93  loss=0.3799  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.855e+00 (batch 80)
  ... step 82/93  loss=0.3100  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.506e+00 (batch 82)
  ... step 84/93  loss=0.7200  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.001e+01 (batch 84)
  ... step 86/93  loss=0.6852  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.054e+01 (batch 86)
  ... step 88/93  loss=0.4449  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.233e+01 (batch 88)
  ... step 90/93  loss=0.2590  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.125e+00 (batch 90)
  ... step 92/93  loss=1.0046  (0.3s since last print)
  â†˜ grad L2 norm â‰ˆ 6.197e+01 (batch 92)
âœ… epoch 25 forward/backward done in 27.4s
  ðŸ”Ž val step 0: batch (4, 32, 2, 324, 1, 16) loss=0.6300
[25] train=0.4278  val=0.7178  RMSE(std)=[Qi:0.838, Qe:0.885, Î“:0.818]  RMSE(phys)=[Qi:60.843, Qe:81.868, Î“:36.119]  (1.7s)
âœ… Saved unified NN â†’ ./mnt/data/bin.cgyro.nn
ðŸŸ¦ Starting epoch 26/30 (train steps â‰ˆ 93)
  ... step 0/93  loss=0.3187  (0.1s since last print)
  â†˜ grad L2 norm â‰ˆ 6.356e+00 (batch 0)
  ... step 2/93  loss=0.4726  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.238e+00 (batch 2)
  ... step 4/93  loss=0.2329  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.253e+00 (batch 4)
  ... step 6/93  loss=0.4344  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.197e+00 (batch 6)
  ... step 8/93  loss=0.8983  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.258e+01 (batch 8)
  ... step 10/93  loss=0.1158  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.195e+00 (batch 10)
  ... step 12/93  loss=0.7939  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.968e+00 (batch 12)
  ... step 14/93  loss=0.3360  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.755e+00 (batch 14)
  ... step 16/93  loss=0.2346  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.941e+00 (batch 16)
  ... step 18/93  loss=0.6387  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.025e+00 (batch 18)
  ... step 20/93  loss=1.2051  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.258e+01 (batch 20)
  ... step 22/93  loss=0.1403  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.988e+00 (batch 22)
  ... step 24/93  loss=0.5909  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.299e+01 (batch 24)
  ... step 26/93  loss=1.0569  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.909e+01 (batch 26)
  ... step 28/93  loss=0.4738  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.578e+00 (batch 28)
  ... step 30/93  loss=0.1854  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.948e+00 (batch 30)
  ... step 32/93  loss=0.7007  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.940e+00 (batch 32)
  ... step 34/93  loss=0.2854  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.739e+00 (batch 34)
  ... step 36/93  loss=0.2971  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.888e+00 (batch 36)
  ... step 38/93  loss=0.1728  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.685e+00 (batch 38)
  ... step 40/93  loss=0.7392  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.242e+01 (batch 40)
  ... step 42/93  loss=0.8638  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.867e+00 (batch 42)
  ... step 44/93  loss=0.2941  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.198e+01 (batch 44)
  ... step 46/93  loss=0.3112  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.116e+00 (batch 46)
  ... step 48/93  loss=0.3240  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.625e+00 (batch 48)
  ... step 50/93  loss=0.4287  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.159e+00 (batch 50)
  ... step 52/93  loss=0.2662  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.630e+00 (batch 52)
  ... step 54/93  loss=0.3486  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.124e+00 (batch 54)
  ... step 56/93  loss=0.1814  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.114e+00 (batch 56)
  ... step 58/93  loss=0.3222  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.073e+00 (batch 58)
  ... step 60/93  loss=0.2218  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.486e+00 (batch 60)
  ... step 62/93  loss=0.7198  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.788e+01 (batch 62)
  ... step 64/93  loss=0.3191  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.128e+00 (batch 64)
  ... step 66/93  loss=0.3272  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.258e+00 (batch 66)
  ... step 68/93  loss=0.3667  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.006e+00 (batch 68)
  ... step 70/93  loss=0.4383  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.139e+01 (batch 70)
  ... step 72/93  loss=0.3982  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.401e+00 (batch 72)
  ... step 74/93  loss=0.5086  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.791e+01 (batch 74)
  ... step 76/93  loss=0.1356  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.217e+00 (batch 76)
  ... step 78/93  loss=0.0945  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.897e+00 (batch 78)
  ... step 80/93  loss=0.5240  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.711e+00 (batch 80)
  ... step 82/93  loss=0.2958  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.810e+00 (batch 82)
  ... step 84/93  loss=0.1864  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.100e+00 (batch 84)
  ... step 86/93  loss=0.2777  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.747e+00 (batch 86)
  ... step 88/93  loss=0.5416  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.207e+01 (batch 88)
  ... step 90/93  loss=0.3330  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.413e+00 (batch 90)
  ... step 92/93  loss=0.4897  (0.3s since last print)
  â†˜ grad L2 norm â‰ˆ 3.099e+01 (batch 92)
âœ… epoch 26 forward/backward done in 27.4s
  ðŸ”Ž val step 0: batch (4, 32, 2, 324, 1, 16) loss=0.4184
[26] train=0.4150  val=0.6705  RMSE(std)=[Qi:0.799, Qe:0.856, Î“:0.801]  RMSE(phys)=[Qi:57.996, Qe:79.225, Î“:35.351]  (1.7s)
âœ… Saved unified NN â†’ ./mnt/data/bin.cgyro.nn
ðŸŸ¦ Starting epoch 27/30 (train steps â‰ˆ 93)
  ... step 0/93  loss=0.3337  (0.1s since last print)
  â†˜ grad L2 norm â‰ˆ 7.798e+00 (batch 0)
  ... step 2/93  loss=0.3578  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.778e+00 (batch 2)
  ... step 4/93  loss=0.2944  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.598e+00 (batch 4)
  ... step 6/93  loss=0.6557  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.805e+01 (batch 6)
  ... step 8/93  loss=0.2654  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.579e+00 (batch 8)
  ... step 10/93  loss=0.3051  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.593e+00 (batch 10)
  ... step 12/93  loss=0.3778  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.492e+00 (batch 12)
  ... step 14/93  loss=0.3565  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.120e+00 (batch 14)
  ... step 16/93  loss=0.4448  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.798e+00 (batch 16)
  ... step 18/93  loss=0.9321  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.325e+01 (batch 18)
  ... step 20/93  loss=0.2635  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.772e+00 (batch 20)
  ... step 22/93  loss=0.2416  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.225e+00 (batch 22)
  ... step 24/93  loss=0.2619  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.089e+00 (batch 24)
  ... step 26/93  loss=0.3547  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.749e+00 (batch 26)
  ... step 28/93  loss=0.3678  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.004e+00 (batch 28)
  ... step 30/93  loss=0.2032  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.003e+00 (batch 30)
  ... step 32/93  loss=0.2790  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.226e+00 (batch 32)
  ... step 34/93  loss=1.2815  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.214e+01 (batch 34)
  ... step 36/93  loss=0.2523  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.912e+00 (batch 36)
  ... step 38/93  loss=0.3382  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.530e+00 (batch 38)
  ... step 40/93  loss=0.2668  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.878e+00 (batch 40)
  ... step 42/93  loss=0.4714  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.884e+00 (batch 42)
  ... step 44/93  loss=0.1489  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.108e+00 (batch 44)
  ... step 46/93  loss=0.1429  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.692e+00 (batch 46)
  ... step 48/93  loss=0.7925  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.590e+01 (batch 48)
  ... step 50/93  loss=0.2694  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.622e+00 (batch 50)
  ... step 52/93  loss=0.4097  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.397e+00 (batch 52)
  ... step 54/93  loss=0.1195  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.725e+00 (batch 54)
  ... step 56/93  loss=0.2590  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.640e+00 (batch 56)
  ... step 58/93  loss=0.3194  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.702e+00 (batch 58)
  ... step 60/93  loss=0.1415  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.676e+00 (batch 60)
  ... step 62/93  loss=0.5698  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.202e+01 (batch 62)
  ... step 64/93  loss=0.3627  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.527e+00 (batch 64)
  ... step 66/93  loss=0.9249  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.148e+01 (batch 66)
  ... step 68/93  loss=0.0949  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.720e+00 (batch 68)
  ... step 70/93  loss=0.2989  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.656e+00 (batch 70)
  ... step 72/93  loss=0.6616  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.121e+01 (batch 72)
  ... step 74/93  loss=0.4885  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.826e+00 (batch 74)
  ... step 76/93  loss=0.6885  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.290e+01 (batch 76)
  ... step 78/93  loss=0.2530  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.198e+00 (batch 78)
  ... step 80/93  loss=0.3878  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.775e+00 (batch 80)
  ... step 82/93  loss=0.4428  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.940e+00 (batch 82)
  ... step 84/93  loss=0.2312  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.183e+00 (batch 84)
  ... step 86/93  loss=0.1055  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.447e+00 (batch 86)
  ... step 88/93  loss=0.4331  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.417e+00 (batch 88)
  ... step 90/93  loss=0.4903  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.169e+01 (batch 90)
  ... step 92/93  loss=0.1936  (0.3s since last print)
  â†˜ grad L2 norm â‰ˆ 1.506e+01 (batch 92)
âœ… epoch 27 forward/backward done in 27.4s
  ðŸ”Ž val step 0: batch (4, 32, 2, 324, 1, 16) loss=0.3213
[27] train=0.3852  val=0.7876  RMSE(std)=[Qi:0.863, Qe:0.927, Î“:0.871]  RMSE(phys)=[Qi:62.679, Qe:85.794, Î“:38.457]  (1.7s)
âœ… Saved unified NN â†’ ./mnt/data/bin.cgyro.nn
ðŸŸ¦ Starting epoch 28/30 (train steps â‰ˆ 93)
  ... step 0/93  loss=0.3957  (0.1s since last print)
  â†˜ grad L2 norm â‰ˆ 7.029e+00 (batch 0)
  ... step 2/93  loss=0.2656  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.078e+00 (batch 2)
  ... step 4/93  loss=0.5974  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.275e+00 (batch 4)
  ... step 6/93  loss=0.2784  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.763e+00 (batch 6)
  ... step 8/93  loss=0.4970  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.150e+00 (batch 8)
  ... step 10/93  loss=0.4205  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.370e+00 (batch 10)
  ... step 12/93  loss=0.3769  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.844e+00 (batch 12)
  ... step 14/93  loss=0.5753  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.097e+01 (batch 14)
  ... step 16/93  loss=0.2188  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.666e+00 (batch 16)
  ... step 18/93  loss=0.3045  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.111e+00 (batch 18)
  ... step 20/93  loss=0.4561  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.688e+00 (batch 20)
  ... step 22/93  loss=0.4970  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.208e+01 (batch 22)
  ... step 24/93  loss=0.1398  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.238e+00 (batch 24)
  ... step 26/93  loss=0.2840  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.924e+00 (batch 26)
  ... step 28/93  loss=0.2064  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.998e+00 (batch 28)
  ... step 30/93  loss=0.1859  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.653e+00 (batch 30)
  ... step 32/93  loss=0.1692  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.326e+00 (batch 32)
  ... step 34/93  loss=0.2736  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.858e+00 (batch 34)
  ... step 36/93  loss=0.2638  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.408e+00 (batch 36)
  ... step 38/93  loss=0.1514  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.780e+00 (batch 38)
  ... step 40/93  loss=0.4463  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.020e+01 (batch 40)
  ... step 42/93  loss=0.3212  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.724e+00 (batch 42)
  ... step 44/93  loss=1.0131  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.845e+01 (batch 44)
  ... step 46/93  loss=0.1905  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.283e+00 (batch 46)
  ... step 48/93  loss=0.1561  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.170e+00 (batch 48)
  ... step 50/93  loss=0.2125  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.126e+00 (batch 50)
  ... step 52/93  loss=0.4587  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.123e+00 (batch 52)
  ... step 54/93  loss=0.2353  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.287e+00 (batch 54)
  ... step 56/93  loss=0.2122  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.265e+00 (batch 56)
  ... step 58/93  loss=0.1966  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.326e+00 (batch 58)
  ... step 60/93  loss=0.3447  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.301e+00 (batch 60)
  ... step 62/93  loss=0.2339  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.575e+00 (batch 62)
  ... step 64/93  loss=0.5513  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.011e+01 (batch 64)
  ... step 66/93  loss=0.1581  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.166e+00 (batch 66)
  ... step 68/93  loss=0.1541  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.796e+00 (batch 68)
  ... step 70/93  loss=0.3576  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.732e+00 (batch 70)
  ... step 72/93  loss=0.3515  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.043e+00 (batch 72)
  ... step 74/93  loss=0.1649  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.371e+00 (batch 74)
  ... step 76/93  loss=0.4607  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.720e+00 (batch 76)
  ... step 78/93  loss=0.1589  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.685e+00 (batch 78)
  ... step 80/93  loss=0.1478  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.275e+00 (batch 80)
  ... step 82/93  loss=0.3781  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.950e+00 (batch 82)
  ... step 84/93  loss=0.4832  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.128e+01 (batch 84)
  ... step 86/93  loss=0.1725  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.907e+00 (batch 86)
  ... step 88/93  loss=0.3798  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.633e+00 (batch 88)
  ... step 90/93  loss=0.2826  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.868e+00 (batch 90)
  ... step 92/93  loss=1.8781  (0.3s since last print)
  â†˜ grad L2 norm â‰ˆ 6.719e+01 (batch 92)
âœ… epoch 28 forward/backward done in 27.4s
  ðŸ”Ž val step 0: batch (4, 32, 2, 324, 1, 16) loss=0.5134
[28] train=0.3409  val=0.7460  RMSE(std)=[Qi:0.850, Qe:0.906, Î“:0.833]  RMSE(phys)=[Qi:61.696, Qe:83.889, Î“:36.805]  (1.7s)
âœ… Saved unified NN â†’ ./mnt/data/bin.cgyro.nn
ðŸŸ¦ Starting epoch 29/30 (train steps â‰ˆ 93)
  ... step 0/93  loss=0.4809  (0.1s since last print)
  â†˜ grad L2 norm â‰ˆ 1.340e+01 (batch 0)
  ... step 2/93  loss=0.2582  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.358e+01 (batch 2)
  ... step 4/93  loss=0.1786  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.261e+00 (batch 4)
  ... step 6/93  loss=0.6204  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.191e+00 (batch 6)
  ... step 8/93  loss=0.3849  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.078e+00 (batch 8)
  ... step 10/93  loss=0.4379  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.052e+00 (batch 10)
  ... step 12/93  loss=0.6498  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.968e+00 (batch 12)
  ... step 14/93  loss=0.1527  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.254e+00 (batch 14)
  ... step 16/93  loss=0.3302  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.334e+00 (batch 16)
  ... step 18/93  loss=0.0640  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.680e+00 (batch 18)
  ... step 20/93  loss=0.1486  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.858e+00 (batch 20)
  ... step 22/93  loss=0.2807  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.215e+00 (batch 22)
  ... step 24/93  loss=0.1225  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.126e+00 (batch 24)
  ... step 26/93  loss=0.2401  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.239e+00 (batch 26)
  ... step 28/93  loss=0.2004  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.426e+00 (batch 28)
  ... step 30/93  loss=0.4409  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.328e+00 (batch 30)
  ... step 32/93  loss=0.2204  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.783e+00 (batch 32)
  ... step 34/93  loss=0.4709  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.380e+00 (batch 34)
  ... step 36/93  loss=0.2030  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.445e+00 (batch 36)
  ... step 38/93  loss=0.2463  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.501e+00 (batch 38)
  ... step 40/93  loss=0.4147  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.494e+00 (batch 40)
  ... step 42/93  loss=0.1522  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.819e+00 (batch 42)
  ... step 44/93  loss=0.3736  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.689e+00 (batch 44)
  ... step 46/93  loss=0.1787  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.187e+00 (batch 46)
  ... step 48/93  loss=0.2211  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.906e+00 (batch 48)
  ... step 50/93  loss=0.3618  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.683e+00 (batch 50)
  ... step 52/93  loss=0.0850  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.629e+00 (batch 52)
  ... step 54/93  loss=0.2989  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.396e+00 (batch 54)
  ... step 56/93  loss=0.4700  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.987e+00 (batch 56)
  ... step 58/93  loss=0.0967  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.598e+00 (batch 58)
  ... step 60/93  loss=0.1305  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.197e+00 (batch 60)
  ... step 62/93  loss=0.2756  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.562e+00 (batch 62)
  ... step 64/93  loss=0.3709  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.565e+00 (batch 64)
  ... step 66/93  loss=0.4693  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.297e+00 (batch 66)
  ... step 68/93  loss=0.7628  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.410e+01 (batch 68)
  ... step 70/93  loss=0.1658  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.440e+00 (batch 70)
  ... step 72/93  loss=0.4322  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.169e+01 (batch 72)
  ... step 74/93  loss=0.7373  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.172e+01 (batch 74)
  ... step 76/93  loss=0.2279  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.562e+00 (batch 76)
  ... step 78/93  loss=0.4602  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.098e+01 (batch 78)
  ... step 80/93  loss=0.3733  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.095e+00 (batch 80)
  ... step 82/93  loss=0.2072  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.618e+00 (batch 82)
  ... step 84/93  loss=0.3776  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.965e+00 (batch 84)
  ... step 86/93  loss=0.5973  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.197e+01 (batch 86)
  ... step 88/93  loss=0.2068  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.951e+00 (batch 88)
  ... step 90/93  loss=0.1810  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.543e+00 (batch 90)
  ... step 92/93  loss=1.7445  (0.3s since last print)
  â†˜ grad L2 norm â‰ˆ 6.798e+01 (batch 92)
âœ… epoch 29 forward/backward done in 27.4s
  ðŸ”Ž val step 0: batch (4, 32, 2, 324, 1, 16) loss=0.2353
[29] train=0.3398  val=0.8183  RMSE(std)=[Qi:0.877, Qe:0.950, Î“:0.886]  RMSE(phys)=[Qi:63.661, Qe:87.882, Î“:39.117]  (1.7s)
âœ… Saved unified NN â†’ ./mnt/data/bin.cgyro.nn
ðŸŸ¦ Starting epoch 30/30 (train steps â‰ˆ 93)
  ... step 0/93  loss=0.2401  (0.1s since last print)
  â†˜ grad L2 norm â‰ˆ 5.302e+00 (batch 0)
  ... step 2/93  loss=0.2054  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.755e+00 (batch 2)
  ... step 4/93  loss=0.2889  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.161e+00 (batch 4)
  ... step 6/93  loss=0.3408  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.830e+00 (batch 6)
  ... step 8/93  loss=0.3153  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.308e+00 (batch 8)
  ... step 10/93  loss=0.3186  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.152e+01 (batch 10)
  ... step 12/93  loss=0.3791  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.035e+01 (batch 12)
  ... step 14/93  loss=0.3613  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.160e+01 (batch 14)
  ... step 16/93  loss=0.2826  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.201e+00 (batch 16)
  ... step 18/93  loss=0.1040  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.111e+00 (batch 18)
  ... step 20/93  loss=0.2851  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.857e+00 (batch 20)
  ... step 22/93  loss=0.5984  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.265e+00 (batch 22)
  ... step 24/93  loss=0.3690  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.866e+00 (batch 24)
  ... step 26/93  loss=0.3779  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.789e+00 (batch 26)
  ... step 28/93  loss=0.1800  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.594e+00 (batch 28)
  ... step 30/93  loss=0.2994  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.391e+00 (batch 30)
  ... step 32/93  loss=0.3317  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.475e+00 (batch 32)
  ... step 34/93  loss=0.4378  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.660e+00 (batch 34)
  ... step 36/93  loss=0.7966  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.556e+01 (batch 36)
  ... step 38/93  loss=0.2301  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.047e+00 (batch 38)
  ... step 40/93  loss=0.2056  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.589e+00 (batch 40)
  ... step 42/93  loss=0.3571  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.771e+00 (batch 42)
  ... step 44/93  loss=0.1171  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.906e+00 (batch 44)
  ... step 46/93  loss=0.3525  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.619e+00 (batch 46)
  ... step 48/93  loss=0.0849  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.599e+00 (batch 48)
  ... step 50/93  loss=0.2428  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.002e+00 (batch 50)
  ... step 52/93  loss=0.3286  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.594e+00 (batch 52)
  ... step 54/93  loss=0.4921  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.420e+00 (batch 54)
  ... step 56/93  loss=0.1991  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.647e+00 (batch 56)
  ... step 58/93  loss=0.2356  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.005e+01 (batch 58)
  ... step 60/93  loss=0.1208  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.129e+00 (batch 60)
  ... step 62/93  loss=0.4488  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.621e+01 (batch 62)
  ... step 64/93  loss=0.4200  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.626e+01 (batch 64)
  ... step 66/93  loss=0.2449  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.746e+00 (batch 66)
  ... step 68/93  loss=0.2677  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.576e+00 (batch 68)
  ... step 70/93  loss=0.5359  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.115e+01 (batch 70)
  ... step 72/93  loss=0.3241  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.147e+01 (batch 72)
  ... step 74/93  loss=0.4980  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.179e+01 (batch 74)
  ... step 76/93  loss=0.1108  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.032e+00 (batch 76)
  ... step 78/93  loss=0.2135  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.714e+00 (batch 78)
  ... step 80/93  loss=0.3011  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.458e+00 (batch 80)
  ... step 82/93  loss=0.3437  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.295e+00 (batch 82)
  ... step 84/93  loss=0.4359  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.708e+00 (batch 84)
  ... step 86/93  loss=0.2829  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.437e+00 (batch 86)
  ... step 88/93  loss=0.2015  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.861e+00 (batch 88)
  ... step 90/93  loss=0.1735  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.457e+00 (batch 90)
  ... step 92/93  loss=0.5524  (0.3s since last print)
  â†˜ grad L2 norm â‰ˆ 1.408e+01 (batch 92)
âœ… epoch 30 forward/backward done in 27.4s
  ðŸ”Ž val step 0: batch (4, 32, 2, 324, 1, 16) loss=0.3392
[30] train=0.3122  val=0.7999  RMSE(std)=[Qi:0.879, Qe:0.930, Î“:0.873]  RMSE(phys)=[Qi:63.846, Qe:86.075, Î“:38.537]  (1.7s)
âœ… Saved unified NN â†’ ./mnt/data/bin.cgyro.nn
Saved metrics â†’ ./mnt/data/myrun_logs_deep_debug_ordered_t/metrics.csv
âœ… Final unified NN â†’ ./mnt/data/bin.cgyro.nn
âœ… Training complete. Artifacts in: ./mnt/data/myrun_logs_deep_debug_ordered_t
