[build_datasets] windows: total=815  train=489  val=163  test=163

[train] dataset:
  y_mean: [[[134.60855 203.21703  80.05728]]]
  y_std : [[[ 85.10512 110.49066  51.77117]]]
  sample Y: min=-1.763e+00, max=2.779e+00, mean=1.555e-02, std=1.035e+00

[val] dataset:
  y_mean: [[[134.60855 203.21703  80.05728]]]
  y_std : [[[ 85.10512 110.49066  51.77117]]]
  sample Y: min=-1.756e+00, max=2.662e+00, mean=-1.294e-01, std=9.179e-01

[test] dataset:
  y_mean: [[[134.60855 203.21703  80.05728]]]
  y_std : [[[ 85.10512 110.49066  51.77117]]]
  sample Y: min=-1.765e+00, max=2.763e+00, mean=2.595e-02, std=1.020e+00
âœ… Saved flux histograms in ./mnt/data/myrun_logs_deep_debug
ðŸŸ¦ Starting epoch 1/15 (train steps â‰ˆ 123)
[1510049] Î¦2FluxDeep forward: input (4, 32, 2, 324, 1, 16)
  ... step 0/123  loss=2.0549  (2.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.737e+01 (batch 0)
  ... step 2/123  loss=0.7656  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.097e+01 (batch 2)
  ... step 4/123  loss=0.4550  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.873e+00 (batch 4)
  ... step 6/123  loss=1.1449  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.027e+01 (batch 6)
  ... step 8/123  loss=0.4511  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.825e+00 (batch 8)
  ... step 10/123  loss=0.7198  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.097e+01 (batch 10)
  ... step 12/123  loss=1.1020  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.148e+01 (batch 12)
  ... step 14/123  loss=2.1491  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.671e+01 (batch 14)
  ... step 16/123  loss=0.5271  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.812e+00 (batch 16)
  ... step 18/123  loss=0.8436  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.054e+00 (batch 18)
  ... step 20/123  loss=0.7326  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.748e+00 (batch 20)
  ... step 22/123  loss=1.0779  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.259e+01 (batch 22)
  ... step 24/123  loss=0.6435  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.353e+00 (batch 24)
  ... step 26/123  loss=0.8003  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.346e+00 (batch 26)
  ... step 28/123  loss=0.6510  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.125e+00 (batch 28)
  ... step 30/123  loss=0.2009  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.853e+00 (batch 30)
  ... step 32/123  loss=0.6101  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.394e+00 (batch 32)
  ... step 34/123  loss=0.4626  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.081e+01 (batch 34)
  ... step 36/123  loss=1.1044  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.207e+00 (batch 36)
  ... step 38/123  loss=1.1886  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.167e+01 (batch 38)
  ... step 40/123  loss=0.6012  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.965e+00 (batch 40)
  ... step 42/123  loss=1.2333  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.199e+01 (batch 42)
  ... step 44/123  loss=0.4839  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.652e+00 (batch 44)
  ... step 46/123  loss=1.1903  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.123e+01 (batch 46)
  ... step 48/123  loss=0.7554  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.133e+00 (batch 48)
  ... step 50/123  loss=1.2044  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.330e+00 (batch 50)
  ... step 52/123  loss=0.6328  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.589e+00 (batch 52)
  ... step 54/123  loss=1.4164  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.894e+00 (batch 54)
  ... step 56/123  loss=1.1332  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.279e+00 (batch 56)
  ... step 58/123  loss=0.9517  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.772e+00 (batch 58)
  ... step 60/123  loss=0.6130  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.597e+00 (batch 60)
  ... step 62/123  loss=0.7957  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.261e+00 (batch 62)
  ... step 64/123  loss=0.2731  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.024e+00 (batch 64)
  ... step 66/123  loss=0.8572  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.152e+00 (batch 66)
  ... step 68/123  loss=1.0611  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.134e+00 (batch 68)
  ... step 70/123  loss=0.4893  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.564e+00 (batch 70)
  ... step 72/123  loss=1.1803  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.949e+00 (batch 72)
  ... step 74/123  loss=0.8490  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.179e+00 (batch 74)
  ... step 76/123  loss=1.2980  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.651e+00 (batch 76)
  ... step 78/123  loss=0.7503  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.085e+00 (batch 78)
  ... step 80/123  loss=0.3621  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.612e+00 (batch 80)
  ... step 82/123  loss=0.6502  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.990e+00 (batch 82)
  ... step 84/123  loss=1.2660  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.569e+00 (batch 84)
  ... step 86/123  loss=1.1944  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.191e+00 (batch 86)
  ... step 88/123  loss=0.4913  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.932e+00 (batch 88)
  ... step 90/123  loss=0.6129  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.388e+00 (batch 90)
  ... step 92/123  loss=0.2058  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.844e+00 (batch 92)
  ... step 94/123  loss=0.5989  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.266e+00 (batch 94)
  ... step 96/123  loss=1.1171  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.331e+00 (batch 96)
  ... step 98/123  loss=0.7599  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.281e+00 (batch 98)
  ... step 100/123  loss=0.6570  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.785e+00 (batch 100)
  ... step 102/123  loss=1.2895  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.094e+00 (batch 102)
  ... step 104/123  loss=0.5999  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.031e+00 (batch 104)
  ... step 106/123  loss=1.1181  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.259e+00 (batch 106)
  ... step 108/123  loss=0.7230  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.493e+00 (batch 108)
  ... step 110/123  loss=0.5733  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.791e+00 (batch 110)
  ... step 112/123  loss=0.4717  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.282e+00 (batch 112)
  ... step 114/123  loss=0.7903  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.676e+00 (batch 114)
  ... step 116/123  loss=0.6886  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.292e+00 (batch 116)
  ... step 118/123  loss=0.5135  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.868e+00 (batch 118)
  ... step 120/123  loss=0.9925  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.486e+00 (batch 120)
  ... step 122/123  loss=0.9328  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.679e+00 (batch 122)
âœ… epoch 1 forward/backward done in 38.5s
  ðŸ”Ž val step 0: batch (4, 32, 2, 324, 1, 16) loss=19.1055
[01] train=0.8011  val=19.6674  RMSE(std)=[Qi:4.527, Qe:4.140, Î“:4.623]  RMSE(phys)=[Qi:385.230, Qe:457.420, Î“:239.350]  (2.2s)
âœ… Saved unified NN â†’ ./mnt/data/bin.cgyro.nn
ðŸŸ¦ Starting epoch 2/15 (train steps â‰ˆ 123)
  ... step 0/123  loss=0.3904  (0.1s since last print)
  â†˜ grad L2 norm â‰ˆ 3.364e+00 (batch 0)
  ... step 2/123  loss=0.4516  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.709e+00 (batch 2)
  ... step 4/123  loss=0.9697  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.486e+00 (batch 4)
  ... step 6/123  loss=0.3320  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.515e+00 (batch 6)
  ... step 8/123  loss=0.3157  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.186e+00 (batch 8)
  ... step 10/123  loss=0.7364  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.484e+00 (batch 10)
  ... step 12/123  loss=0.5583  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.851e+00 (batch 12)
  ... step 14/123  loss=0.3623  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.519e+00 (batch 14)
  ... step 16/123  loss=0.1867  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.196e+00 (batch 16)
  ... step 18/123  loss=0.2320  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.261e+00 (batch 18)
  ... step 20/123  loss=0.5368  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.411e+00 (batch 20)
  ... step 22/123  loss=0.2134  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.075e+00 (batch 22)
  ... step 24/123  loss=0.7625  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.099e+00 (batch 24)
  ... step 26/123  loss=0.6211  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.214e+00 (batch 26)
  ... step 28/123  loss=0.3106  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.824e+00 (batch 28)
  ... step 30/123  loss=0.5014  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.281e+00 (batch 30)
  ... step 32/123  loss=0.3606  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.520e+00 (batch 32)
  ... step 34/123  loss=0.2598  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.559e+00 (batch 34)
  ... step 36/123  loss=0.3582  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.046e+00 (batch 36)
  ... step 38/123  loss=0.5220  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.571e+00 (batch 38)
  ... step 40/123  loss=0.3499  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.096e+00 (batch 40)
  ... step 42/123  loss=0.9053  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.630e+00 (batch 42)
  ... step 44/123  loss=0.9985  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.757e+00 (batch 44)
  ... step 46/123  loss=0.6938  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.532e+00 (batch 46)
  ... step 48/123  loss=0.7367  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.423e+00 (batch 48)
  ... step 50/123  loss=1.4615  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.864e+00 (batch 50)
  ... step 52/123  loss=0.1727  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.633e+00 (batch 52)
  ... step 54/123  loss=0.4462  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.957e+00 (batch 54)
  ... step 56/123  loss=0.4817  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.776e+00 (batch 56)
  ... step 58/123  loss=1.4266  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.471e+00 (batch 58)
  ... step 60/123  loss=1.1964  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.516e+00 (batch 60)
  ... step 62/123  loss=0.7387  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.611e+00 (batch 62)
  ... step 64/123  loss=1.3404  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.416e+00 (batch 64)
  ... step 66/123  loss=0.3404  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.846e+00 (batch 66)
  ... step 68/123  loss=1.0930  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.445e+00 (batch 68)
  ... step 70/123  loss=0.8446  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.346e+00 (batch 70)
  ... step 72/123  loss=0.9800  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.048e+00 (batch 72)
  ... step 74/123  loss=0.9122  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.914e+00 (batch 74)
  ... step 76/123  loss=0.5134  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.212e+00 (batch 76)
  ... step 78/123  loss=0.6741  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.638e+00 (batch 78)
  ... step 80/123  loss=0.4002  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.223e+00 (batch 80)
  ... step 82/123  loss=0.3270  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.401e+00 (batch 82)
  ... step 84/123  loss=0.5920  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.937e+00 (batch 84)
  ... step 86/123  loss=0.3529  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.408e+00 (batch 86)
  ... step 88/123  loss=0.5462  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.991e+00 (batch 88)
  ... step 90/123  loss=0.0824  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.677e-01 (batch 90)
  ... step 92/123  loss=0.4104  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.765e+00 (batch 92)
  ... step 94/123  loss=0.9792  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.442e+00 (batch 94)
  ... step 96/123  loss=1.2194  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.153e+00 (batch 96)
  ... step 98/123  loss=0.3355  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.794e+00 (batch 98)
  ... step 100/123  loss=0.5695  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.817e+00 (batch 100)
  ... step 102/123  loss=1.2248  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.166e+00 (batch 102)
  ... step 104/123  loss=1.0244  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.803e+00 (batch 104)
  ... step 106/123  loss=0.8840  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.593e+00 (batch 106)
  ... step 108/123  loss=0.7678  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.259e+00 (batch 108)
  ... step 110/123  loss=0.4056  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.246e+00 (batch 110)
  ... step 112/123  loss=0.7265  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.556e+00 (batch 112)
  ... step 114/123  loss=0.9121  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.016e+00 (batch 114)
  ... step 116/123  loss=0.9608  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.136e+00 (batch 116)
  ... step 118/123  loss=0.6010  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.189e+00 (batch 118)
  ... step 120/123  loss=0.3819  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.644e+00 (batch 120)
  ... step 122/123  loss=0.4176  (0.3s since last print)
  â†˜ grad L2 norm â‰ˆ 3.180e+00 (batch 122)
âœ… epoch 2 forward/backward done in 36.3s
  ðŸ”Ž val step 0: batch (4, 32, 2, 324, 1, 16) loss=59.5377
[02] train=0.6148  val=60.6603  RMSE(std)=[Qi:7.604, Qe:7.995, Î“:7.762]  RMSE(phys)=[Qi:647.110, Qe:883.403, Î“:401.823]  (2.2s)
âœ… Saved unified NN â†’ ./mnt/data/bin.cgyro.nn
ðŸŸ¦ Starting epoch 3/15 (train steps â‰ˆ 123)
  ... step 0/123  loss=0.9433  (0.1s since last print)
  â†˜ grad L2 norm â‰ˆ 2.386e+00 (batch 0)
  ... step 2/123  loss=0.7464  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.597e+00 (batch 2)
  ... step 4/123  loss=1.0178  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.317e+00 (batch 4)
  ... step 6/123  loss=0.4786  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.614e+00 (batch 6)
  ... step 8/123  loss=0.2789  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.501e+00 (batch 8)
  ... step 10/123  loss=0.3745  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.875e+00 (batch 10)
  ... step 12/123  loss=0.3352  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.368e+00 (batch 12)
  ... step 14/123  loss=0.3844  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.461e+00 (batch 14)
  ... step 16/123  loss=0.5396  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.653e+00 (batch 16)
  ... step 18/123  loss=1.1065  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.399e+00 (batch 18)
  ... step 20/123  loss=0.4163  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.827e+00 (batch 20)
  ... step 22/123  loss=1.3125  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.819e+00 (batch 22)
  ... step 24/123  loss=1.3068  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.146e+00 (batch 24)
  ... step 26/123  loss=0.2060  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.069e+00 (batch 26)
  ... step 28/123  loss=0.7562  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.591e+00 (batch 28)
  ... step 30/123  loss=0.1796  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.390e+00 (batch 30)
  ... step 32/123  loss=1.1367  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.240e+00 (batch 32)
  ... step 34/123  loss=1.1456  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.343e+00 (batch 34)
  ... step 36/123  loss=0.3565  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.173e+00 (batch 36)
  ... step 38/123  loss=0.2362  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.597e+00 (batch 38)
  ... step 40/123  loss=0.8093  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.587e+00 (batch 40)
  ... step 42/123  loss=0.1631  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.822e+00 (batch 42)
  ... step 44/123  loss=0.9488  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.755e+00 (batch 44)
  ... step 46/123  loss=0.9776  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.137e+00 (batch 46)
  ... step 48/123  loss=0.5515  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.025e+00 (batch 48)
  ... step 50/123  loss=0.9907  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.857e+00 (batch 50)
  ... step 52/123  loss=0.6341  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.682e+00 (batch 52)
  ... step 54/123  loss=0.1531  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.394e+00 (batch 54)
  ... step 56/123  loss=1.2993  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.037e+00 (batch 56)
  ... step 58/123  loss=0.6009  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.479e+00 (batch 58)
  ... step 60/123  loss=0.1605  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.462e+00 (batch 60)
  ... step 62/123  loss=0.3431  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.243e+00 (batch 62)
  ... step 64/123  loss=0.4441  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.302e+00 (batch 64)
  ... step 66/123  loss=0.8032  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.310e+00 (batch 66)
  ... step 68/123  loss=0.5232  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.915e+00 (batch 68)
  ... step 70/123  loss=0.1466  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.086e+00 (batch 70)
  ... step 72/123  loss=0.4320  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.275e+00 (batch 72)
  ... step 74/123  loss=0.9275  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.022e+00 (batch 74)
  ... step 76/123  loss=0.3094  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.105e+00 (batch 76)
  ... step 78/123  loss=0.3384  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.148e+00 (batch 78)
  ... step 80/123  loss=0.6813  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.431e+00 (batch 80)
  ... step 82/123  loss=0.4084  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.712e+00 (batch 82)
  ... step 84/123  loss=0.1437  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.276e-01 (batch 84)
  ... step 86/123  loss=0.6192  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.375e+00 (batch 86)
  ... step 88/123  loss=1.0509  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.472e+00 (batch 88)
  ... step 90/123  loss=0.4687  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.759e+00 (batch 90)
  ... step 92/123  loss=0.2426  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.621e+00 (batch 92)
  ... step 94/123  loss=0.3907  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.378e+00 (batch 94)
  ... step 96/123  loss=1.0808  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.257e+00 (batch 96)
  ... step 98/123  loss=0.1414  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.348e+00 (batch 98)
  ... step 100/123  loss=0.5382  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.651e+00 (batch 100)
  ... step 102/123  loss=0.0776  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.800e-01 (batch 102)
  ... step 104/123  loss=0.6677  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.381e+00 (batch 104)
  ... step 106/123  loss=0.5116  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.169e+00 (batch 106)
  ... step 108/123  loss=0.3500  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.458e+00 (batch 108)
  ... step 110/123  loss=0.9596  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.533e+00 (batch 110)
  ... step 112/123  loss=0.4328  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.348e+00 (batch 112)
  ... step 114/123  loss=0.8650  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.523e+00 (batch 114)
  ... step 116/123  loss=0.5489  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.490e+00 (batch 116)
  ... step 118/123  loss=0.4145  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.347e+00 (batch 118)
  ... step 120/123  loss=0.5461  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.991e+00 (batch 120)
  ... step 122/123  loss=0.4060  (0.3s since last print)
  â†˜ grad L2 norm â‰ˆ 5.134e+00 (batch 122)
âœ… epoch 3 forward/backward done in 36.3s
  ðŸ”Ž val step 0: batch (4, 32, 2, 324, 1, 16) loss=21.7791
[03] train=0.6351  val=22.1570  RMSE(std)=[Qi:4.848, Qe:4.805, Î“:4.459]  RMSE(phys)=[Qi:412.576, Qe:530.931, Î“:230.829]  (2.2s)
âœ… Saved unified NN â†’ ./mnt/data/bin.cgyro.nn
ðŸŸ¦ Starting epoch 4/15 (train steps â‰ˆ 123)
  ... step 0/123  loss=0.8091  (0.1s since last print)
  â†˜ grad L2 norm â‰ˆ 1.948e+00 (batch 0)
  ... step 2/123  loss=0.1578  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.740e-01 (batch 2)
  ... step 4/123  loss=0.6159  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.961e+00 (batch 4)
  ... step 6/123  loss=0.2588  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.277e+00 (batch 6)
  ... step 8/123  loss=0.7629  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.300e+00 (batch 8)
  ... step 10/123  loss=0.1065  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.838e-01 (batch 10)
  ... step 12/123  loss=0.2389  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.399e+00 (batch 12)
  ... step 14/123  loss=0.5966  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.626e+00 (batch 14)
  ... step 16/123  loss=0.3923  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.655e+00 (batch 16)
  ... step 18/123  loss=1.1993  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.724e+00 (batch 18)
  ... step 20/123  loss=0.1444  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.197e+00 (batch 20)
  ... step 22/123  loss=0.3600  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.375e+00 (batch 22)
  ... step 24/123  loss=0.6798  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.570e+00 (batch 24)
  ... step 26/123  loss=0.2276  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.608e+00 (batch 26)
  ... step 28/123  loss=0.2776  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.028e+00 (batch 28)
  ... step 30/123  loss=0.4248  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.042e+00 (batch 30)
  ... step 32/123  loss=0.0452  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.674e-01 (batch 32)
  ... step 34/123  loss=0.3677  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.388e+00 (batch 34)
  ... step 36/123  loss=0.2531  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.041e+00 (batch 36)
  ... step 38/123  loss=0.5540  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.667e+00 (batch 38)
  ... step 40/123  loss=1.5426  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.621e+00 (batch 40)
  ... step 42/123  loss=0.1826  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.153e+00 (batch 42)
  ... step 44/123  loss=1.0257  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.874e+00 (batch 44)
  ... step 46/123  loss=0.5702  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.900e+00 (batch 46)
  ... step 48/123  loss=0.2740  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.578e+00 (batch 48)
  ... step 50/123  loss=0.3392  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.275e+00 (batch 50)
  ... step 52/123  loss=1.0110  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.598e+00 (batch 52)
  ... step 54/123  loss=0.2750  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.705e+00 (batch 54)
  ... step 56/123  loss=0.3714  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.493e+00 (batch 56)
  ... step 58/123  loss=0.5923  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.306e+00 (batch 58)
  ... step 60/123  loss=0.4786  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.698e+00 (batch 60)
  ... step 62/123  loss=1.0094  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.623e+00 (batch 62)
  ... step 64/123  loss=0.8484  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.775e+00 (batch 64)
  ... step 66/123  loss=0.5988  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.360e+00 (batch 66)
  ... step 68/123  loss=0.1951  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.449e+00 (batch 68)
  ... step 70/123  loss=0.3547  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.104e+00 (batch 70)
  ... step 72/123  loss=0.2319  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.310e+00 (batch 72)
  ... step 74/123  loss=0.3480  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.187e+00 (batch 74)
  ... step 76/123  loss=0.2023  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.321e+00 (batch 76)
  ... step 78/123  loss=1.7165  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.378e+00 (batch 78)
  ... step 80/123  loss=0.5158  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.649e+00 (batch 80)
  ... step 82/123  loss=0.8996  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.804e+00 (batch 82)
  ... step 84/123  loss=0.2797  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.977e+00 (batch 84)
  ... step 86/123  loss=0.5157  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.106e+00 (batch 86)
  ... step 88/123  loss=0.1473  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.048e+00 (batch 88)
  ... step 90/123  loss=0.5458  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.805e+00 (batch 90)
  ... step 92/123  loss=0.8672  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.926e+00 (batch 92)
  ... step 94/123  loss=0.6844  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.694e+00 (batch 94)
  ... step 96/123  loss=1.5849  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.308e+00 (batch 96)
  ... step 98/123  loss=0.6760  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.470e+00 (batch 98)
  ... step 100/123  loss=1.1170  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.869e+00 (batch 100)
  ... step 102/123  loss=0.6038  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.857e+00 (batch 102)
  ... step 104/123  loss=0.2009  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.417e+00 (batch 104)
  ... step 106/123  loss=0.2105  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.388e+00 (batch 106)
  ... step 108/123  loss=0.4452  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.727e+00 (batch 108)
  ... step 110/123  loss=0.5443  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.804e+00 (batch 110)
  ... step 112/123  loss=0.2474  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.192e+00 (batch 112)
  ... step 114/123  loss=0.3732  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.717e+00 (batch 114)
  ... step 116/123  loss=0.2566  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.259e+00 (batch 116)
  ... step 118/123  loss=0.4385  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.094e+00 (batch 118)
  ... step 120/123  loss=0.4806  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.771e+00 (batch 120)
  ... step 122/123  loss=1.4652  (0.3s since last print)
  â†˜ grad L2 norm â‰ˆ 1.081e+01 (batch 122)
âœ… epoch 4 forward/backward done in 36.3s
  ðŸ”Ž val step 0: batch (4, 32, 2, 324, 1, 16) loss=42.8247
[04] train=0.5755  val=43.5934  RMSE(std)=[Qi:6.553, Qe:6.551, Î“:6.703]  RMSE(phys)=[Qi:557.657, Qe:723.852, Î“:347.002]  (2.2s)
âœ… Saved unified NN â†’ ./mnt/data/bin.cgyro.nn
ðŸŸ¦ Starting epoch 5/15 (train steps â‰ˆ 123)
  ... step 0/123  loss=0.2713  (0.1s since last print)
  â†˜ grad L2 norm â‰ˆ 2.124e+00 (batch 0)
  ... step 2/123  loss=0.3783  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.829e+00 (batch 2)
  ... step 4/123  loss=0.3546  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.570e+00 (batch 4)
  ... step 6/123  loss=0.7984  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.217e+00 (batch 6)
  ... step 8/123  loss=0.7342  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.366e+00 (batch 8)
  ... step 10/123  loss=0.3395  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.244e+00 (batch 10)
  ... step 12/123  loss=0.9102  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.939e+00 (batch 12)
  ... step 14/123  loss=0.2295  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.093e+00 (batch 14)
  ... step 16/123  loss=0.7666  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.837e+00 (batch 16)
  ... step 18/123  loss=0.5344  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.959e+00 (batch 18)
  ... step 20/123  loss=0.7089  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.971e+00 (batch 20)
  ... step 22/123  loss=0.4853  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.303e+00 (batch 22)
  ... step 24/123  loss=0.0767  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.915e-01 (batch 24)
  ... step 26/123  loss=0.7778  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.932e+00 (batch 26)
  ... step 28/123  loss=0.4550  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.742e+00 (batch 28)
  ... step 30/123  loss=0.6268  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.056e+00 (batch 30)
  ... step 32/123  loss=1.0733  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.896e+00 (batch 32)
  ... step 34/123  loss=0.6656  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.464e+00 (batch 34)
  ... step 36/123  loss=0.3800  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.177e+00 (batch 36)
  ... step 38/123  loss=0.7684  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.514e+00 (batch 38)
  ... step 40/123  loss=0.5882  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.242e+00 (batch 40)
  ... step 42/123  loss=0.3718  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.695e+00 (batch 42)
  ... step 44/123  loss=0.5155  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.961e+00 (batch 44)
  ... step 46/123  loss=0.3232  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.955e+00 (batch 46)
  ... step 48/123  loss=0.3674  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.760e+00 (batch 48)
  ... step 50/123  loss=0.2522  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.312e+00 (batch 50)
  ... step 52/123  loss=0.1675  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.292e-01 (batch 52)
  ... step 54/123  loss=0.5286  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.391e+00 (batch 54)
  ... step 56/123  loss=0.6803  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.645e+00 (batch 56)
  ... step 58/123  loss=0.2776  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.307e+00 (batch 58)
  ... step 60/123  loss=0.4356  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.941e+00 (batch 60)
  ... step 62/123  loss=0.4973  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.405e+00 (batch 62)
  ... step 64/123  loss=2.0297  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.113e+00 (batch 64)
  ... step 66/123  loss=0.6865  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.576e+00 (batch 66)
  ... step 68/123  loss=0.7814  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.008e+00 (batch 68)
  ... step 70/123  loss=0.4997  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.118e+00 (batch 70)
  ... step 72/123  loss=0.9297  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.527e+00 (batch 72)
  ... step 74/123  loss=1.4625  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.990e+00 (batch 74)
  ... step 76/123  loss=0.2896  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.389e+00 (batch 76)
  ... step 78/123  loss=0.2437  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.205e+00 (batch 78)
  ... step 80/123  loss=1.1475  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.448e+00 (batch 80)
  ... step 82/123  loss=0.8969  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.861e+00 (batch 82)
  ... step 84/123  loss=0.3851  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.633e+00 (batch 84)
  ... step 86/123  loss=0.3974  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.218e+00 (batch 86)
  ... step 88/123  loss=0.3353  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.595e+00 (batch 88)
  ... step 90/123  loss=0.4202  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.413e+00 (batch 90)
  ... step 92/123  loss=0.2852  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.771e+00 (batch 92)
  ... step 94/123  loss=0.3941  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.792e+00 (batch 94)
  ... step 96/123  loss=0.2414  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.297e+00 (batch 96)
  ... step 98/123  loss=0.6191  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.759e+00 (batch 98)
  ... step 100/123  loss=0.2900  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.173e+00 (batch 100)
  ... step 102/123  loss=0.9255  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.224e+00 (batch 102)
  ... step 104/123  loss=1.3811  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.207e+00 (batch 104)
  ... step 106/123  loss=0.8486  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.293e+00 (batch 106)
  ... step 108/123  loss=0.2383  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.769e+00 (batch 108)
  ... step 110/123  loss=0.6594  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.967e+00 (batch 110)
  ... step 112/123  loss=0.2158  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.690e+00 (batch 112)
  ... step 114/123  loss=0.7640  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.748e+00 (batch 114)
  ... step 116/123  loss=0.8255  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.409e+00 (batch 116)
  ... step 118/123  loss=0.8102  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.059e+00 (batch 118)
  ... step 120/123  loss=0.6593  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.198e+00 (batch 120)
  ... step 122/123  loss=0.1019  (0.3s since last print)
  â†˜ grad L2 norm â‰ˆ 1.218e+00 (batch 122)
âœ… epoch 5 forward/backward done in 36.3s
  ðŸ”Ž val step 0: batch (4, 32, 2, 324, 1, 16) loss=5.4804
[05] train=0.5719  val=6.0526  RMSE(std)=[Qi:2.381, Qe:2.576, Î“:2.419]  RMSE(phys)=[Qi:202.630, Qe:284.664, Î“:125.231]  (2.2s)
âœ… Saved unified NN â†’ ./mnt/data/bin.cgyro.nn
ðŸŸ¦ Starting epoch 6/15 (train steps â‰ˆ 123)
  ... step 0/123  loss=1.1288  (0.1s since last print)
  â†˜ grad L2 norm â‰ˆ 2.147e+00 (batch 0)
  ... step 2/123  loss=1.2603  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.301e+00 (batch 2)
  ... step 4/123  loss=0.2841  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.299e+00 (batch 4)
  ... step 6/123  loss=0.7891  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.645e+00 (batch 6)
  ... step 8/123  loss=0.3557  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.822e+00 (batch 8)
  ... step 10/123  loss=0.1985  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.163e+00 (batch 10)
  ... step 12/123  loss=0.2969  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.809e+00 (batch 12)
  ... step 14/123  loss=0.9189  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.032e+00 (batch 14)
  ... step 16/123  loss=0.1151  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.031e+00 (batch 16)
  ... step 18/123  loss=0.2832  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.966e+00 (batch 18)
  ... step 20/123  loss=0.4938  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.841e+00 (batch 20)
  ... step 22/123  loss=0.3730  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.979e+00 (batch 22)
  ... step 24/123  loss=0.7578  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.522e+00 (batch 24)
  ... step 26/123  loss=0.7139  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.362e+00 (batch 26)
  ... step 28/123  loss=0.3717  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.701e+00 (batch 28)
  ... step 30/123  loss=0.4335  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.497e+00 (batch 30)
  ... step 32/123  loss=0.5086  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.175e+00 (batch 32)
  ... step 34/123  loss=0.4943  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.298e+00 (batch 34)
  ... step 36/123  loss=0.1832  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.510e-01 (batch 36)
  ... step 38/123  loss=0.3374  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.679e+00 (batch 38)
  ... step 40/123  loss=1.4707  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.763e+00 (batch 40)
  ... step 42/123  loss=1.2845  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.009e+00 (batch 42)
  ... step 44/123  loss=0.1335  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.950e-01 (batch 44)
  ... step 46/123  loss=0.4586  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.421e+00 (batch 46)
  ... step 48/123  loss=0.3837  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.554e+00 (batch 48)
  ... step 50/123  loss=0.2004  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.447e+00 (batch 50)
  ... step 52/123  loss=0.0749  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.824e-01 (batch 52)
  ... step 54/123  loss=1.0823  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.989e+00 (batch 54)
  ... step 56/123  loss=1.9471  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.462e+00 (batch 56)
  ... step 58/123  loss=0.3529  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.174e+00 (batch 58)
  ... step 60/123  loss=0.3938  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.351e+00 (batch 60)
  ... step 62/123  loss=0.1919  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.345e+00 (batch 62)
  ... step 64/123  loss=0.4257  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.906e+00 (batch 64)
  ... step 66/123  loss=0.4292  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.038e+00 (batch 66)
  ... step 68/123  loss=0.3425  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.983e-01 (batch 68)
  ... step 70/123  loss=0.2055  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.152e+00 (batch 70)
  ... step 72/123  loss=0.3102  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.152e+00 (batch 72)
  ... step 74/123  loss=0.5277  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.088e+00 (batch 74)
  ... step 76/123  loss=1.0767  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.565e+00 (batch 76)
  ... step 78/123  loss=0.9839  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.060e+00 (batch 78)
  ... step 80/123  loss=0.4746  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.748e+00 (batch 80)
  ... step 82/123  loss=0.4874  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.231e+00 (batch 82)
  ... step 84/123  loss=0.5912  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.825e+00 (batch 84)
  ... step 86/123  loss=0.3287  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.204e+00 (batch 86)
  ... step 88/123  loss=0.6589  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.332e+00 (batch 88)
  ... step 90/123  loss=1.1570  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.057e+00 (batch 90)
  ... step 92/123  loss=0.6794  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.052e+00 (batch 92)
  ... step 94/123  loss=0.2944  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.592e+00 (batch 94)
  ... step 96/123  loss=0.6220  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.793e+00 (batch 96)
  ... step 98/123  loss=0.2154  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.837e+00 (batch 98)
  ... step 100/123  loss=0.1912  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.685e+00 (batch 100)
  ... step 102/123  loss=0.4220  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.321e+00 (batch 102)
  ... step 104/123  loss=0.3465  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.177e+00 (batch 104)
  ... step 106/123  loss=0.4962  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.482e+00 (batch 106)
  ... step 108/123  loss=0.5274  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.144e+00 (batch 108)
  ... step 110/123  loss=0.4521  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.326e+00 (batch 110)
  ... step 112/123  loss=0.1836  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.727e+00 (batch 112)
  ... step 114/123  loss=1.3237  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.594e+00 (batch 114)
  ... step 116/123  loss=0.8930  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.514e+00 (batch 116)
  ... step 118/123  loss=0.2600  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.832e+00 (batch 118)
  ... step 120/123  loss=0.1280  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.077e+00 (batch 120)
  ... step 122/123  loss=1.0556  (0.3s since last print)
  â†˜ grad L2 norm â‰ˆ 8.416e+00 (batch 122)
âœ… epoch 6 forward/backward done in 36.3s
  ðŸ”Ž val step 0: batch (4, 32, 2, 324, 1, 16) loss=17.0326
[06] train=0.5403  val=17.5353  RMSE(std)=[Qi:4.228, Qe:4.232, Î“:4.102]  RMSE(phys)=[Qi:359.786, Qe:467.581, Î“:212.358]  (2.2s)
âœ… Saved unified NN â†’ ./mnt/data/bin.cgyro.nn
ðŸŸ¦ Starting epoch 7/15 (train steps â‰ˆ 123)
  ... step 0/123  loss=0.3014  (0.1s since last print)
  â†˜ grad L2 norm â‰ˆ 1.329e+00 (batch 0)
  ... step 2/123  loss=0.3556  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.036e+00 (batch 2)
  ... step 4/123  loss=0.6114  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.988e+00 (batch 4)
  ... step 6/123  loss=0.4630  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.651e+00 (batch 6)
  ... step 8/123  loss=0.5320  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.809e+00 (batch 8)
  ... step 10/123  loss=0.3579  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.774e+00 (batch 10)
  ... step 12/123  loss=0.2308  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.369e+00 (batch 12)
  ... step 14/123  loss=0.8572  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.728e+00 (batch 14)
  ... step 16/123  loss=0.3965  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.956e+00 (batch 16)
  ... step 18/123  loss=0.3474  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.451e+00 (batch 18)
  ... step 20/123  loss=0.9066  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.160e+00 (batch 20)
  ... step 22/123  loss=0.7661  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.318e+00 (batch 22)
  ... step 24/123  loss=0.4666  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.559e+00 (batch 24)
  ... step 26/123  loss=0.3656  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.272e+00 (batch 26)
  ... step 28/123  loss=1.1434  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.092e+00 (batch 28)
  ... step 30/123  loss=0.7951  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.508e+00 (batch 30)
  ... step 32/123  loss=0.5506  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.832e+00 (batch 32)
  ... step 34/123  loss=0.6559  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.589e+00 (batch 34)
  ... step 36/123  loss=0.3679  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.426e+00 (batch 36)
  ... step 38/123  loss=0.6421  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.271e+00 (batch 38)
  ... step 40/123  loss=0.4675  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.788e+00 (batch 40)
  ... step 42/123  loss=0.0837  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.014e+00 (batch 42)
  ... step 44/123  loss=0.0657  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.134e-01 (batch 44)
  ... step 46/123  loss=0.4998  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.384e+00 (batch 46)
  ... step 48/123  loss=0.2785  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.727e+00 (batch 48)
  ... step 50/123  loss=0.9039  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.022e+00 (batch 50)
  ... step 52/123  loss=0.2362  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.642e+00 (batch 52)
  ... step 54/123  loss=0.4845  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.212e+00 (batch 54)
  ... step 56/123  loss=0.4528  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.796e+00 (batch 56)
  ... step 58/123  loss=0.5433  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.217e+00 (batch 58)
  ... step 60/123  loss=0.3362  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.547e+00 (batch 60)
  ... step 62/123  loss=0.3922  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.648e+00 (batch 62)
  ... step 64/123  loss=0.9110  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.576e+00 (batch 64)
  ... step 66/123  loss=0.7346  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.750e+00 (batch 66)
  ... step 68/123  loss=0.3522  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.599e+00 (batch 68)
  ... step 70/123  loss=0.7582  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.317e+00 (batch 70)
  ... step 72/123  loss=0.6942  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.150e+00 (batch 72)
  ... step 74/123  loss=1.4041  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.290e+00 (batch 74)
  ... step 76/123  loss=0.4309  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.872e+00 (batch 76)
  ... step 78/123  loss=0.2457  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.263e+00 (batch 78)
  ... step 80/123  loss=0.6220  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.434e+00 (batch 80)
  ... step 82/123  loss=0.1814  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.165e+00 (batch 82)
  ... step 84/123  loss=0.7768  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.372e+00 (batch 84)
  ... step 86/123  loss=0.5759  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.196e+00 (batch 86)
  ... step 88/123  loss=0.2926  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.967e+00 (batch 88)
  ... step 90/123  loss=0.8273  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.096e+00 (batch 90)
  ... step 92/123  loss=0.4486  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.473e+00 (batch 92)
  ... step 94/123  loss=0.6021  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.561e+00 (batch 94)
  ... step 96/123  loss=0.2291  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.151e+00 (batch 96)
  ... step 98/123  loss=0.4110  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.145e+00 (batch 98)
  ... step 100/123  loss=0.4174  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.541e+00 (batch 100)
  ... step 102/123  loss=0.3337  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.929e+00 (batch 102)
  ... step 104/123  loss=0.3145  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.015e+00 (batch 104)
  ... step 106/123  loss=0.3279  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.594e+00 (batch 106)
  ... step 108/123  loss=0.6906  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.354e+00 (batch 108)
  ... step 110/123  loss=0.3763  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.509e+00 (batch 110)
  ... step 112/123  loss=0.8513  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.668e+00 (batch 112)
  ... step 114/123  loss=0.3708  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.024e+00 (batch 114)
  ... step 116/123  loss=0.3139  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.885e+00 (batch 116)
  ... step 118/123  loss=0.2673  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.117e+00 (batch 118)
  ... step 120/123  loss=0.1494  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.246e+00 (batch 120)
  ... step 122/123  loss=7.0879  (0.3s since last print)
  â†˜ grad L2 norm â‰ˆ 3.073e+01 (batch 122)
âœ… epoch 7 forward/backward done in 36.3s
  ðŸ”Ž val step 0: batch (4, 32, 2, 324, 1, 16) loss=12.4621
[07] train=0.5281  val=12.9047  RMSE(std)=[Qi:3.507, Qe:3.688, Î“:3.579]  RMSE(phys)=[Qi:298.502, Qe:407.529, Î“:185.280]  (2.2s)
âœ… Saved unified NN â†’ ./mnt/data/bin.cgyro.nn
ðŸŸ¦ Starting epoch 8/15 (train steps â‰ˆ 123)
  ... step 0/123  loss=0.2042  (0.1s since last print)
  â†˜ grad L2 norm â‰ˆ 1.072e+00 (batch 0)
  ... step 2/123  loss=1.2854  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.422e+00 (batch 2)
  ... step 4/123  loss=1.1273  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.252e+00 (batch 4)
  ... step 6/123  loss=0.6450  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.418e+00 (batch 6)
  ... step 8/123  loss=0.3876  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.232e-01 (batch 8)
  ... step 10/123  loss=0.2167  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.005e+00 (batch 10)
  ... step 12/123  loss=0.3478  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.191e+00 (batch 12)
  ... step 14/123  loss=1.0114  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.279e+00 (batch 14)
  ... step 16/123  loss=0.1451  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.263e-01 (batch 16)
  ... step 18/123  loss=0.3266  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.062e-01 (batch 18)
  ... step 20/123  loss=0.4584  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.443e+00 (batch 20)
  ... step 22/123  loss=0.3224  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.033e+00 (batch 22)
  ... step 24/123  loss=1.1255  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.552e+00 (batch 24)
  ... step 26/123  loss=0.5367  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.425e+00 (batch 26)
  ... step 28/123  loss=0.4199  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.556e+00 (batch 28)
  ... step 30/123  loss=0.4713  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.400e+00 (batch 30)
  ... step 32/123  loss=0.7277  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.835e+00 (batch 32)
  ... step 34/123  loss=0.6011  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.544e+00 (batch 34)
  ... step 36/123  loss=0.7408  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.965e+00 (batch 36)
  ... step 38/123  loss=0.4607  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.377e+00 (batch 38)
  ... step 40/123  loss=0.2913  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.022e+00 (batch 40)
  ... step 42/123  loss=0.4375  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.240e+00 (batch 42)
  ... step 44/123  loss=0.6746  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.849e+00 (batch 44)
  ... step 46/123  loss=0.4896  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.524e+00 (batch 46)
  ... step 48/123  loss=0.2888  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.093e+00 (batch 48)
  ... step 50/123  loss=0.2855  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.356e+00 (batch 50)
  ... step 52/123  loss=0.4570  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.290e+00 (batch 52)
  ... step 54/123  loss=0.3028  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.328e+00 (batch 54)
  ... step 56/123  loss=0.6023  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.038e+00 (batch 56)
  ... step 58/123  loss=0.4032  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.025e+00 (batch 58)
  ... step 60/123  loss=1.2327  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.402e+00 (batch 60)
  ... step 62/123  loss=0.6327  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.064e+00 (batch 62)
  ... step 64/123  loss=0.7941  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.848e+00 (batch 64)
  ... step 66/123  loss=0.2808  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.329e+00 (batch 66)
  ... step 68/123  loss=0.2822  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.908e+00 (batch 68)
  ... step 70/123  loss=0.2834  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.002e+00 (batch 70)
  ... step 72/123  loss=0.1639  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.219e+00 (batch 72)
  ... step 74/123  loss=0.7936  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.343e+00 (batch 74)
  ... step 76/123  loss=0.2285  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.326e+00 (batch 76)
  ... step 78/123  loss=0.7821  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.738e+00 (batch 78)
  ... step 80/123  loss=0.9367  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.738e+00 (batch 80)
  ... step 82/123  loss=0.1676  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.130e+00 (batch 82)
  ... step 84/123  loss=0.3508  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.733e+00 (batch 84)
  ... step 86/123  loss=0.1160  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.909e-01 (batch 86)
  ... step 88/123  loss=0.2613  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.242e-01 (batch 88)
  ... step 90/123  loss=0.1375  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.769e-01 (batch 90)
  ... step 92/123  loss=0.2830  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.776e+00 (batch 92)
  ... step 94/123  loss=0.3894  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.954e+00 (batch 94)
  ... step 96/123  loss=1.2935  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.879e+00 (batch 96)
  ... step 98/123  loss=0.3294  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.664e+00 (batch 98)
  ... step 100/123  loss=0.3768  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.364e+00 (batch 100)
  ... step 102/123  loss=0.4412  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.659e+00 (batch 102)
  ... step 104/123  loss=1.0382  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.606e+00 (batch 104)
  ... step 106/123  loss=0.4177  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.805e+00 (batch 106)
  ... step 108/123  loss=0.2829  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.572e+00 (batch 108)
  ... step 110/123  loss=0.6351  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.838e+00 (batch 110)
  ... step 112/123  loss=0.4811  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.926e+00 (batch 112)
  ... step 114/123  loss=0.2671  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.302e+00 (batch 114)
  ... step 116/123  loss=0.3696  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.141e+00 (batch 116)
  ... step 118/123  loss=1.3191  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.535e+00 (batch 118)
  ... step 120/123  loss=0.9696  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.495e+00 (batch 120)
  ... step 122/123  loss=1.2395  (0.3s since last print)
  â†˜ grad L2 norm â‰ˆ 1.485e+01 (batch 122)
âœ… epoch 8 forward/backward done in 36.3s
  ðŸ”Ž val step 0: batch (4, 32, 2, 324, 1, 16) loss=11.5280
[08] train=0.5493  val=11.9844  RMSE(std)=[Qi:3.365, Qe:3.470, Î“:3.547]  RMSE(phys)=[Qi:286.416, Qe:383.443, Î“:183.651]  (2.2s)
âœ… Saved unified NN â†’ ./mnt/data/bin.cgyro.nn
ðŸŸ¦ Starting epoch 9/15 (train steps â‰ˆ 123)
  ... step 0/123  loss=0.7107  (0.1s since last print)
  â†˜ grad L2 norm â‰ˆ 3.820e+00 (batch 0)
  ... step 2/123  loss=0.2798  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.061e+00 (batch 2)
  ... step 4/123  loss=1.4057  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.318e+00 (batch 4)
  ... step 6/123  loss=0.3019  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.260e+00 (batch 6)
  ... step 8/123  loss=0.8587  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.319e+00 (batch 8)
  ... step 10/123  loss=0.3859  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.745e+00 (batch 10)
  ... step 12/123  loss=0.3689  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.679e+00 (batch 12)
  ... step 14/123  loss=0.4767  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.260e+00 (batch 14)
  ... step 16/123  loss=1.1520  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.809e+00 (batch 16)
  ... step 18/123  loss=0.4243  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.339e+00 (batch 18)
  ... step 20/123  loss=0.7678  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.081e+00 (batch 20)
  ... step 22/123  loss=0.8099  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.369e+00 (batch 22)
  ... step 24/123  loss=0.1434  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.614e-01 (batch 24)
  ... step 26/123  loss=0.2198  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.160e+00 (batch 26)
  ... step 28/123  loss=2.5548  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.772e+00 (batch 28)
  ... step 30/123  loss=0.7109  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.728e+00 (batch 30)
  ... step 32/123  loss=1.1719  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.565e+00 (batch 32)
  ... step 34/123  loss=1.0109  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.274e+00 (batch 34)
  ... step 36/123  loss=0.7148  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.662e+00 (batch 36)
  ... step 38/123  loss=0.3390  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.320e+00 (batch 38)
  ... step 40/123  loss=0.7050  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.765e+00 (batch 40)
  ... step 42/123  loss=0.5630  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.529e+00 (batch 42)
  ... step 44/123  loss=0.6219  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.209e+00 (batch 44)
  ... step 46/123  loss=1.2729  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.445e+00 (batch 46)
  ... step 48/123  loss=0.5711  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.197e+00 (batch 48)
  ... step 50/123  loss=0.7067  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.932e+00 (batch 50)
  ... step 52/123  loss=1.0327  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.951e+00 (batch 52)
  ... step 54/123  loss=0.2430  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.061e+00 (batch 54)
  ... step 56/123  loss=0.3779  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.340e+00 (batch 56)
  ... step 58/123  loss=0.5275  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.523e+00 (batch 58)
  ... step 60/123  loss=0.5244  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.767e+00 (batch 60)
  ... step 62/123  loss=1.0643  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.408e+00 (batch 62)
  ... step 64/123  loss=0.5353  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.811e+00 (batch 64)
  ... step 66/123  loss=0.3335  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.555e+00 (batch 66)
  ... step 68/123  loss=0.2595  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.604e+00 (batch 68)
  ... step 70/123  loss=0.1232  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.166e-01 (batch 70)
  ... step 72/123  loss=0.4749  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.102e+00 (batch 72)
  ... step 74/123  loss=0.3094  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.776e+00 (batch 74)
  ... step 76/123  loss=0.3327  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.480e+00 (batch 76)
  ... step 78/123  loss=0.7958  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.581e+00 (batch 78)
  ... step 80/123  loss=0.3048  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.579e+00 (batch 80)
  ... step 82/123  loss=0.2025  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.035e-01 (batch 82)
  ... step 84/123  loss=0.2022  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.420e-01 (batch 84)
  ... step 86/123  loss=0.1634  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.345e-01 (batch 86)
  ... step 88/123  loss=0.3925  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.506e+00 (batch 88)
  ... step 90/123  loss=0.5360  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.090e+00 (batch 90)
  ... step 92/123  loss=0.2276  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.847e+00 (batch 92)
  ... step 94/123  loss=0.2181  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.803e+00 (batch 94)
  ... step 96/123  loss=1.0503  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.260e+00 (batch 96)
  ... step 98/123  loss=0.2639  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.001e+00 (batch 98)
  ... step 100/123  loss=0.4665  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.204e+00 (batch 100)
  ... step 102/123  loss=0.5736  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.609e+00 (batch 102)
  ... step 104/123  loss=0.4310  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.454e+00 (batch 104)
  ... step 106/123  loss=0.4024  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.255e+00 (batch 106)
  ... step 108/123  loss=0.3786  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.461e+00 (batch 108)
  ... step 110/123  loss=0.5406  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.157e+00 (batch 110)
  ... step 112/123  loss=0.3026  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.568e-01 (batch 112)
  ... step 114/123  loss=0.4059  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.263e+00 (batch 114)
  ... step 116/123  loss=0.6937  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.614e+00 (batch 116)
  ... step 118/123  loss=0.8696  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.824e+00 (batch 118)
  ... step 120/123  loss=0.1901  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.343e-01 (batch 120)
  ... step 122/123  loss=8.9703  (0.3s since last print)
  â†˜ grad L2 norm â‰ˆ 3.030e+01 (batch 122)
âœ… epoch 9 forward/backward done in 36.3s
  ðŸ”Ž val step 0: batch (4, 32, 2, 324, 1, 16) loss=13.4159
[09] train=0.5766  val=14.0374  RMSE(std)=[Qi:3.797, Qe:3.737, Î“:3.706]  RMSE(phys)=[Qi:323.118, Qe:412.858, Î“:191.870]  (2.2s)
âœ… Saved unified NN â†’ ./mnt/data/bin.cgyro.nn
ðŸŸ¦ Starting epoch 10/15 (train steps â‰ˆ 123)
  ... step 0/123  loss=0.3386  (0.1s since last print)
  â†˜ grad L2 norm â‰ˆ 1.925e+00 (batch 0)
  ... step 2/123  loss=0.9211  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.948e+00 (batch 2)
  ... step 4/123  loss=1.0614  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.211e+00 (batch 4)
  ... step 6/123  loss=0.2196  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.236e-01 (batch 6)
  ... step 8/123  loss=1.0052  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.325e+00 (batch 8)
  ... step 10/123  loss=0.2636  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.259e-01 (batch 10)
  ... step 12/123  loss=0.7104  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.237e+00 (batch 12)
  ... step 14/123  loss=0.6636  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.694e+00 (batch 14)
  ... step 16/123  loss=0.8456  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.913e+00 (batch 16)
  ... step 18/123  loss=1.9547  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.687e+00 (batch 18)
  ... step 20/123  loss=1.5161  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.233e+00 (batch 20)
  ... step 22/123  loss=0.2912  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.558e-01 (batch 22)
  ... step 24/123  loss=0.2985  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.070e+00 (batch 24)
  ... step 26/123  loss=0.5346  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.754e+00 (batch 26)
  ... step 28/123  loss=0.5932  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.707e+00 (batch 28)
  ... step 30/123  loss=0.3453  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.072e+00 (batch 30)
  ... step 32/123  loss=1.2389  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.770e+00 (batch 32)
  ... step 34/123  loss=0.6144  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.466e+00 (batch 34)
  ... step 36/123  loss=1.0574  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.221e+00 (batch 36)
  ... step 38/123  loss=0.4403  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.298e+00 (batch 38)
  ... step 40/123  loss=0.3980  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.328e+00 (batch 40)
  ... step 42/123  loss=0.8376  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.816e+00 (batch 42)
  ... step 44/123  loss=0.9084  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.403e+00 (batch 44)
  ... step 46/123  loss=0.3311  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.411e+00 (batch 46)
  ... step 48/123  loss=0.3574  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.624e+00 (batch 48)
  ... step 50/123  loss=0.7036  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.426e+00 (batch 50)
  ... step 52/123  loss=0.7865  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.922e+00 (batch 52)
  ... step 54/123  loss=0.1685  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.175e+00 (batch 54)
  ... step 56/123  loss=0.7623  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.983e+00 (batch 56)
  ... step 58/123  loss=0.2165  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.261e+00 (batch 58)
  ... step 60/123  loss=0.4583  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.707e+00 (batch 60)
  ... step 62/123  loss=0.2353  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.389e+00 (batch 62)
  ... step 64/123  loss=0.1909  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.897e+00 (batch 64)
  ... step 66/123  loss=0.1991  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.512e+00 (batch 66)
  ... step 68/123  loss=0.2185  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.294e+00 (batch 68)
  ... step 70/123  loss=0.3958  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.970e+00 (batch 70)
  ... step 72/123  loss=0.3725  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.619e+00 (batch 72)
  ... step 74/123  loss=0.4922  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.463e+00 (batch 74)
  ... step 76/123  loss=0.7864  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.925e+00 (batch 76)
  ... step 78/123  loss=0.5864  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.441e+00 (batch 78)
  ... step 80/123  loss=0.4615  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.711e+00 (batch 80)
  ... step 82/123  loss=0.6170  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.791e+00 (batch 82)
  ... step 84/123  loss=0.1545  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.904e-01 (batch 84)
  ... step 86/123  loss=0.2412  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.001e+00 (batch 86)
  ... step 88/123  loss=0.6879  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.521e+00 (batch 88)
  ... step 90/123  loss=0.6811  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.909e+00 (batch 90)
  ... step 92/123  loss=0.5234  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.397e+00 (batch 92)
  ... step 94/123  loss=0.2576  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.984e+00 (batch 94)
  ... step 96/123  loss=0.1334  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.135e+00 (batch 96)
  ... step 98/123  loss=0.2702  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.069e+00 (batch 98)
  ... step 100/123  loss=2.0187  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.592e+00 (batch 100)
  ... step 102/123  loss=0.8273  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.203e+00 (batch 102)
  ... step 104/123  loss=0.5891  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.430e+00 (batch 104)
  ... step 106/123  loss=0.6028  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.872e+00 (batch 106)
  ... step 108/123  loss=0.4514  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.763e+00 (batch 108)
  ... step 110/123  loss=1.3561  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.949e+00 (batch 110)
  ... step 112/123  loss=0.4417  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.730e+00 (batch 112)
  ... step 114/123  loss=0.3802  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.481e+00 (batch 114)
  ... step 116/123  loss=0.2092  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.568e-01 (batch 116)
  ... step 118/123  loss=0.3646  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.242e+00 (batch 118)
  ... step 120/123  loss=0.7869  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.430e+00 (batch 120)
  ... step 122/123  loss=0.0677  (0.3s since last print)
  â†˜ grad L2 norm â‰ˆ 8.291e-01 (batch 122)
âœ… epoch 10 forward/backward done in 36.3s
  ðŸ”Ž val step 0: batch (4, 32, 2, 324, 1, 16) loss=0.7294
[10] train=0.5666  val=0.9880  RMSE(std)=[Qi:0.995, Qe:0.980, Î“:1.006]  RMSE(phys)=[Qi:84.699, Qe:108.326, Î“:52.089]  (2.2s)
âœ… Saved unified NN â†’ ./mnt/data/bin.cgyro.nn
ðŸŸ¦ Starting epoch 11/15 (train steps â‰ˆ 123)
  ... step 0/123  loss=0.5317  (0.1s since last print)
  â†˜ grad L2 norm â‰ˆ 2.229e+00 (batch 0)
  ... step 2/123  loss=0.6576  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.796e+00 (batch 2)
  ... step 4/123  loss=0.3575  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.681e+00 (batch 4)
  ... step 6/123  loss=1.1407  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.531e+00 (batch 6)
  ... step 8/123  loss=0.4125  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.449e+00 (batch 8)
  ... step 10/123  loss=0.6448  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.352e+00 (batch 10)
  ... step 12/123  loss=0.7226  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.018e+00 (batch 12)
  ... step 14/123  loss=0.1033  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.484e-01 (batch 14)
  ... step 16/123  loss=0.3244  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.473e+00 (batch 16)
  ... step 18/123  loss=0.6668  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.130e+00 (batch 18)
  ... step 20/123  loss=0.2199  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.099e+00 (batch 20)
  ... step 22/123  loss=0.5565  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.117e+00 (batch 22)
  ... step 24/123  loss=0.4432  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.474e+00 (batch 24)
  ... step 26/123  loss=0.6677  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.266e+00 (batch 26)
  ... step 28/123  loss=0.6554  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.607e+00 (batch 28)
  ... step 30/123  loss=0.5304  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.874e+00 (batch 30)
  ... step 32/123  loss=0.1714  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.430e+00 (batch 32)
  ... step 34/123  loss=0.3586  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.632e-01 (batch 34)
  ... step 36/123  loss=1.3651  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.751e+00 (batch 36)
  ... step 38/123  loss=0.3550  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.491e+00 (batch 38)
  ... step 40/123  loss=0.2403  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.843e+00 (batch 40)
  ... step 42/123  loss=0.5275  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.383e+00 (batch 42)
  ... step 44/123  loss=0.1092  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.927e-01 (batch 44)
  ... step 46/123  loss=0.4557  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.513e+00 (batch 46)
  ... step 48/123  loss=0.4831  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.705e+00 (batch 48)
  ... step 50/123  loss=0.2201  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.119e+00 (batch 50)
  ... step 52/123  loss=0.4595  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.353e+00 (batch 52)
  ... step 54/123  loss=0.2016  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.240e+00 (batch 54)
  ... step 56/123  loss=0.4472  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.849e+00 (batch 56)
  ... step 58/123  loss=0.3661  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.610e+00 (batch 58)
  ... step 60/123  loss=0.2603  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.626e+00 (batch 60)
  ... step 62/123  loss=0.3928  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.725e+00 (batch 62)
  ... step 64/123  loss=0.4022  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.379e+00 (batch 64)
  ... step 66/123  loss=0.2726  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.093e+00 (batch 66)
  ... step 68/123  loss=0.6678  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.479e+00 (batch 68)
  ... step 70/123  loss=0.2869  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.289e+00 (batch 70)
  ... step 72/123  loss=0.7201  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.548e+00 (batch 72)
  ... step 74/123  loss=0.4170  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.300e+00 (batch 74)
  ... step 76/123  loss=0.5082  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.431e+00 (batch 76)
  ... step 78/123  loss=0.8942  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.981e+00 (batch 78)
  ... step 80/123  loss=0.5822  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.082e+00 (batch 80)
  ... step 82/123  loss=0.9259  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.965e+00 (batch 82)
  ... step 84/123  loss=0.5005  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.887e+00 (batch 84)
  ... step 86/123  loss=0.2312  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.143e+00 (batch 86)
  ... step 88/123  loss=0.5640  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.059e+00 (batch 88)
  ... step 90/123  loss=0.8662  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.393e+00 (batch 90)
  ... step 92/123  loss=0.2893  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.712e+00 (batch 92)
  ... step 94/123  loss=0.5240  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.118e+00 (batch 94)
  ... step 96/123  loss=0.4141  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.433e+00 (batch 96)
  ... step 98/123  loss=0.2531  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.431e+00 (batch 98)
  ... step 100/123  loss=0.6432  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.061e+00 (batch 100)
  ... step 102/123  loss=0.4671  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.887e+00 (batch 102)
  ... step 104/123  loss=0.5725  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.971e+00 (batch 104)
  ... step 106/123  loss=0.2967  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.515e+00 (batch 106)
  ... step 108/123  loss=0.8997  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.314e+00 (batch 108)
  ... step 110/123  loss=0.6257  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.411e+00 (batch 110)
  ... step 112/123  loss=0.0776  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.460e-01 (batch 112)
  ... step 114/123  loss=0.9227  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.659e+00 (batch 114)
  ... step 116/123  loss=0.4619  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.411e+00 (batch 116)
  ... step 118/123  loss=0.5850  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.942e+00 (batch 118)
  ... step 120/123  loss=0.3172  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.095e+00 (batch 120)
  ... step 122/123  loss=0.7185  (0.3s since last print)
  â†˜ grad L2 norm â‰ˆ 5.533e+00 (batch 122)
âœ… epoch 11 forward/backward done in 36.3s
  ðŸ”Ž val step 0: batch (4, 32, 2, 324, 1, 16) loss=28.4308
[11] train=0.5058  val=29.0337  RMSE(std)=[Qi:5.445, Qe:5.446, Î“:5.272]  RMSE(phys)=[Qi:463.421, Qe:601.695, Î“:272.942]  (2.2s)
âœ… Saved unified NN â†’ ./mnt/data/bin.cgyro.nn
ðŸŸ¦ Starting epoch 12/15 (train steps â‰ˆ 123)
  ... step 0/123  loss=0.2717  (0.1s since last print)
  â†˜ grad L2 norm â‰ˆ 1.688e+00 (batch 0)
  ... step 2/123  loss=0.4723  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.751e+00 (batch 2)
  ... step 4/123  loss=0.5388  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.054e+00 (batch 4)
  ... step 6/123  loss=0.5642  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.158e+00 (batch 6)
  ... step 8/123  loss=0.2643  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.340e+00 (batch 8)
  ... step 10/123  loss=0.7659  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.839e+00 (batch 10)
  ... step 12/123  loss=0.3031  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.731e+00 (batch 12)
  ... step 14/123  loss=0.3924  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.928e+00 (batch 14)
  ... step 16/123  loss=0.2555  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.846e+00 (batch 16)
  ... step 18/123  loss=0.3965  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.949e+00 (batch 18)
  ... step 20/123  loss=0.3265  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.416e+00 (batch 20)
  ... step 22/123  loss=0.3445  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.539e+00 (batch 22)
  ... step 24/123  loss=0.2688  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.180e+00 (batch 24)
  ... step 26/123  loss=0.3179  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.528e+00 (batch 26)
  ... step 28/123  loss=0.4039  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.473e-01 (batch 28)
  ... step 30/123  loss=0.0776  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.221e-01 (batch 30)
  ... step 32/123  loss=1.5913  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.149e+00 (batch 32)
  ... step 34/123  loss=0.4096  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.062e+00 (batch 34)
  ... step 36/123  loss=0.7983  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.908e+00 (batch 36)
  ... step 38/123  loss=0.2371  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.585e-01 (batch 38)
  ... step 40/123  loss=0.4524  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.012e+00 (batch 40)
  ... step 42/123  loss=0.2370  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.168e-01 (batch 42)
  ... step 44/123  loss=0.2511  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.428e+00 (batch 44)
  ... step 46/123  loss=0.8713  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.035e+00 (batch 46)
  ... step 48/123  loss=0.9967  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.727e+00 (batch 48)
  ... step 50/123  loss=0.6141  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.966e+00 (batch 50)
  ... step 52/123  loss=0.4810  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.389e+00 (batch 52)
  ... step 54/123  loss=0.4380  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.709e+00 (batch 54)
  ... step 56/123  loss=1.5181  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.153e+00 (batch 56)
  ... step 58/123  loss=0.2031  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.612e-01 (batch 58)
  ... step 60/123  loss=0.1702  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.753e-01 (batch 60)
  ... step 62/123  loss=0.7905  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.199e+00 (batch 62)
  ... step 64/123  loss=0.3221  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.409e+00 (batch 64)
  ... step 66/123  loss=0.4892  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.486e+00 (batch 66)
  ... step 68/123  loss=0.6014  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.527e+00 (batch 68)
  ... step 70/123  loss=0.4306  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.653e+00 (batch 70)
  ... step 72/123  loss=0.1357  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.593e-01 (batch 72)
  ... step 74/123  loss=2.0804  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.502e+00 (batch 74)
  ... step 76/123  loss=0.3790  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.527e+00 (batch 76)
  ... step 78/123  loss=0.8012  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.754e+00 (batch 78)
  ... step 80/123  loss=0.6696  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.345e+00 (batch 80)
  ... step 82/123  loss=0.3832  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.480e+00 (batch 82)
  ... step 84/123  loss=0.4981  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.713e+00 (batch 84)
  ... step 86/123  loss=0.5229  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.641e+00 (batch 86)
  ... step 88/123  loss=0.2340  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.742e+00 (batch 88)
  ... step 90/123  loss=0.4798  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.231e+00 (batch 90)
  ... step 92/123  loss=0.4510  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.391e+00 (batch 92)
  ... step 94/123  loss=0.3162  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.002e+00 (batch 94)
  ... step 96/123  loss=0.6941  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.938e+00 (batch 96)
  ... step 98/123  loss=0.5074  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.514e+00 (batch 98)
  ... step 100/123  loss=0.2440  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.004e+00 (batch 100)
  ... step 102/123  loss=0.6058  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.612e+00 (batch 102)
  ... step 104/123  loss=0.1815  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.299e-01 (batch 104)
  ... step 106/123  loss=0.2288  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.259e+00 (batch 106)
  ... step 108/123  loss=0.2920  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.489e+00 (batch 108)
  ... step 110/123  loss=0.6598  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.026e+00 (batch 110)
  ... step 112/123  loss=0.4314  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.636e+00 (batch 112)
  ... step 114/123  loss=1.0278  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.291e+00 (batch 114)
  ... step 116/123  loss=0.3073  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.506e+00 (batch 116)
  ... step 118/123  loss=0.3852  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.558e+00 (batch 118)
  ... step 120/123  loss=0.2759  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.813e+00 (batch 120)
  ... step 122/123  loss=0.0080  (0.3s since last print)
  â†˜ grad L2 norm â‰ˆ 4.343e-01 (batch 122)
âœ… epoch 12 forward/backward done in 36.3s
  ðŸ”Ž val step 0: batch (4, 32, 2, 324, 1, 16) loss=14.4594
[12] train=0.5122  val=14.9856  RMSE(std)=[Qi:3.819, Qe:3.851, Î“:3.942]  RMSE(phys)=[Qi:324.999, Qe:425.548, Î“:204.087]  (2.2s)
âœ… Saved unified NN â†’ ./mnt/data/bin.cgyro.nn
ðŸŸ¦ Starting epoch 13/15 (train steps â‰ˆ 123)
  ... step 0/123  loss=0.1260  (0.1s since last print)
  â†˜ grad L2 norm â‰ˆ 6.790e-01 (batch 0)
  ... step 2/123  loss=0.4601  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.981e+00 (batch 2)
  ... step 4/123  loss=0.2877  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.228e+00 (batch 4)
  ... step 6/123  loss=0.1045  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.020e-01 (batch 6)
  ... step 8/123  loss=1.7665  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.066e+00 (batch 8)
  ... step 10/123  loss=0.9491  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.044e+00 (batch 10)
  ... step 12/123  loss=0.3326  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.485e+00 (batch 12)
  ... step 14/123  loss=0.3843  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.143e+00 (batch 14)
  ... step 16/123  loss=1.0718  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.045e+00 (batch 16)
  ... step 18/123  loss=0.2410  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.589e+00 (batch 18)
  ... step 20/123  loss=0.4590  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.686e+00 (batch 20)
  ... step 22/123  loss=0.2592  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.375e+00 (batch 22)
  ... step 24/123  loss=1.4127  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.376e+00 (batch 24)
  ... step 26/123  loss=0.3560  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.346e+00 (batch 26)
  ... step 28/123  loss=0.8682  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.046e+00 (batch 28)
  ... step 30/123  loss=1.0410  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.777e+00 (batch 30)
  ... step 32/123  loss=0.2213  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.220e+00 (batch 32)
  ... step 34/123  loss=0.4804  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.701e+00 (batch 34)
  ... step 36/123  loss=0.2876  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.526e+00 (batch 36)
  ... step 38/123  loss=0.2897  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.720e+00 (batch 38)
  ... step 40/123  loss=0.2725  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.505e+00 (batch 40)
  ... step 42/123  loss=0.2107  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.174e+00 (batch 42)
  ... step 44/123  loss=1.5440  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.958e+00 (batch 44)
  ... step 46/123  loss=0.5312  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.318e+00 (batch 46)
  ... step 48/123  loss=0.3101  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.710e+00 (batch 48)
  ... step 50/123  loss=0.4771  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.558e+00 (batch 50)
  ... step 52/123  loss=0.5493  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.840e+00 (batch 52)
  ... step 54/123  loss=0.3514  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.177e+00 (batch 54)
  ... step 56/123  loss=0.4512  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.439e+00 (batch 56)
  ... step 58/123  loss=0.4082  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.399e+00 (batch 58)
  ... step 60/123  loss=0.4597  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.926e+00 (batch 60)
  ... step 62/123  loss=0.2668  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.408e+00 (batch 62)
  ... step 64/123  loss=0.6576  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.101e+00 (batch 64)
  ... step 66/123  loss=0.6326  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.985e+00 (batch 66)
  ... step 68/123  loss=0.6308  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.961e+00 (batch 68)
  ... step 70/123  loss=0.3313  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.561e+00 (batch 70)
  ... step 72/123  loss=0.5773  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.179e+00 (batch 72)
  ... step 74/123  loss=0.4547  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.534e+00 (batch 74)
  ... step 76/123  loss=0.0947  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.112e+00 (batch 76)
  ... step 78/123  loss=0.2915  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.700e+00 (batch 78)
  ... step 80/123  loss=0.4068  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.313e+00 (batch 80)
  ... step 82/123  loss=0.5988  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.100e+00 (batch 82)
  ... step 84/123  loss=0.2903  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.229e+00 (batch 84)
  ... step 86/123  loss=0.9782  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.735e+00 (batch 86)
  ... step 88/123  loss=0.6918  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.981e+00 (batch 88)
  ... step 90/123  loss=0.2890  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.192e+00 (batch 90)
  ... step 92/123  loss=0.1585  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.797e-01 (batch 92)
  ... step 94/123  loss=0.3477  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.006e+00 (batch 94)
  ... step 96/123  loss=0.5558  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.703e+00 (batch 96)
  ... step 98/123  loss=0.2547  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.260e+00 (batch 98)
  ... step 100/123  loss=0.2602  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.014e+00 (batch 100)
  ... step 102/123  loss=0.2975  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.402e+00 (batch 102)
  ... step 104/123  loss=0.5417  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.125e+00 (batch 104)
  ... step 106/123  loss=0.2611  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.083e+00 (batch 106)
  ... step 108/123  loss=0.3931  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.261e+00 (batch 108)
  ... step 110/123  loss=0.1893  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.722e-01 (batch 110)
  ... step 112/123  loss=0.4563  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.690e+00 (batch 112)
  ... step 114/123  loss=1.1821  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.695e+00 (batch 114)
  ... step 116/123  loss=0.4800  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.945e+00 (batch 116)
  ... step 118/123  loss=0.5928  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.736e+00 (batch 118)
  ... step 120/123  loss=0.3478  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.796e+00 (batch 120)
  ... step 122/123  loss=5.5352  (0.3s since last print)
  â†˜ grad L2 norm â‰ˆ 1.904e+01 (batch 122)
âœ… epoch 13 forward/backward done in 36.3s
  ðŸ”Ž val step 0: batch (4, 32, 2, 324, 1, 16) loss=11.7787
[13] train=0.4990  val=12.2231  RMSE(std)=[Qi:3.443, Qe:3.518, Î“:3.527]  RMSE(phys)=[Qi:293.057, Qe:388.677, Î“:182.579]  (2.2s)
âœ… Saved unified NN â†’ ./mnt/data/bin.cgyro.nn
ðŸŸ¦ Starting epoch 14/15 (train steps â‰ˆ 123)
  ... step 0/123  loss=0.6165  (0.1s since last print)
  â†˜ grad L2 norm â‰ˆ 1.899e+00 (batch 0)
  ... step 2/123  loss=0.4550  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.199e+00 (batch 2)
  ... step 4/123  loss=0.6278  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.336e+00 (batch 4)
  ... step 6/123  loss=0.4714  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.300e+00 (batch 6)
  ... step 8/123  loss=0.7322  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.644e+00 (batch 8)
  ... step 10/123  loss=0.5174  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.709e+00 (batch 10)
  ... step 12/123  loss=0.2506  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.194e+00 (batch 12)
  ... step 14/123  loss=0.2200  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.946e-01 (batch 14)
  ... step 16/123  loss=0.3840  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.766e-01 (batch 16)
  ... step 18/123  loss=0.6536  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.181e+00 (batch 18)
  ... step 20/123  loss=1.3673  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.969e+00 (batch 20)
  ... step 22/123  loss=0.4482  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.106e+00 (batch 22)
  ... step 24/123  loss=0.1795  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.077e+00 (batch 24)
  ... step 26/123  loss=0.2035  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.199e-01 (batch 26)
  ... step 28/123  loss=0.3478  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.307e+00 (batch 28)
  ... step 30/123  loss=0.4009  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.099e+00 (batch 30)
  ... step 32/123  loss=1.7901  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.480e+00 (batch 32)
  ... step 34/123  loss=0.3621  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.084e+00 (batch 34)
  ... step 36/123  loss=0.1012  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.770e-01 (batch 36)
  ... step 38/123  loss=0.5298  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.077e+00 (batch 38)
  ... step 40/123  loss=1.0943  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.495e+00 (batch 40)
  ... step 42/123  loss=0.3221  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.581e+00 (batch 42)
  ... step 44/123  loss=0.3274  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.523e+00 (batch 44)
  ... step 46/123  loss=0.3555  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.667e-01 (batch 46)
  ... step 48/123  loss=0.3975  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.604e+00 (batch 48)
  ... step 50/123  loss=0.2520  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.973e-01 (batch 50)
  ... step 52/123  loss=0.9325  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.263e+00 (batch 52)
  ... step 54/123  loss=0.3498  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.377e+00 (batch 54)
  ... step 56/123  loss=0.9963  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.904e+00 (batch 56)
  ... step 58/123  loss=0.6879  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.944e+00 (batch 58)
  ... step 60/123  loss=0.3721  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.773e+00 (batch 60)
  ... step 62/123  loss=0.2811  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.043e+00 (batch 62)
  ... step 64/123  loss=1.0063  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.694e+00 (batch 64)
  ... step 66/123  loss=0.8668  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.854e+00 (batch 66)
  ... step 68/123  loss=0.6103  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.527e+00 (batch 68)
  ... step 70/123  loss=0.2187  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.053e+00 (batch 70)
  ... step 72/123  loss=0.2647  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.083e+00 (batch 72)
  ... step 74/123  loss=0.1949  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.991e-01 (batch 74)
  ... step 76/123  loss=0.5204  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.169e+00 (batch 76)
  ... step 78/123  loss=0.5364  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.375e+00 (batch 78)
  ... step 80/123  loss=0.6741  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.735e+00 (batch 80)
  ... step 82/123  loss=0.9042  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.073e+00 (batch 82)
  ... step 84/123  loss=0.2407  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.329e+00 (batch 84)
  ... step 86/123  loss=0.5625  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.493e+00 (batch 86)
  ... step 88/123  loss=0.4006  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.404e+00 (batch 88)
  ... step 90/123  loss=0.4141  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.611e+00 (batch 90)
  ... step 92/123  loss=0.2621  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.190e+00 (batch 92)
  ... step 94/123  loss=0.3663  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.679e+00 (batch 94)
  ... step 96/123  loss=0.2865  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.743e+00 (batch 96)
  ... step 98/123  loss=1.0179  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.434e+00 (batch 98)
  ... step 100/123  loss=0.3114  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.500e+00 (batch 100)
  ... step 102/123  loss=0.2739  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.766e-01 (batch 102)
  ... step 104/123  loss=0.4024  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.023e+00 (batch 104)
  ... step 106/123  loss=0.2982  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.963e+00 (batch 106)
  ... step 108/123  loss=0.5158  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.173e+00 (batch 108)
  ... step 110/123  loss=0.7365  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.825e+00 (batch 110)
  ... step 112/123  loss=0.5819  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.302e+00 (batch 112)
  ... step 114/123  loss=0.6760  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.088e+00 (batch 114)
  ... step 116/123  loss=0.2558  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.136e+00 (batch 116)
  ... step 118/123  loss=0.5452  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.529e+00 (batch 118)
  ... step 120/123  loss=1.2949  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.943e+00 (batch 120)
  ... step 122/123  loss=0.0456  (0.3s since last print)
  â†˜ grad L2 norm â‰ˆ 1.039e+00 (batch 122)
âœ… epoch 14 forward/backward done in 36.3s
  ðŸ”Ž val step 0: batch (4, 32, 2, 324, 1, 16) loss=10.9148
[14] train=0.4991  val=11.3734  RMSE(std)=[Qi:3.348, Qe:3.326, Î“:3.442]  RMSE(phys)=[Qi:284.925, Qe:367.514, Î“:178.202]  (2.2s)
âœ… Saved unified NN â†’ ./mnt/data/bin.cgyro.nn
ðŸŸ¦ Starting epoch 15/15 (train steps â‰ˆ 123)
  ... step 0/123  loss=0.3094  (0.1s since last print)
  â†˜ grad L2 norm â‰ˆ 1.597e+00 (batch 0)
  ... step 2/123  loss=0.8219  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.127e+00 (batch 2)
  ... step 4/123  loss=0.7870  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.051e+00 (batch 4)
  ... step 6/123  loss=0.7297  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.428e+00 (batch 6)
  ... step 8/123  loss=0.5632  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.871e+00 (batch 8)
  ... step 10/123  loss=0.2162  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.002e+00 (batch 10)
  ... step 12/123  loss=0.4164  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.223e+00 (batch 12)
  ... step 14/123  loss=0.4612  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.927e+00 (batch 14)
  ... step 16/123  loss=0.1538  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.828e-01 (batch 16)
  ... step 18/123  loss=0.3217  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.528e+00 (batch 18)
  ... step 20/123  loss=0.2734  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.089e+00 (batch 20)
  ... step 22/123  loss=0.5849  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.734e+00 (batch 22)
  ... step 24/123  loss=0.7857  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.168e+00 (batch 24)
  ... step 26/123  loss=0.4196  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.593e+00 (batch 26)
  ... step 28/123  loss=0.7664  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.467e+00 (batch 28)
  ... step 30/123  loss=0.3651  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.002e+00 (batch 30)
  ... step 32/123  loss=0.3180  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.202e+00 (batch 32)
  ... step 34/123  loss=0.1592  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.339e-01 (batch 34)
  ... step 36/123  loss=0.3621  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.086e+00 (batch 36)
  ... step 38/123  loss=0.3369  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.170e+00 (batch 38)
  ... step 40/123  loss=0.5120  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.037e+00 (batch 40)
  ... step 42/123  loss=0.6199  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.403e+00 (batch 42)
  ... step 44/123  loss=0.5392  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.301e+00 (batch 44)
  ... step 46/123  loss=0.0912  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.991e-01 (batch 46)
  ... step 48/123  loss=1.2799  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.098e+00 (batch 48)
  ... step 50/123  loss=0.2217  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.686e-01 (batch 50)
  ... step 52/123  loss=0.5273  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.745e+00 (batch 52)
  ... step 54/123  loss=0.4503  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.363e+00 (batch 54)
  ... step 56/123  loss=0.9020  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.706e+00 (batch 56)
  ... step 58/123  loss=0.5126  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.319e+00 (batch 58)
  ... step 60/123  loss=0.2540  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.611e+00 (batch 60)
  ... step 62/123  loss=0.2905  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.542e+00 (batch 62)
  ... step 64/123  loss=0.5292  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.206e+00 (batch 64)
  ... step 66/123  loss=1.2993  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.014e+00 (batch 66)
  ... step 68/123  loss=0.3395  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.145e+00 (batch 68)
  ... step 70/123  loss=0.3471  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.741e+00 (batch 70)
  ... step 72/123  loss=0.3220  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.422e+00 (batch 72)
  ... step 74/123  loss=0.4136  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.741e+00 (batch 74)
  ... step 76/123  loss=0.4896  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.613e+00 (batch 76)
  ... step 78/123  loss=0.4668  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.893e+00 (batch 78)
  ... step 80/123  loss=0.4862  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.270e+00 (batch 80)
  ... step 82/123  loss=0.1903  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.037e-01 (batch 82)
  ... step 84/123  loss=1.4413  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.294e+00 (batch 84)
  ... step 86/123  loss=0.2056  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.789e-01 (batch 86)
  ... step 88/123  loss=0.2858  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.571e+00 (batch 88)
  ... step 90/123  loss=0.2263  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.759e+00 (batch 90)
  ... step 92/123  loss=0.1393  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.034e+00 (batch 92)
  ... step 94/123  loss=0.4591  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.099e+00 (batch 94)
  ... step 96/123  loss=0.4902  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.713e+00 (batch 96)
  ... step 98/123  loss=0.1382  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.682e-01 (batch 98)
  ... step 100/123  loss=0.6496  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.610e+00 (batch 100)
  ... step 102/123  loss=0.8302  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.934e+00 (batch 102)
  ... step 104/123  loss=0.3866  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.789e+00 (batch 104)
  ... step 106/123  loss=0.4950  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.050e+00 (batch 106)
  ... step 108/123  loss=0.9713  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.336e+00 (batch 108)
  ... step 110/123  loss=0.4185  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.922e+00 (batch 110)
  ... step 112/123  loss=0.4898  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.684e+00 (batch 112)
  ... step 114/123  loss=0.8440  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.217e+00 (batch 114)
  ... step 116/123  loss=0.5091  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.366e+00 (batch 116)
  ... step 118/123  loss=0.8215  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.070e+00 (batch 118)
  ... step 120/123  loss=1.0174  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.110e+00 (batch 120)
  ... step 122/123  loss=0.0656  (0.3s since last print)
  â†˜ grad L2 norm â‰ˆ 1.219e+00 (batch 122)
âœ… epoch 15 forward/backward done in 36.3s
  ðŸ”Ž val step 0: batch (4, 32, 2, 324, 1, 16) loss=12.2238
[15] train=0.4909  val=12.4706  RMSE(std)=[Qi:3.688, Qe:3.412, Î“:3.488]  RMSE(phys)=[Qi:313.904, Qe:376.989, Î“:180.575]  (2.2s)
âœ… Saved unified NN â†’ ./mnt/data/bin.cgyro.nn
Saved metrics â†’ ./mnt/data/myrun_logs_deep_debug/metrics.csv
âœ… Final unified NN â†’ ./mnt/data/bin.cgyro.nn
âœ… Training complete. Artifacts in: ./mnt/data/myrun_logs_deep_debug
