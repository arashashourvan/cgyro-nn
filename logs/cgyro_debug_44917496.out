[build_datasets] windows: total=615  train=369  val=123  test=123

[train] dataset:
  y_mean: [[[149.46204 223.82538  88.89333]]]
  y_std : [[[72.62504 92.54556 44.15767]]]
  sample Y: min=-1.324e+00, max=2.054e+00, mean=-2.283e-01, std=7.447e-01

[val] dataset:
  y_mean: [[[149.46204 223.82538  88.89333]]]
  y_std : [[[72.62504 92.54556 44.15767]]]
  sample Y: min=-1.976e+00, max=2.718e+00, mean=-3.365e-01, std=1.145e+00

[test] dataset:
  y_mean: [[[149.46204 223.82538  88.89333]]]
  y_std : [[[72.62504 92.54556 44.15767]]]
  sample Y: min=-1.871e+00, max=3.058e+00, mean=-1.987e-02, std=1.562e+00
âœ… Saved flux histograms in ./mnt/data/myrun_logs_ot2
ðŸŸ¦ Starting epoch 1/30 (train steps â‰ˆ 93)
[2199029] Î¦2FluxDeep forward: input (4, 32, 2, 324, 1, 16)
  ... step 0/93  loss=2.1085  (2.0s since last print)
  â†˜ grad L2 norm â‰ˆ 2.413e+01 (batch 0)
  ... step 2/93  loss=0.5596  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.176e+01 (batch 2)
  ... step 4/93  loss=2.2301  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.314e+01 (batch 4)
  ... step 6/93  loss=1.1089  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.692e+01 (batch 6)
  ... step 8/93  loss=0.5199  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.073e+01 (batch 8)
  ... step 10/93  loss=0.7806  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.502e+01 (batch 10)
  ... step 12/93  loss=1.9145  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.490e+01 (batch 12)
  ... step 14/93  loss=1.6458  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.965e+01 (batch 14)
  ... step 16/93  loss=1.7094  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.805e+01 (batch 16)
  ... step 18/93  loss=0.4399  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.084e+01 (batch 18)
  ... step 20/93  loss=1.0821  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.580e+01 (batch 20)
  ... step 22/93  loss=1.3697  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.856e+01 (batch 22)
  ... step 24/93  loss=1.3397  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.899e+01 (batch 24)
  ... step 26/93  loss=2.7861  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.518e+01 (batch 26)
  ... step 28/93  loss=0.9257  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.342e+01 (batch 28)
  ... step 30/93  loss=0.9515  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.372e+01 (batch 30)
  ... step 32/93  loss=2.0040  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.575e+01 (batch 32)
  ... step 34/93  loss=0.9044  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.329e+01 (batch 34)
  ... step 36/93  loss=1.1958  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.346e+01 (batch 36)
  ... step 38/93  loss=0.8249  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.362e+01 (batch 38)
  ... step 40/93  loss=0.6837  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.302e+01 (batch 40)
  ... step 42/93  loss=0.7104  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.309e+00 (batch 42)
  ... step 44/93  loss=0.9081  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.422e+01 (batch 44)
  ... step 46/93  loss=0.4993  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.073e+01 (batch 46)
  ... step 48/93  loss=0.8497  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.101e+01 (batch 48)
  ... step 50/93  loss=0.6676  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.085e+01 (batch 50)
  ... step 52/93  loss=1.6109  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.969e+01 (batch 52)
  ... step 54/93  loss=1.2497  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.412e+01 (batch 54)
  ... step 56/93  loss=0.6363  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.033e+01 (batch 56)
  ... step 58/93  loss=1.0329  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.628e+01 (batch 58)
  ... step 60/93  loss=1.8435  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.907e+01 (batch 60)
  ... step 62/93  loss=0.4114  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.290e+00 (batch 62)
  ... step 64/93  loss=2.1090  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.978e+01 (batch 64)
  ... step 66/93  loss=0.7743  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.200e+01 (batch 66)
  ... step 68/93  loss=0.6189  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.320e+00 (batch 68)
  ... step 70/93  loss=0.4272  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.542e+00 (batch 70)
  ... step 72/93  loss=0.9863  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.055e+01 (batch 72)
  ... step 74/93  loss=0.8594  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.203e+01 (batch 74)
  ... step 76/93  loss=0.5561  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.760e+00 (batch 76)
  ... step 78/93  loss=1.0581  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.379e+01 (batch 78)
  ... step 80/93  loss=0.4897  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.938e+00 (batch 80)
  ... step 82/93  loss=0.7483  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.087e+01 (batch 82)
  ... step 84/93  loss=0.8272  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.079e+01 (batch 84)
  ... step 86/93  loss=1.3053  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.277e+01 (batch 86)
  ... step 88/93  loss=2.1488  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.043e+01 (batch 88)
  ... step 90/93  loss=1.2142  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.217e+01 (batch 90)
  ... step 92/93  loss=0.5531  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.886e+01 (batch 92)
âœ… epoch 1 forward/backward done in 28.2s
  ðŸ”Ž val step 0: batch (4, 32, 2, 324, 1, 16) loss=1.1761
[01] train=1.0260  val=1.3985  RMSE(std)=[Qi:1.161, Qe:1.224, Î“:1.162]  RMSE(phys)=[Qi:84.345, Qe:113.246, Î“:51.293]  (1.6s)
âœ… Saved unified NN â†’ ./mnt/data/bin.cgyro.nn
ðŸŸ¦ Starting epoch 2/30 (train steps â‰ˆ 93)
  ... step 0/93  loss=0.6729  (0.1s since last print)
  â†˜ grad L2 norm â‰ˆ 1.015e+01 (batch 0)
  ... step 2/93  loss=1.2790  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.468e+01 (batch 2)
  ... step 4/93  loss=1.0652  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.330e+01 (batch 4)
  ... step 6/93  loss=2.1450  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.820e+01 (batch 6)
  ... step 8/93  loss=1.0311  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.259e+01 (batch 8)
  ... step 10/93  loss=0.7912  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.025e+01 (batch 10)
  ... step 12/93  loss=1.3147  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.281e+01 (batch 12)
  ... step 14/93  loss=0.6561  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.097e+01 (batch 14)
  ... step 16/93  loss=0.9569  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.102e+01 (batch 16)
  ... step 18/93  loss=0.4800  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.083e+00 (batch 18)
  ... step 20/93  loss=1.6249  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.296e+01 (batch 20)
  ... step 22/93  loss=1.1143  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.330e+01 (batch 22)
  ... step 24/93  loss=1.4475  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.842e+01 (batch 24)
  ... step 26/93  loss=0.2955  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.533e+00 (batch 26)
  ... step 28/93  loss=1.0262  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.294e+01 (batch 28)
  ... step 30/93  loss=1.0786  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.272e+01 (batch 30)
  ... step 32/93  loss=0.3889  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.689e+00 (batch 32)
  ... step 34/93  loss=1.6609  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.436e+01 (batch 34)
  ... step 36/93  loss=1.0211  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.477e+01 (batch 36)
  ... step 38/93  loss=1.3821  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.632e+01 (batch 38)
  ... step 40/93  loss=2.1010  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.756e+01 (batch 40)
  ... step 42/93  loss=0.4216  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.771e+00 (batch 42)
  ... step 44/93  loss=0.9554  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.252e+00 (batch 44)
  ... step 46/93  loss=1.2716  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.108e+01 (batch 46)
  ... step 48/93  loss=0.6537  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.463e+00 (batch 48)
  ... step 50/93  loss=1.7565  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.327e+01 (batch 50)
  ... step 52/93  loss=1.3426  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.219e+01 (batch 52)
  ... step 54/93  loss=1.2121  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.197e+01 (batch 54)
  ... step 56/93  loss=1.4951  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.529e+01 (batch 56)
  ... step 58/93  loss=1.0102  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.160e+01 (batch 58)
  ... step 60/93  loss=0.9154  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.046e+01 (batch 60)
  ... step 62/93  loss=1.6974  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.521e+01 (batch 62)
  ... step 64/93  loss=0.9616  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.820e+00 (batch 64)
  ... step 66/93  loss=0.5483  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.685e+00 (batch 66)
  ... step 68/93  loss=1.6069  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.193e+01 (batch 68)
  ... step 70/93  loss=1.1900  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.566e+01 (batch 70)
  ... step 72/93  loss=0.6063  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.975e+00 (batch 72)
  ... step 74/93  loss=0.8007  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.124e+01 (batch 74)
  ... step 76/93  loss=2.0877  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.167e+01 (batch 76)
  ... step 78/93  loss=0.6848  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.614e+00 (batch 78)
  ... step 80/93  loss=0.4168  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.869e+00 (batch 80)
  ... step 82/93  loss=1.0739  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.186e+01 (batch 82)
  ... step 84/93  loss=0.6844  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.090e+00 (batch 84)
  ... step 86/93  loss=0.2754  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.310e+00 (batch 86)
  ... step 88/93  loss=0.4883  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.992e+00 (batch 88)
  ... step 90/93  loss=1.1755  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.175e+01 (batch 90)
  ... step 92/93  loss=0.2500  (0.3s since last print)
  â†˜ grad L2 norm â‰ˆ 1.008e+01 (batch 92)
âœ… epoch 2 forward/backward done in 26.3s
  ðŸ”Ž val step 0: batch (4, 32, 2, 324, 1, 16) loss=1.1345
[02] train=0.9874  val=1.3445  RMSE(std)=[Qi:1.130, Qe:1.201, Î“:1.147]  RMSE(phys)=[Qi:82.033, Qe:111.111, Î“:50.658]  (1.6s)
âœ… Saved unified NN â†’ ./mnt/data/bin.cgyro.nn
ðŸŸ¦ Starting epoch 3/30 (train steps â‰ˆ 93)
  ... step 0/93  loss=1.3132  (0.1s since last print)
  â†˜ grad L2 norm â‰ˆ 1.157e+01 (batch 0)
  ... step 2/93  loss=1.4180  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.437e+01 (batch 2)
  ... step 4/93  loss=0.8382  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.930e+00 (batch 4)
  ... step 6/93  loss=1.5064  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.427e+01 (batch 6)
  ... step 8/93  loss=0.4395  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.650e+00 (batch 8)
  ... step 10/93  loss=1.0019  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.117e+01 (batch 10)
  ... step 12/93  loss=0.8686  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.308e+01 (batch 12)
  ... step 14/93  loss=0.7243  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.103e+01 (batch 14)
  ... step 16/93  loss=0.9698  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.254e+01 (batch 16)
  ... step 18/93  loss=1.9057  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.678e+01 (batch 18)
  ... step 20/93  loss=0.6370  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.559e+00 (batch 20)
  ... step 22/93  loss=1.0502  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.453e+01 (batch 22)
  ... step 24/93  loss=1.4807  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.494e+01 (batch 24)
  ... step 26/93  loss=0.3596  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.315e+00 (batch 26)
  ... step 28/93  loss=0.5782  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.129e+01 (batch 28)
  ... step 30/93  loss=0.8192  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.051e+01 (batch 30)
  ... step 32/93  loss=0.9192  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.080e+01 (batch 32)
  ... step 34/93  loss=0.9958  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.033e+00 (batch 34)
  ... step 36/93  loss=0.3440  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.544e+00 (batch 36)
  ... step 38/93  loss=1.0003  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.357e+01 (batch 38)
  ... step 40/93  loss=1.2341  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.253e+01 (batch 40)
  ... step 42/93  loss=0.5772  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.320e+01 (batch 42)
  ... step 44/93  loss=0.9783  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.986e+00 (batch 44)
  ... step 46/93  loss=1.0891  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.271e+01 (batch 46)
  ... step 48/93  loss=0.8202  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.333e+00 (batch 48)
  ... step 50/93  loss=0.7254  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.006e+01 (batch 50)
  ... step 52/93  loss=1.0180  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.030e+01 (batch 52)
  ... step 54/93  loss=0.3991  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.174e+00 (batch 54)
  ... step 56/93  loss=1.0838  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.325e+01 (batch 56)
  ... step 58/93  loss=1.0023  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.303e+01 (batch 58)
  ... step 60/93  loss=1.5852  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.799e+01 (batch 60)
  ... step 62/93  loss=0.9312  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.632e+01 (batch 62)
  ... step 64/93  loss=0.7366  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.239e+01 (batch 64)
  ... step 66/93  loss=0.8528  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.305e+01 (batch 66)
  ... step 68/93  loss=0.6728  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.064e+00 (batch 68)
  ... step 70/93  loss=1.4994  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.554e+01 (batch 70)
  ... step 72/93  loss=0.2115  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.013e+00 (batch 72)
  ... step 74/93  loss=0.8701  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.610e+01 (batch 74)
  ... step 76/93  loss=1.8111  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.336e+01 (batch 76)
  ... step 78/93  loss=1.8600  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.369e+01 (batch 78)
  ... step 80/93  loss=0.2464  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.162e+00 (batch 80)
  ... step 82/93  loss=0.5036  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.031e+01 (batch 82)
  ... step 84/93  loss=2.5115  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.277e+01 (batch 84)
  ... step 86/93  loss=0.9364  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.229e+00 (batch 86)
  ... step 88/93  loss=1.1364  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.539e+00 (batch 88)
  ... step 90/93  loss=0.7854  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.821e+01 (batch 90)
  ... step 92/93  loss=0.7621  (0.3s since last print)
  â†˜ grad L2 norm â‰ˆ 2.299e+01 (batch 92)
âœ… epoch 3 forward/backward done in 26.4s
  ðŸ”Ž val step 0: batch (4, 32, 2, 324, 1, 16) loss=1.0483
[03] train=0.9221  val=1.2307  RMSE(std)=[Qi:1.089, Qe:1.154, Î“:1.084]  RMSE(phys)=[Qi:79.064, Qe:106.825, Î“:47.856]  (1.6s)
âœ… Saved unified NN â†’ ./mnt/data/bin.cgyro.nn
ðŸŸ¦ Starting epoch 4/30 (train steps â‰ˆ 93)
  ... step 0/93  loss=0.6856  (0.1s since last print)
  â†˜ grad L2 norm â‰ˆ 9.006e+00 (batch 0)
  ... step 2/93  loss=0.6079  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.222e+00 (batch 2)
  ... step 4/93  loss=0.3943  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.630e+00 (batch 4)
  ... step 6/93  loss=0.8567  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.300e+01 (batch 6)
  ... step 8/93  loss=0.4782  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.664e+00 (batch 8)
  ... step 10/93  loss=0.5455  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.962e+00 (batch 10)
  ... step 12/93  loss=0.3998  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.325e+00 (batch 12)
  ... step 14/93  loss=1.3970  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.730e+01 (batch 14)
  ... step 16/93  loss=1.5489  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.613e+01 (batch 16)
  ... step 18/93  loss=0.9871  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.614e+01 (batch 18)
  ... step 20/93  loss=0.8509  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.121e+01 (batch 20)
  ... step 22/93  loss=0.4447  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.489e+00 (batch 22)
  ... step 24/93  loss=1.1312  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.675e+01 (batch 24)
  ... step 26/93  loss=0.8287  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.727e+00 (batch 26)
  ... step 28/93  loss=1.2618  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.487e+01 (batch 28)
  ... step 30/93  loss=0.6515  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.056e+00 (batch 30)
  ... step 32/93  loss=1.0361  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.506e+01 (batch 32)
  ... step 34/93  loss=0.3385  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.106e+01 (batch 34)
  ... step 36/93  loss=0.6194  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.323e+01 (batch 36)
  ... step 38/93  loss=1.1014  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.649e+01 (batch 38)
  ... step 40/93  loss=0.8231  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.293e+01 (batch 40)
  ... step 42/93  loss=0.8228  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.857e+00 (batch 42)
  ... step 44/93  loss=0.9503  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.880e+01 (batch 44)
  ... step 46/93  loss=0.9965  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.898e+01 (batch 46)
  ... step 48/93  loss=0.1884  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.674e+00 (batch 48)
  ... step 50/93  loss=0.3705  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.628e+01 (batch 50)
  ... step 52/93  loss=0.6481  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.858e+01 (batch 52)
  ... step 54/93  loss=1.1449  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.298e+01 (batch 54)
  ... step 56/93  loss=1.1487  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.064e+01 (batch 56)
  ... step 58/93  loss=0.7309  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.035e+01 (batch 58)
  ... step 60/93  loss=0.9218  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.386e+01 (batch 60)
  ... step 62/93  loss=0.5912  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.706e+01 (batch 62)
  ... step 64/93  loss=1.3914  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.618e+01 (batch 64)
  ... step 66/93  loss=1.5501  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.964e+01 (batch 66)
  ... step 68/93  loss=0.1959  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.297e+00 (batch 68)
  ... step 70/93  loss=0.5983  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.088e+00 (batch 70)
  ... step 72/93  loss=0.4419  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.757e+00 (batch 72)
  ... step 74/93  loss=0.6353  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.907e+00 (batch 74)
  ... step 76/93  loss=0.4261  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.351e+01 (batch 76)
  ... step 78/93  loss=0.3739  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.826e+00 (batch 78)
  ... step 80/93  loss=1.1677  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.508e+01 (batch 80)
  ... step 82/93  loss=0.8349  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.513e+01 (batch 82)
  ... step 84/93  loss=0.3343  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.142e+00 (batch 84)
  ... step 86/93  loss=0.4417  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.335e+00 (batch 86)
  ... step 88/93  loss=0.4823  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.007e+01 (batch 88)
  ... step 90/93  loss=0.5593  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.064e+01 (batch 90)
  ... step 92/93  loss=0.7820  (0.3s since last print)
  â†˜ grad L2 norm â‰ˆ 2.087e+01 (batch 92)
âœ… epoch 4 forward/backward done in 26.4s
  ðŸ”Ž val step 0: batch (4, 32, 2, 324, 1, 16) loss=6.7037
[04] train=0.7928  val=6.6560  RMSE(std)=[Qi:2.410, Qe:2.583, Î“:2.736]  RMSE(phys)=[Qi:175.017, Qe:239.085, Î“:120.821]  (1.6s)
âœ… Saved unified NN â†’ ./mnt/data/bin.cgyro.nn
ðŸŸ¦ Starting epoch 5/30 (train steps â‰ˆ 93)
  ... step 0/93  loss=0.9303  (0.1s since last print)
  â†˜ grad L2 norm â‰ˆ 1.503e+01 (batch 0)
  ... step 2/93  loss=0.4052  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.430e+00 (batch 2)
  ... step 4/93  loss=1.2805  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.569e+01 (batch 4)
  ... step 6/93  loss=0.4111  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.626e+00 (batch 6)
  ... step 8/93  loss=0.4429  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.173e+00 (batch 8)
  ... step 10/93  loss=0.7671  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.122e+01 (batch 10)
  ... step 12/93  loss=0.6466  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.356e+01 (batch 12)
  ... step 14/93  loss=0.8112  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.508e+00 (batch 14)
  ... step 16/93  loss=0.8152  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.356e+01 (batch 16)
  ... step 18/93  loss=0.5856  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.035e+01 (batch 18)
  ... step 20/93  loss=0.6931  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.106e+01 (batch 20)
  ... step 22/93  loss=0.8736  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.474e+00 (batch 22)
  ... step 24/93  loss=0.6561  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.256e+00 (batch 24)
  ... step 26/93  loss=0.3806  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.322e+00 (batch 26)
  ... step 28/93  loss=0.5505  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.992e+00 (batch 28)
  ... step 30/93  loss=0.7270  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.853e+01 (batch 30)
  ... step 32/93  loss=1.0578  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.260e+01 (batch 32)
  ... step 34/93  loss=0.5570  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.249e+01 (batch 34)
  ... step 36/93  loss=0.5368  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.538e+00 (batch 36)
  ... step 38/93  loss=0.6322  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.049e+00 (batch 38)
  ... step 40/93  loss=1.0435  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.330e+01 (batch 40)
  ... step 42/93  loss=1.3232  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.759e+01 (batch 42)
  ... step 44/93  loss=0.8017  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.354e+01 (batch 44)
  ... step 46/93  loss=0.8194  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.361e+00 (batch 46)
  ... step 48/93  loss=1.3189  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.789e+01 (batch 48)
  ... step 50/93  loss=0.7080  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.132e+01 (batch 50)
  ... step 52/93  loss=0.3130  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.651e+00 (batch 52)
  ... step 54/93  loss=0.6181  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.720e+00 (batch 54)
  ... step 56/93  loss=0.3889  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.669e+00 (batch 56)
  ... step 58/93  loss=0.6497  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.942e+00 (batch 58)
  ... step 60/93  loss=0.8967  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.132e+01 (batch 60)
  ... step 62/93  loss=0.4001  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.898e+00 (batch 62)
  ... step 64/93  loss=0.4149  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.705e+00 (batch 64)
  ... step 66/93  loss=0.8054  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.285e+00 (batch 66)
  ... step 68/93  loss=0.6916  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.129e+01 (batch 68)
  ... step 70/93  loss=0.7057  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.562e+01 (batch 70)
  ... step 72/93  loss=0.1964  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.331e+00 (batch 72)
  ... step 74/93  loss=1.3572  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.420e+01 (batch 74)
  ... step 76/93  loss=1.0687  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.093e+01 (batch 76)
  ... step 78/93  loss=0.5355  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.112e+01 (batch 78)
  ... step 80/93  loss=0.6635  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.227e+01 (batch 80)
  ... step 82/93  loss=0.6239  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.039e+01 (batch 82)
  ... step 84/93  loss=1.2696  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.536e+01 (batch 84)
  ... step 86/93  loss=0.7559  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.193e+01 (batch 86)
  ... step 88/93  loss=0.6763  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.500e+01 (batch 88)
  ... step 90/93  loss=0.6240  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.977e+00 (batch 90)
  ... step 92/93  loss=0.7330  (0.3s since last print)
  â†˜ grad L2 norm â‰ˆ 1.630e+01 (batch 92)
âœ… epoch 5 forward/backward done in 26.4s
  ðŸ”Ž val step 0: batch (4, 32, 2, 324, 1, 16) loss=1.2045
[05] train=0.7122  val=0.8582  RMSE(std)=[Qi:0.903, Qe:0.971, Î“:0.904]  RMSE(phys)=[Qi:65.569, Qe:89.837, Î“:39.916]  (1.6s)
âœ… Saved unified NN â†’ ./mnt/data/bin.cgyro.nn
ðŸŸ¦ Starting epoch 6/30 (train steps â‰ˆ 93)
  ... step 0/93  loss=0.6066  (0.1s since last print)
  â†˜ grad L2 norm â‰ˆ 1.079e+01 (batch 0)
  ... step 2/93  loss=0.6993  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.023e+01 (batch 2)
  ... step 4/93  loss=0.7572  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.615e+01 (batch 4)
  ... step 6/93  loss=0.9679  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.420e+01 (batch 6)
  ... step 8/93  loss=0.7871  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.320e+01 (batch 8)
  ... step 10/93  loss=0.2251  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.750e+00 (batch 10)
  ... step 12/93  loss=0.7233  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.152e+01 (batch 12)
  ... step 14/93  loss=0.2987  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.948e+00 (batch 14)
  ... step 16/93  loss=0.3014  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.003e+00 (batch 16)
  ... step 18/93  loss=0.4593  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.696e+00 (batch 18)
  ... step 20/93  loss=0.7262  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.338e+01 (batch 20)
  ... step 22/93  loss=0.4145  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.030e+01 (batch 22)
  ... step 24/93  loss=0.8257  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.062e+01 (batch 24)
  ... step 26/93  loss=0.9685  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.988e+01 (batch 26)
  ... step 28/93  loss=1.0647  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.249e+01 (batch 28)
  ... step 30/93  loss=1.0746  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.305e+01 (batch 30)
  ... step 32/93  loss=0.2972  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.280e+00 (batch 32)
  ... step 34/93  loss=0.7673  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.015e+01 (batch 34)
  ... step 36/93  loss=0.5019  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.565e+00 (batch 36)
  ... step 38/93  loss=0.4241  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.039e+01 (batch 38)
  ... step 40/93  loss=0.4950  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.197e+01 (batch 40)
  ... step 42/93  loss=0.7048  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.514e+01 (batch 42)
  ... step 44/93  loss=1.0405  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.848e+01 (batch 44)
  ... step 46/93  loss=0.6323  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.968e+01 (batch 46)
  ... step 48/93  loss=0.8014  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.139e+01 (batch 48)
  ... step 50/93  loss=0.6760  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.645e+01 (batch 50)
  ... step 52/93  loss=0.4272  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.585e+00 (batch 52)
  ... step 54/93  loss=0.3585  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.784e+00 (batch 54)
  ... step 56/93  loss=0.4307  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.465e+00 (batch 56)
  ... step 58/93  loss=0.6607  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.174e+01 (batch 58)
  ... step 60/93  loss=0.3836  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.672e+00 (batch 60)
  ... step 62/93  loss=0.6778  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.493e+00 (batch 62)
  ... step 64/93  loss=0.3614  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.584e+00 (batch 64)
  ... step 66/93  loss=0.5824  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.040e+01 (batch 66)
  ... step 68/93  loss=1.3548  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.027e+01 (batch 68)
  ... step 70/93  loss=1.0882  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.373e+01 (batch 70)
  ... step 72/93  loss=0.2735  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.478e+00 (batch 72)
  ... step 74/93  loss=1.0073  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.485e+01 (batch 74)
  ... step 76/93  loss=0.5419  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.701e+00 (batch 76)
  ... step 78/93  loss=0.7499  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.062e+01 (batch 78)
  ... step 80/93  loss=0.5069  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.939e+00 (batch 80)
  ... step 82/93  loss=0.5548  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.151e+01 (batch 82)
  ... step 84/93  loss=0.5610  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.333e+00 (batch 84)
  ... step 86/93  loss=0.7811  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.012e+01 (batch 86)
  ... step 88/93  loss=0.7185  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.169e+00 (batch 88)
  ... step 90/93  loss=0.6993  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.105e+01 (batch 90)
  ... step 92/93  loss=1.3477  (0.3s since last print)
  â†˜ grad L2 norm â‰ˆ 3.538e+01 (batch 92)
âœ… epoch 6 forward/backward done in 26.4s
  ðŸ”Ž val step 0: batch (4, 32, 2, 324, 1, 16) loss=1.0953
[06] train=0.7016  val=0.8562  RMSE(std)=[Qi:0.898, Qe:0.967, Î“:0.910]  RMSE(phys)=[Qi:65.182, Qe:89.499, Î“:40.176]  (1.6s)
âœ… Saved unified NN â†’ ./mnt/data/bin.cgyro.nn
ðŸŸ¦ Starting epoch 7/30 (train steps â‰ˆ 93)
  ... step 0/93  loss=1.1966  (0.1s since last print)
  â†˜ grad L2 norm â‰ˆ 1.257e+01 (batch 0)
  ... step 2/93  loss=0.9278  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.019e+01 (batch 2)
  ... step 4/93  loss=0.5184  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.071e+01 (batch 4)
  ... step 6/93  loss=0.4203  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.210e+00 (batch 6)
  ... step 8/93  loss=0.3492  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.680e+00 (batch 8)
  ... step 10/93  loss=0.6290  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.084e+01 (batch 10)
  ... step 12/93  loss=1.0575  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.500e+01 (batch 12)
  ... step 14/93  loss=0.8460  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.226e+01 (batch 14)
  ... step 16/93  loss=0.4093  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.063e+01 (batch 16)
  ... step 18/93  loss=0.6050  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.416e+00 (batch 18)
  ... step 20/93  loss=0.4742  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.155e+00 (batch 20)
  ... step 22/93  loss=1.3241  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.580e+01 (batch 22)
  ... step 24/93  loss=0.4261  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.789e+00 (batch 24)
  ... step 26/93  loss=0.5927  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.023e+01 (batch 26)
  ... step 28/93  loss=0.8024  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.079e+01 (batch 28)
  ... step 30/93  loss=1.1336  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.603e+00 (batch 30)
  ... step 32/93  loss=0.4107  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.538e+00 (batch 32)
  ... step 34/93  loss=0.5468  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.090e+00 (batch 34)
  ... step 36/93  loss=0.3360  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.457e+00 (batch 36)
  ... step 38/93  loss=0.4663  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.634e+00 (batch 38)
  ... step 40/93  loss=0.5172  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.549e+00 (batch 40)
  ... step 42/93  loss=0.6524  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.450e+00 (batch 42)
  ... step 44/93  loss=0.3536  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.456e+00 (batch 44)
  ... step 46/93  loss=0.3131  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.629e+00 (batch 46)
  ... step 48/93  loss=0.4432  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.970e+00 (batch 48)
  ... step 50/93  loss=0.4277  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.937e+00 (batch 50)
  ... step 52/93  loss=0.7320  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.186e+01 (batch 52)
  ... step 54/93  loss=0.9963  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.073e+01 (batch 54)
  ... step 56/93  loss=0.4643  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.537e+00 (batch 56)
  ... step 58/93  loss=0.6952  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.077e+01 (batch 58)
  ... step 60/93  loss=0.2037  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.778e+00 (batch 60)
  ... step 62/93  loss=0.6875  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.470e+00 (batch 62)
  ... step 64/93  loss=0.3942  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.471e+00 (batch 64)
  ... step 66/93  loss=0.2165  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.200e+00 (batch 66)
  ... step 68/93  loss=0.7547  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.066e+01 (batch 68)
  ... step 70/93  loss=0.6140  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.440e+00 (batch 70)
  ... step 72/93  loss=0.2641  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.873e+00 (batch 72)
  ... step 74/93  loss=0.3653  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.483e+00 (batch 74)
  ... step 76/93  loss=0.4985  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.318e+00 (batch 76)
  ... step 78/93  loss=0.5515  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.247e+01 (batch 78)
  ... step 80/93  loss=1.2296  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.288e+01 (batch 80)
  ... step 82/93  loss=0.4794  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.258e+01 (batch 82)
  ... step 84/93  loss=0.9309  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.143e+01 (batch 84)
  ... step 86/93  loss=0.4377  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.446e+00 (batch 86)
  ... step 88/93  loss=0.5886  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.899e+00 (batch 88)
  ... step 90/93  loss=0.6450  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.844e+00 (batch 90)
  ... step 92/93  loss=0.6391  (0.3s since last print)
  â†˜ grad L2 norm â‰ˆ 1.847e+01 (batch 92)
âœ… epoch 7 forward/backward done in 26.4s
  ðŸ”Ž val step 0: batch (4, 32, 2, 324, 1, 16) loss=1.1451
[07] train=0.6494  val=0.9089  RMSE(std)=[Qi:0.932, Qe:0.992, Î“:0.935]  RMSE(phys)=[Qi:67.683, Qe:91.796, Î“:41.291]  (1.6s)
âœ… Saved unified NN â†’ ./mnt/data/bin.cgyro.nn
ðŸŸ¦ Starting epoch 8/30 (train steps â‰ˆ 93)
  ... step 0/93  loss=0.3721  (0.1s since last print)
  â†˜ grad L2 norm â‰ˆ 8.958e+00 (batch 0)
  ... step 2/93  loss=0.6983  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.863e+00 (batch 2)
  ... step 4/93  loss=0.4539  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.523e+00 (batch 4)
  ... step 6/93  loss=0.6404  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.478e+01 (batch 6)
  ... step 8/93  loss=0.3532  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.048e+01 (batch 8)
  ... step 10/93  loss=0.3514  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.048e+01 (batch 10)
  ... step 12/93  loss=0.2706  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.133e+00 (batch 12)
  ... step 14/93  loss=0.3389  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.140e+01 (batch 14)
  ... step 16/93  loss=0.2995  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.298e+00 (batch 16)
  ... step 18/93  loss=0.3630  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.053e+00 (batch 18)
  ... step 20/93  loss=0.5092  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.963e+00 (batch 20)
  ... step 22/93  loss=0.5069  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.142e+01 (batch 22)
  ... step 24/93  loss=0.3007  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.770e+00 (batch 24)
  ... step 26/93  loss=0.3842  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.033e+01 (batch 26)
  ... step 28/93  loss=0.6773  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.913e+00 (batch 28)
  ... step 30/93  loss=1.1437  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.720e+01 (batch 30)
  ... step 32/93  loss=0.3991  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.058e+01 (batch 32)
  ... step 34/93  loss=0.7409  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.898e+01 (batch 34)
  ... step 36/93  loss=0.5985  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.158e+01 (batch 36)
  ... step 38/93  loss=0.6690  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.127e+01 (batch 38)
  ... step 40/93  loss=1.1243  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.486e+01 (batch 40)
  ... step 42/93  loss=0.9222  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.377e+01 (batch 42)
  ... step 44/93  loss=0.3706  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.907e+00 (batch 44)
  ... step 46/93  loss=0.8252  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.095e+01 (batch 46)
  ... step 48/93  loss=0.3754  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.442e+00 (batch 48)
  ... step 50/93  loss=0.3794  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.924e+00 (batch 50)
  ... step 52/93  loss=0.2624  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.164e+00 (batch 52)
  ... step 54/93  loss=0.5611  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.144e+01 (batch 54)
  ... step 56/93  loss=0.3865  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.779e+00 (batch 56)
  ... step 58/93  loss=0.4711  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.852e+00 (batch 58)
  ... step 60/93  loss=0.3874  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.365e+00 (batch 60)
  ... step 62/93  loss=0.7117  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.333e+00 (batch 62)
  ... step 64/93  loss=0.2715  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.273e+00 (batch 64)
  ... step 66/93  loss=1.1280  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.279e+01 (batch 66)
  ... step 68/93  loss=0.9068  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.586e+00 (batch 68)
  ... step 70/93  loss=0.8326  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.165e+01 (batch 70)
  ... step 72/93  loss=0.9974  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.347e+01 (batch 72)
  ... step 74/93  loss=0.3752  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.101e+00 (batch 74)
  ... step 76/93  loss=0.2491  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.501e+00 (batch 76)
  ... step 78/93  loss=1.1503  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.461e+01 (batch 78)
  ... step 80/93  loss=0.2352  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.137e+00 (batch 80)
  ... step 82/93  loss=0.2801  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.977e+00 (batch 82)
  ... step 84/93  loss=0.8288  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.226e+01 (batch 84)
  ... step 86/93  loss=0.6504  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.727e+00 (batch 86)
  ... step 88/93  loss=0.5567  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.980e+00 (batch 88)
  ... step 90/93  loss=0.4745  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.731e+00 (batch 90)
  ... step 92/93  loss=0.4085  (0.3s since last print)
  â†˜ grad L2 norm â‰ˆ 1.593e+01 (batch 92)
âœ… epoch 8 forward/backward done in 26.4s
  ðŸ”Ž val step 0: batch (4, 32, 2, 324, 1, 16) loss=1.0586
[08] train=0.6219  val=0.8532  RMSE(std)=[Qi:0.898, Qe:0.957, Î“:0.914]  RMSE(phys)=[Qi:65.244, Qe:88.595, Î“:40.380]  (1.6s)
âœ… Saved unified NN â†’ ./mnt/data/bin.cgyro.nn
ðŸŸ¦ Starting epoch 9/30 (train steps â‰ˆ 93)
  ... step 0/93  loss=0.2604  (0.1s since last print)
  â†˜ grad L2 norm â‰ˆ 5.523e+00 (batch 0)
  ... step 2/93  loss=0.4357  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.219e+01 (batch 2)
  ... step 4/93  loss=0.6916  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.027e+01 (batch 4)
  ... step 6/93  loss=0.8397  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.334e+01 (batch 6)
  ... step 8/93  loss=0.8210  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.238e+01 (batch 8)
  ... step 10/93  loss=1.3762  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.722e+01 (batch 10)
  ... step 12/93  loss=0.5582  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.033e+01 (batch 12)
  ... step 14/93  loss=0.9141  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.222e+01 (batch 14)
  ... step 16/93  loss=0.2766  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.142e+00 (batch 16)
  ... step 18/93  loss=0.9915  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.283e+01 (batch 18)
  ... step 20/93  loss=0.6822  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.588e+00 (batch 20)
  ... step 22/93  loss=0.6704  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.195e+01 (batch 22)
  ... step 24/93  loss=0.4008  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.280e+00 (batch 24)
  ... step 26/93  loss=0.4175  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.062e+00 (batch 26)
  ... step 28/93  loss=0.7712  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.849e+00 (batch 28)
  ... step 30/93  loss=0.8938  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.267e+00 (batch 30)
  ... step 32/93  loss=0.5250  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.323e+00 (batch 32)
  ... step 34/93  loss=0.9749  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.091e+00 (batch 34)
  ... step 36/93  loss=0.2262  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.221e+00 (batch 36)
  ... step 38/93  loss=0.5417  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.544e+00 (batch 38)
  ... step 40/93  loss=0.2974  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.721e+00 (batch 40)
  ... step 42/93  loss=0.3918  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.799e+00 (batch 42)
  ... step 44/93  loss=0.2064  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.024e+00 (batch 44)
  ... step 46/93  loss=0.4235  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.017e+00 (batch 46)
  ... step 48/93  loss=0.4953  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.667e+00 (batch 48)
  ... step 50/93  loss=0.3912  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.753e+00 (batch 50)
  ... step 52/93  loss=0.6491  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.568e+00 (batch 52)
  ... step 54/93  loss=0.4040  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.286e+00 (batch 54)
  ... step 56/93  loss=0.4070  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.787e+00 (batch 56)
  ... step 58/93  loss=0.5046  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.944e+00 (batch 58)
  ... step 60/93  loss=0.4166  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.787e+00 (batch 60)
  ... step 62/93  loss=1.3532  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.518e+01 (batch 62)
  ... step 64/93  loss=0.3537  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.072e+00 (batch 64)
  ... step 66/93  loss=0.6705  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.265e+00 (batch 66)
  ... step 68/93  loss=1.0109  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.161e+01 (batch 68)
  ... step 70/93  loss=0.4894  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.049e+01 (batch 70)
  ... step 72/93  loss=0.3046  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.221e+00 (batch 72)
  ... step 74/93  loss=1.0167  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.509e+01 (batch 74)
  ... step 76/93  loss=1.0903  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.006e+01 (batch 76)
  ... step 78/93  loss=0.2672  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.548e+00 (batch 78)
  ... step 80/93  loss=0.1739  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.683e+00 (batch 80)
  ... step 82/93  loss=0.3086  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.223e+00 (batch 82)
  ... step 84/93  loss=0.5976  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.136e+00 (batch 84)
  ... step 86/93  loss=0.4442  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.047e+01 (batch 86)
  ... step 88/93  loss=0.7806  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.197e+01 (batch 88)
  ... step 90/93  loss=0.2163  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.752e+00 (batch 90)
  ... step 92/93  loss=0.5703  (0.3s since last print)
  â†˜ grad L2 norm â‰ˆ 1.781e+01 (batch 92)
âœ… epoch 9 forward/backward done in 26.4s
  ðŸ”Ž val step 0: batch (4, 32, 2, 324, 1, 16) loss=1.0357
[09] train=0.6113  val=0.7280  RMSE(std)=[Qi:0.823, Qe:0.897, Î“:0.837]  RMSE(phys)=[Qi:59.777, Qe:83.050, Î“:36.976]  (1.6s)
âœ… Saved unified NN â†’ ./mnt/data/bin.cgyro.nn
ðŸŸ¦ Starting epoch 10/30 (train steps â‰ˆ 93)
  ... step 0/93  loss=1.0949  (0.1s since last print)
  â†˜ grad L2 norm â‰ˆ 1.849e+01 (batch 0)
  ... step 2/93  loss=0.7958  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.146e+01 (batch 2)
  ... step 4/93  loss=0.6161  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.432e+01 (batch 4)
  ... step 6/93  loss=1.5852  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.717e+01 (batch 6)
  ... step 8/93  loss=0.4110  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.610e+00 (batch 8)
  ... step 10/93  loss=0.1231  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.096e+00 (batch 10)
  ... step 12/93  loss=0.8647  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.095e+01 (batch 12)
  ... step 14/93  loss=0.9562  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.120e+01 (batch 14)
  ... step 16/93  loss=0.2591  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.935e+00 (batch 16)
  ... step 18/93  loss=0.9389  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.272e+01 (batch 18)
  ... step 20/93  loss=0.9735  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.230e+01 (batch 20)
  ... step 22/93  loss=0.5059  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.254e+00 (batch 22)
  ... step 24/93  loss=0.4373  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.426e+01 (batch 24)
  ... step 26/93  loss=0.3878  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.758e+00 (batch 26)
  ... step 28/93  loss=0.1289  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.848e+00 (batch 28)
  ... step 30/93  loss=0.7691  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.285e+01 (batch 30)
  ... step 32/93  loss=0.3445  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.900e+00 (batch 32)
  ... step 34/93  loss=1.2405  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.195e+01 (batch 34)
  ... step 36/93  loss=0.3789  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.648e+00 (batch 36)
  ... step 38/93  loss=0.6005  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.149e+00 (batch 38)
  ... step 40/93  loss=0.3282  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.536e+00 (batch 40)
  ... step 42/93  loss=0.3014  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.593e+00 (batch 42)
  ... step 44/93  loss=0.6688  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.076e+01 (batch 44)
  ... step 46/93  loss=1.1496  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.186e+01 (batch 46)
  ... step 48/93  loss=0.6164  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.018e+01 (batch 48)
  ... step 50/93  loss=1.1975  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.140e+01 (batch 50)
  ... step 52/93  loss=1.2431  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.667e+01 (batch 52)
  ... step 54/93  loss=0.2506  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.551e+00 (batch 54)
  ... step 56/93  loss=0.6221  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.159e+01 (batch 56)
  ... step 58/93  loss=0.3421  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.121e+00 (batch 58)
  ... step 60/93  loss=0.4267  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.087e+00 (batch 60)
  ... step 62/93  loss=0.3747  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.224e+00 (batch 62)
  ... step 64/93  loss=0.2611  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.354e+00 (batch 64)
  ... step 66/93  loss=0.3509  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.622e+00 (batch 66)
  ... step 68/93  loss=0.4007  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.373e+00 (batch 68)
  ... step 70/93  loss=0.3967  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.734e+00 (batch 70)
  ... step 72/93  loss=0.2021  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.263e+00 (batch 72)
  ... step 74/93  loss=0.3081  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.880e+00 (batch 74)
  ... step 76/93  loss=0.3171  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.069e+00 (batch 76)
  ... step 78/93  loss=0.5068  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.644e+00 (batch 78)
  ... step 80/93  loss=0.8746  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.135e+01 (batch 80)
  ... step 82/93  loss=0.5402  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.418e+00 (batch 82)
  ... step 84/93  loss=0.3431  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.838e+00 (batch 84)
  ... step 86/93  loss=0.8616  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.083e+01 (batch 86)
  ... step 88/93  loss=0.8784  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.070e+00 (batch 88)
  ... step 90/93  loss=0.6404  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.434e+00 (batch 90)
  ... step 92/93  loss=0.2880  (0.3s since last print)
  â†˜ grad L2 norm â‰ˆ 1.093e+01 (batch 92)
âœ… epoch 10 forward/backward done in 26.4s
  ðŸ”Ž val step 0: batch (4, 32, 2, 324, 1, 16) loss=1.1664
[10] train=0.6145  val=0.7799  RMSE(std)=[Qi:0.855, Qe:0.924, Î“:0.868]  RMSE(phys)=[Qi:62.120, Qe:85.548, Î“:38.335]  (1.6s)
âœ… Saved unified NN â†’ ./mnt/data/bin.cgyro.nn
ðŸŸ¦ Starting epoch 11/30 (train steps â‰ˆ 93)
  ... step 0/93  loss=0.4631  (0.1s since last print)
  â†˜ grad L2 norm â‰ˆ 8.466e+00 (batch 0)
  ... step 2/93  loss=0.4485  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.871e+00 (batch 2)
  ... step 4/93  loss=0.4915  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.434e+00 (batch 4)
  ... step 6/93  loss=0.2830  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.785e+00 (batch 6)
  ... step 8/93  loss=0.6751  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.410e+00 (batch 8)
  ... step 10/93  loss=0.9042  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.447e+01 (batch 10)
  ... step 12/93  loss=0.2054  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.539e+00 (batch 12)
  ... step 14/93  loss=0.2293  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.459e+00 (batch 14)
  ... step 16/93  loss=0.6392  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.138e+00 (batch 16)
  ... step 18/93  loss=1.1820  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.307e+01 (batch 18)
  ... step 20/93  loss=0.6215  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.593e+00 (batch 20)
  ... step 22/93  loss=0.4353  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.434e+00 (batch 22)
  ... step 24/93  loss=1.1220  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.587e+01 (batch 24)
  ... step 26/93  loss=1.3148  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.105e+01 (batch 26)
  ... step 28/93  loss=0.2267  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.100e+00 (batch 28)
  ... step 30/93  loss=0.9145  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.043e+00 (batch 30)
  ... step 32/93  loss=0.6761  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.655e+00 (batch 32)
  ... step 34/93  loss=0.4928  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.225e+00 (batch 34)
  ... step 36/93  loss=0.1529  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.537e+00 (batch 36)
  ... step 38/93  loss=0.5781  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.345e+01 (batch 38)
  ... step 40/93  loss=0.8933  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.403e+00 (batch 40)
  ... step 42/93  loss=0.3254  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.246e+00 (batch 42)
  ... step 44/93  loss=0.3418  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.400e+00 (batch 44)
  ... step 46/93  loss=0.3087  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.409e+00 (batch 46)
  ... step 48/93  loss=0.4412  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.325e+00 (batch 48)
  ... step 50/93  loss=0.5000  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.021e+00 (batch 50)
  ... step 52/93  loss=0.6321  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.722e+00 (batch 52)
  ... step 54/93  loss=0.3222  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.179e+00 (batch 54)
  ... step 56/93  loss=0.5114  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.247e+00 (batch 56)
  ... step 58/93  loss=0.4005  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.561e+00 (batch 58)
  ... step 60/93  loss=0.4795  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.375e+00 (batch 60)
  ... step 62/93  loss=0.9021  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.177e+00 (batch 62)
  ... step 64/93  loss=0.5352  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.443e+00 (batch 64)
  ... step 66/93  loss=0.1668  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.944e+00 (batch 66)
  ... step 68/93  loss=0.1777  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.260e+00 (batch 68)
  ... step 70/93  loss=0.9455  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.182e+01 (batch 70)
  ... step 72/93  loss=0.9046  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.374e+01 (batch 72)
  ... step 74/93  loss=0.2792  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.660e+00 (batch 74)
  ... step 76/93  loss=0.4858  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.703e+00 (batch 76)
  ... step 78/93  loss=0.7194  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.009e+01 (batch 78)
  ... step 80/93  loss=0.5476  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.004e+01 (batch 80)
  ... step 82/93  loss=0.8002  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.300e+00 (batch 82)
  ... step 84/93  loss=0.8973  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.146e+01 (batch 84)
  ... step 86/93  loss=0.5220  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.131e+00 (batch 86)
  ... step 88/93  loss=0.7388  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.125e+01 (batch 88)
  ... step 90/93  loss=0.8557  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.047e+01 (batch 90)
  ... step 92/93  loss=0.9569  (0.3s since last print)
  â†˜ grad L2 norm â‰ˆ 3.896e+01 (batch 92)
âœ… epoch 11 forward/backward done in 26.4s
  ðŸ”Ž val step 0: batch (4, 32, 2, 324, 1, 16) loss=0.9317
[11] train=0.5985  val=0.9715  RMSE(std)=[Qi:0.961, Qe:1.033, Î“:0.961]  RMSE(phys)=[Qi:69.802, Qe:95.592, Î“:42.439]  (1.6s)
âœ… Saved unified NN â†’ ./mnt/data/bin.cgyro.nn
ðŸŸ¦ Starting epoch 12/30 (train steps â‰ˆ 93)
  ... step 0/93  loss=0.6646  (0.1s since last print)
  â†˜ grad L2 norm â‰ˆ 7.732e+00 (batch 0)
  ... step 2/93  loss=0.7062  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.397e+00 (batch 2)
  ... step 4/93  loss=0.5545  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.114e+00 (batch 4)
  ... step 6/93  loss=0.4265  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.516e+00 (batch 6)
  ... step 8/93  loss=0.3689  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.343e+00 (batch 8)
  ... step 10/93  loss=0.3975  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.080e+00 (batch 10)
  ... step 12/93  loss=0.4841  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.444e+00 (batch 12)
  ... step 14/93  loss=0.5175  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.778e+00 (batch 14)
  ... step 16/93  loss=0.2301  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.963e+00 (batch 16)
  ... step 18/93  loss=0.7486  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.015e+00 (batch 18)
  ... step 20/93  loss=0.4875  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.467e+00 (batch 20)
  ... step 22/93  loss=0.3990  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.333e+00 (batch 22)
  ... step 24/93  loss=0.5235  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.106e+00 (batch 24)
  ... step 26/93  loss=0.8163  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.070e+01 (batch 26)
  ... step 28/93  loss=0.9606  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.196e+01 (batch 28)
  ... step 30/93  loss=0.4201  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.497e+00 (batch 30)
  ... step 32/93  loss=0.9189  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.769e+01 (batch 32)
  ... step 34/93  loss=1.1092  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.242e+01 (batch 34)
  ... step 36/93  loss=0.8881  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.410e+01 (batch 36)
  ... step 38/93  loss=0.4260  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.718e+00 (batch 38)
  ... step 40/93  loss=0.6968  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.946e+00 (batch 40)
  ... step 42/93  loss=0.5005  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.734e+00 (batch 42)
  ... step 44/93  loss=0.8589  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.404e+01 (batch 44)
  ... step 46/93  loss=0.4514  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.920e+00 (batch 46)
  ... step 48/93  loss=1.0104  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.731e+00 (batch 48)
  ... step 50/93  loss=0.3590  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.569e+00 (batch 50)
  ... step 52/93  loss=0.5595  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.010e+01 (batch 52)
  ... step 54/93  loss=0.3460  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.891e+00 (batch 54)
  ... step 56/93  loss=0.4005  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.238e+00 (batch 56)
  ... step 58/93  loss=0.3416  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.015e+00 (batch 58)
  ... step 60/93  loss=0.3812  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.274e+00 (batch 60)
  ... step 62/93  loss=1.2293  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.337e+01 (batch 62)
  ... step 64/93  loss=0.6313  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.288e+00 (batch 64)
  ... step 66/93  loss=0.6613  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.228e+01 (batch 66)
  ... step 68/93  loss=0.3572  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.326e+00 (batch 68)
  ... step 70/93  loss=0.5130  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.472e+00 (batch 70)
  ... step 72/93  loss=0.1744  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.243e+00 (batch 72)
  ... step 74/93  loss=0.5031  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.005e+00 (batch 74)
  ... step 76/93  loss=0.6588  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.125e+01 (batch 76)
  ... step 78/93  loss=0.1374  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.592e+00 (batch 78)
  ... step 80/93  loss=0.1414  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.772e+00 (batch 80)
  ... step 82/93  loss=0.6119  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.400e+00 (batch 82)
  ... step 84/93  loss=0.4416  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.887e+00 (batch 84)
  ... step 86/93  loss=0.7473  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.149e+01 (batch 86)
  ... step 88/93  loss=0.9039  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.506e+01 (batch 88)
  ... step 90/93  loss=0.4967  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.616e+00 (batch 90)
  ... step 92/93  loss=0.1204  (0.3s since last print)
  â†˜ grad L2 norm â‰ˆ 7.273e+00 (batch 92)
âœ… epoch 12 forward/backward done in 26.4s
  ðŸ”Ž val step 0: batch (4, 32, 2, 324, 1, 16) loss=1.0262
[12] train=0.5719  val=0.9014  RMSE(std)=[Qi:0.931, Qe:0.990, Î“:0.926]  RMSE(phys)=[Qi:67.587, Qe:91.614, Î“:40.909]  (1.6s)
âœ… Saved unified NN â†’ ./mnt/data/bin.cgyro.nn
ðŸŸ¦ Starting epoch 13/30 (train steps â‰ˆ 93)
  ... step 0/93  loss=0.6436  (0.1s since last print)
  â†˜ grad L2 norm â‰ˆ 1.090e+01 (batch 0)
  ... step 2/93  loss=0.3540  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.036e+00 (batch 2)
  ... step 4/93  loss=0.4407  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.487e+00 (batch 4)
  ... step 6/93  loss=1.5432  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.352e+01 (batch 6)
  ... step 8/93  loss=0.2236  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.711e+00 (batch 8)
  ... step 10/93  loss=0.3934  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.070e+00 (batch 10)
  ... step 12/93  loss=0.4030  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.532e+00 (batch 12)
  ... step 14/93  loss=0.4725  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.604e+00 (batch 14)
  ... step 16/93  loss=0.6171  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.513e+00 (batch 16)
  ... step 18/93  loss=0.1641  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.239e+00 (batch 18)
  ... step 20/93  loss=0.7461  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.042e+01 (batch 20)
  ... step 22/93  loss=0.1136  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.855e+00 (batch 22)
  ... step 24/93  loss=0.4858  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.297e+00 (batch 24)
  ... step 26/93  loss=0.3645  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.944e+00 (batch 26)
  ... step 28/93  loss=0.6165  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.513e+00 (batch 28)
  ... step 30/93  loss=0.8304  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.029e+01 (batch 30)
  ... step 32/93  loss=0.7373  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.151e+00 (batch 32)
  ... step 34/93  loss=0.4050  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.480e+00 (batch 34)
  ... step 36/93  loss=0.5488  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.346e+01 (batch 36)
  ... step 38/93  loss=1.3148  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.466e+01 (batch 38)
  ... step 40/93  loss=0.8207  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.135e+01 (batch 40)
  ... step 42/93  loss=1.5262  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.329e+01 (batch 42)
  ... step 44/93  loss=0.8047  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.744e+00 (batch 44)
  ... step 46/93  loss=0.4677  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.164e+01 (batch 46)
  ... step 48/93  loss=0.8843  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.247e+01 (batch 48)
  ... step 50/93  loss=0.4546  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.178e+00 (batch 50)
  ... step 52/93  loss=0.7037  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.712e+00 (batch 52)
  ... step 54/93  loss=0.7741  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.750e+00 (batch 54)
  ... step 56/93  loss=0.5659  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.012e+00 (batch 56)
  ... step 58/93  loss=0.2799  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.687e+00 (batch 58)
  ... step 60/93  loss=0.3226  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.261e+00 (batch 60)
  ... step 62/93  loss=0.4503  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.738e+00 (batch 62)
  ... step 64/93  loss=0.7536  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.088e+01 (batch 64)
  ... step 66/93  loss=0.3225  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.402e+00 (batch 66)
  ... step 68/93  loss=0.2643  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.266e+00 (batch 68)
  ... step 70/93  loss=0.3499  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.800e+00 (batch 70)
  ... step 72/93  loss=1.0996  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.573e+01 (batch 72)
  ... step 74/93  loss=1.0875  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.241e+01 (batch 74)
  ... step 76/93  loss=0.3113  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.428e+00 (batch 76)
  ... step 78/93  loss=0.3657  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.521e+00 (batch 78)
  ... step 80/93  loss=0.2102  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.022e+00 (batch 80)
  ... step 82/93  loss=0.7564  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.433e+00 (batch 82)
  ... step 84/93  loss=0.1428  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.644e+00 (batch 84)
  ... step 86/93  loss=1.9295  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.154e+01 (batch 86)
  ... step 88/93  loss=0.3101  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.223e+00 (batch 88)
  ... step 90/93  loss=1.1788  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.528e+01 (batch 90)
  ... step 92/93  loss=0.5976  (0.3s since last print)
  â†˜ grad L2 norm â‰ˆ 2.687e+01 (batch 92)
âœ… epoch 13 forward/backward done in 26.4s
  ðŸ”Ž val step 0: batch (4, 32, 2, 324, 1, 16) loss=0.6079
[13] train=0.5507  val=0.6525  RMSE(std)=[Qi:0.785, Qe:0.852, Î“:0.785]  RMSE(phys)=[Qi:56.976, Qe:78.878, Î“:34.648]  (1.6s)
âœ… Saved unified NN â†’ ./mnt/data/bin.cgyro.nn
ðŸŸ¦ Starting epoch 14/30 (train steps â‰ˆ 93)
  ... step 0/93  loss=0.9422  (0.1s since last print)
  â†˜ grad L2 norm â‰ˆ 1.060e+01 (batch 0)
  ... step 2/93  loss=0.2817  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.462e+00 (batch 2)
  ... step 4/93  loss=0.8317  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.184e+01 (batch 4)
  ... step 6/93  loss=0.4889  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.021e+00 (batch 6)
  ... step 8/93  loss=0.6320  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.039e+01 (batch 8)
  ... step 10/93  loss=0.6840  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.290e+01 (batch 10)
  ... step 12/93  loss=0.1854  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.432e+00 (batch 12)
  ... step 14/93  loss=0.4679  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.964e+00 (batch 14)
  ... step 16/93  loss=1.1347  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.289e+01 (batch 16)
  ... step 18/93  loss=0.3596  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.740e+00 (batch 18)
  ... step 20/93  loss=0.8738  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.238e+01 (batch 20)
  ... step 22/93  loss=0.5313  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.850e+00 (batch 22)
  ... step 24/93  loss=0.2514  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.340e+00 (batch 24)
  ... step 26/93  loss=0.3465  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.752e+00 (batch 26)
  ... step 28/93  loss=0.4257  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.350e+00 (batch 28)
  ... step 30/93  loss=0.6646  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.258e+00 (batch 30)
  ... step 32/93  loss=0.1768  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.301e+00 (batch 32)
  ... step 34/93  loss=0.4465  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.200e+00 (batch 34)
  ... step 36/93  loss=0.4799  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.385e+00 (batch 36)
  ... step 38/93  loss=1.0395  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.409e+01 (batch 38)
  ... step 40/93  loss=0.3807  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.868e+00 (batch 40)
  ... step 42/93  loss=0.3446  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.362e+00 (batch 42)
  ... step 44/93  loss=0.4372  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.591e+00 (batch 44)
  ... step 46/93  loss=0.3959  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.908e+00 (batch 46)
  ... step 48/93  loss=0.3928  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.756e+00 (batch 48)
  ... step 50/93  loss=0.4748  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.198e+00 (batch 50)
  ... step 52/93  loss=0.5456  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.201e+01 (batch 52)
  ... step 54/93  loss=0.3273  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.036e+00 (batch 54)
  ... step 56/93  loss=0.3091  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.021e+00 (batch 56)
  ... step 58/93  loss=0.5400  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.630e+00 (batch 58)
  ... step 60/93  loss=0.4044  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.872e+00 (batch 60)
  ... step 62/93  loss=0.3416  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.483e+00 (batch 62)
  ... step 64/93  loss=0.5473  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.124e+00 (batch 64)
  ... step 66/93  loss=0.6963  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.521e+00 (batch 66)
  ... step 68/93  loss=0.3733  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.317e+00 (batch 68)
  ... step 70/93  loss=0.5701  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.501e+00 (batch 70)
  ... step 72/93  loss=0.4314  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.882e+00 (batch 72)
  ... step 74/93  loss=1.6743  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.750e+01 (batch 74)
  ... step 76/93  loss=0.6674  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.388e+01 (batch 76)
  ... step 78/93  loss=0.6462  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.026e+00 (batch 78)
  ... step 80/93  loss=0.5090  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.017e+01 (batch 80)
  ... step 82/93  loss=0.6912  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.184e+01 (batch 82)
  ... step 84/93  loss=0.3253  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.420e+00 (batch 84)
  ... step 86/93  loss=0.3876  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.421e+00 (batch 86)
  ... step 88/93  loss=0.3414  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.011e+00 (batch 88)
  ... step 90/93  loss=0.2372  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.938e+00 (batch 90)
  ... step 92/93  loss=1.2050  (0.3s since last print)
  â†˜ grad L2 norm â‰ˆ 4.315e+01 (batch 92)
âœ… epoch 14 forward/backward done in 26.4s
  ðŸ”Ž val step 0: batch (4, 32, 2, 324, 1, 16) loss=0.7449
[14] train=0.5244  val=0.7964  RMSE(std)=[Qi:0.862, Qe:0.943, Î“:0.870]  RMSE(phys)=[Qi:62.597, Qe:87.245, Î“:38.430]  (1.6s)
âœ… Saved unified NN â†’ ./mnt/data/bin.cgyro.nn
ðŸŸ¦ Starting epoch 15/30 (train steps â‰ˆ 93)
  ... step 0/93  loss=1.1341  (0.1s since last print)
  â†˜ grad L2 norm â‰ˆ 1.271e+01 (batch 0)
  ... step 2/93  loss=0.6996  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.079e+01 (batch 2)
  ... step 4/93  loss=0.6466  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.159e+00 (batch 4)
  ... step 6/93  loss=0.2098  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.496e+00 (batch 6)
  ... step 8/93  loss=0.5565  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.932e+00 (batch 8)
  ... step 10/93  loss=0.6283  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.146e+00 (batch 10)
  ... step 12/93  loss=0.7987  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.106e+00 (batch 12)
  ... step 14/93  loss=0.6723  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.185e+01 (batch 14)
  ... step 16/93  loss=0.7074  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.245e+00 (batch 16)
  ... step 18/93  loss=0.3885  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.850e+00 (batch 18)
  ... step 20/93  loss=0.5898  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.036e+00 (batch 20)
  ... step 22/93  loss=0.4509  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.671e+00 (batch 22)
  ... step 24/93  loss=0.7083  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.044e+01 (batch 24)
  ... step 26/93  loss=0.5634  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.628e+00 (batch 26)
  ... step 28/93  loss=0.3636  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.520e+00 (batch 28)
  ... step 30/93  loss=0.3089  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.958e+00 (batch 30)
  ... step 32/93  loss=0.3830  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.524e+00 (batch 32)
  ... step 34/93  loss=0.1890  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.020e+00 (batch 34)
  ... step 36/93  loss=0.7228  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.692e+00 (batch 36)
  ... step 38/93  loss=0.3973  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.885e+00 (batch 38)
  ... step 40/93  loss=0.5199  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.187e+01 (batch 40)
  ... step 42/93  loss=0.3747  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.997e+00 (batch 42)
  ... step 44/93  loss=0.3246  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.094e+00 (batch 44)
  ... step 46/93  loss=0.3121  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.768e+00 (batch 46)
  ... step 48/93  loss=0.4463  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.769e+00 (batch 48)
  ... step 50/93  loss=0.5171  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.425e+00 (batch 50)
  ... step 52/93  loss=0.4407  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.692e+00 (batch 52)
  ... step 54/93  loss=0.4460  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.683e+00 (batch 54)
  ... step 56/93  loss=0.5958  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.072e+01 (batch 56)
  ... step 58/93  loss=0.3130  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.146e+00 (batch 58)
  ... step 60/93  loss=0.1473  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.137e+00 (batch 60)
  ... step 62/93  loss=1.2411  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.112e+01 (batch 62)
  ... step 64/93  loss=1.0317  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.365e+01 (batch 64)
  ... step 66/93  loss=0.4405  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.573e+00 (batch 66)
  ... step 68/93  loss=0.6590  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.182e+00 (batch 68)
  ... step 70/93  loss=0.7494  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.679e+00 (batch 70)
  ... step 72/93  loss=0.9895  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.079e+01 (batch 72)
  ... step 74/93  loss=0.6860  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.864e+00 (batch 74)
  ... step 76/93  loss=0.2955  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.994e+00 (batch 76)
  ... step 78/93  loss=0.1489  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.709e+00 (batch 78)
  ... step 80/93  loss=0.5020  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.762e+00 (batch 80)
  ... step 82/93  loss=0.6416  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.379e+00 (batch 82)
  ... step 84/93  loss=0.4914  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.109e+01 (batch 84)
  ... step 86/93  loss=0.6673  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.422e+01 (batch 86)
  ... step 88/93  loss=0.7785  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.232e+01 (batch 88)
  ... step 90/93  loss=0.4240  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.974e+00 (batch 90)
  ... step 92/93  loss=0.1640  (0.3s since last print)
  â†˜ grad L2 norm â‰ˆ 1.101e+01 (batch 92)
âœ… epoch 15 forward/backward done in 26.4s
  ðŸ”Ž val step 0: batch (4, 32, 2, 324, 1, 16) loss=0.6868
[15] train=0.5257  val=0.6588  RMSE(std)=[Qi:0.785, Qe:0.862, Î“:0.785]  RMSE(phys)=[Qi:57.013, Qe:79.787, Î“:34.680]  (1.6s)
âœ… Saved unified NN â†’ ./mnt/data/bin.cgyro.nn
ðŸŸ¦ Starting epoch 16/30 (train steps â‰ˆ 93)
  ... step 0/93  loss=0.6732  (0.1s since last print)
  â†˜ grad L2 norm â‰ˆ 9.372e+00 (batch 0)
  ... step 2/93  loss=0.7773  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.716e+01 (batch 2)
  ... step 4/93  loss=0.7115  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.749e+00 (batch 4)
  ... step 6/93  loss=0.6142  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.217e+01 (batch 6)
  ... step 8/93  loss=0.3406  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.408e+00 (batch 8)
  ... step 10/93  loss=0.5576  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.085e+01 (batch 10)
  ... step 12/93  loss=0.4900  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.017e+00 (batch 12)
  ... step 14/93  loss=0.5812  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.783e+00 (batch 14)
  ... step 16/93  loss=0.1831  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.335e+00 (batch 16)
  ... step 18/93  loss=0.2086  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.487e+00 (batch 18)
  ... step 20/93  loss=0.2792  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.080e+00 (batch 20)
  ... step 22/93  loss=0.4220  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.231e+00 (batch 22)
  ... step 24/93  loss=0.7435  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.645e+00 (batch 24)
  ... step 26/93  loss=0.4215  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.263e+00 (batch 26)
  ... step 28/93  loss=0.6443  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.582e+00 (batch 28)
  ... step 30/93  loss=0.1076  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.763e+00 (batch 30)
  ... step 32/93  loss=0.2313  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.457e+00 (batch 32)
  ... step 34/93  loss=0.2641  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.337e+00 (batch 34)
  ... step 36/93  loss=0.4139  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.064e+00 (batch 36)
  ... step 38/93  loss=0.2903  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.155e+00 (batch 38)
  ... step 40/93  loss=0.5412  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.130e+00 (batch 40)
  ... step 42/93  loss=0.7262  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.738e+00 (batch 42)
  ... step 44/93  loss=0.1501  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.072e+00 (batch 44)
  ... step 46/93  loss=0.3893  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.998e+00 (batch 46)
  ... step 48/93  loss=0.5116  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.356e+00 (batch 48)
  ... step 50/93  loss=0.3532  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.845e+00 (batch 50)
  ... step 52/93  loss=0.2432  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.425e+00 (batch 52)
  ... step 54/93  loss=0.7004  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.179e+01 (batch 54)
  ... step 56/93  loss=0.3970  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.804e+00 (batch 56)
  ... step 58/93  loss=0.4141  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.697e+00 (batch 58)
  ... step 60/93  loss=0.3627  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.648e+00 (batch 60)
  ... step 62/93  loss=0.4663  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.933e+00 (batch 62)
  ... step 64/93  loss=0.4581  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.062e+00 (batch 64)
  ... step 66/93  loss=0.7491  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.165e+01 (batch 66)
  ... step 68/93  loss=0.6133  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.383e+00 (batch 68)
  ... step 70/93  loss=0.3987  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.772e+00 (batch 70)
  ... step 72/93  loss=0.6676  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.790e+00 (batch 72)
  ... step 74/93  loss=0.7202  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.385e+01 (batch 74)
  ... step 76/93  loss=0.4635  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.620e+00 (batch 76)
  ... step 78/93  loss=0.7140  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.779e+00 (batch 78)
  ... step 80/93  loss=0.5517  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.081e+01 (batch 80)
  ... step 82/93  loss=0.3493  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.287e+00 (batch 82)
  ... step 84/93  loss=1.1156  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.972e+01 (batch 84)
  ... step 86/93  loss=0.1558  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.836e+00 (batch 86)
  ... step 88/93  loss=0.2696  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.627e+00 (batch 88)
  ... step 90/93  loss=0.7283  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.206e+01 (batch 90)
  ... step 92/93  loss=1.4186  (0.3s since last print)
  â†˜ grad L2 norm â‰ˆ 2.502e+01 (batch 92)
âœ… epoch 16 forward/backward done in 26.4s
  ðŸ”Ž val step 0: batch (4, 32, 2, 324, 1, 16) loss=0.6850
[16] train=0.4819  val=0.7147  RMSE(std)=[Qi:0.819, Qe:0.894, Î“:0.821]  RMSE(phys)=[Qi:59.455, Qe:82.729, Î“:36.271]  (1.6s)
âœ… Saved unified NN â†’ ./mnt/data/bin.cgyro.nn
ðŸŸ¦ Starting epoch 17/30 (train steps â‰ˆ 93)
  ... step 0/93  loss=0.5451  (0.1s since last print)
  â†˜ grad L2 norm â‰ˆ 1.326e+01 (batch 0)
  ... step 2/93  loss=0.3751  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.219e+01 (batch 2)
  ... step 4/93  loss=0.2407  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.459e+00 (batch 4)
  ... step 6/93  loss=0.2106  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.097e+00 (batch 6)
  ... step 8/93  loss=1.1201  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.780e+01 (batch 8)
  ... step 10/93  loss=0.4144  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.806e+00 (batch 10)
  ... step 12/93  loss=1.1206  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.108e+01 (batch 12)
  ... step 14/93  loss=0.3894  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.401e+00 (batch 14)
  ... step 16/93  loss=0.6100  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.607e+00 (batch 16)
  ... step 18/93  loss=0.2513  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.505e+00 (batch 18)
  ... step 20/93  loss=0.4840  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.009e+01 (batch 20)
  ... step 22/93  loss=0.4889  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.086e+01 (batch 22)
  ... step 24/93  loss=0.6086  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.520e+00 (batch 24)
  ... step 26/93  loss=0.4563  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.239e+00 (batch 26)
  ... step 28/93  loss=0.3557  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.578e+00 (batch 28)
  ... step 30/93  loss=0.2060  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.970e+00 (batch 30)
  ... step 32/93  loss=0.6034  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.001e+01 (batch 32)
  ... step 34/93  loss=0.8604  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.000e+01 (batch 34)
  ... step 36/93  loss=0.6085  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.265e+01 (batch 36)
  ... step 38/93  loss=0.6452  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.060e+01 (batch 38)
  ... step 40/93  loss=0.1883  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.147e+00 (batch 40)
  ... step 42/93  loss=0.4103  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.748e+00 (batch 42)
  ... step 44/93  loss=0.2783  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.854e+00 (batch 44)
  ... step 46/93  loss=0.2883  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.925e+00 (batch 46)
  ... step 48/93  loss=0.3021  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.071e+00 (batch 48)
  ... step 50/93  loss=0.4893  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.803e+00 (batch 50)
  ... step 52/93  loss=0.1579  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.873e+00 (batch 52)
  ... step 54/93  loss=0.4535  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.667e+00 (batch 54)
  ... step 56/93  loss=0.9152  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.406e+01 (batch 56)
  ... step 58/93  loss=0.6740  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.852e+00 (batch 58)
  ... step 60/93  loss=0.5098  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.020e+00 (batch 60)
  ... step 62/93  loss=0.7592  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.222e+01 (batch 62)
  ... step 64/93  loss=0.6079  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.170e+01 (batch 64)
  ... step 66/93  loss=0.3066  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.259e+00 (batch 66)
  ... step 68/93  loss=0.2687  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.657e+00 (batch 68)
  ... step 70/93  loss=0.5374  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.791e+00 (batch 70)
  ... step 72/93  loss=0.6594  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.586e+00 (batch 72)
  ... step 74/93  loss=0.5334  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.853e+00 (batch 74)
  ... step 76/93  loss=0.5961  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.382e+00 (batch 76)
  ... step 78/93  loss=0.5448  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.275e+00 (batch 78)
  ... step 80/93  loss=0.1507  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.437e+00 (batch 80)
  ... step 82/93  loss=0.6949  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.341e+01 (batch 82)
  ... step 84/93  loss=0.2806  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.422e+00 (batch 84)
  ... step 86/93  loss=0.5054  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.897e+00 (batch 86)
  ... step 88/93  loss=0.4855  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.148e+00 (batch 88)
  ... step 90/93  loss=0.3670  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.312e+01 (batch 90)
  ... step 92/93  loss=1.1923  (0.3s since last print)
  â†˜ grad L2 norm â‰ˆ 4.242e+01 (batch 92)
âœ… epoch 17 forward/backward done in 26.4s
  ðŸ”Ž val step 0: batch (4, 32, 2, 324, 1, 16) loss=0.5582
[17] train=0.4939  val=0.8137  RMSE(std)=[Qi:0.881, Qe:0.938, Î“:0.886]  RMSE(phys)=[Qi:63.995, Qe:86.844, Î“:39.103]  (1.6s)
âœ… Saved unified NN â†’ ./mnt/data/bin.cgyro.nn
ðŸŸ¦ Starting epoch 18/30 (train steps â‰ˆ 93)
  ... step 0/93  loss=0.4133  (0.1s since last print)
  â†˜ grad L2 norm â‰ˆ 5.560e+00 (batch 0)
  ... step 2/93  loss=0.3406  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.431e+00 (batch 2)
  ... step 4/93  loss=0.2495  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.805e+00 (batch 4)
  ... step 6/93  loss=0.1423  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.558e+00 (batch 6)
  ... step 8/93  loss=0.9935  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.329e+01 (batch 8)
  ... step 10/93  loss=0.4733  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.949e+00 (batch 10)
  ... step 12/93  loss=0.7014  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.723e+00 (batch 12)
  ... step 14/93  loss=0.5838  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.117e+01 (batch 14)
  ... step 16/93  loss=1.2634  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.373e+01 (batch 16)
  ... step 18/93  loss=0.2066  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.093e+00 (batch 18)
  ... step 20/93  loss=0.1868  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.162e+00 (batch 20)
  ... step 22/93  loss=0.2000  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.759e+00 (batch 22)
  ... step 24/93  loss=0.3507  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.630e+00 (batch 24)
  ... step 26/93  loss=0.5592  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.205e+00 (batch 26)
  ... step 28/93  loss=0.4708  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.547e+00 (batch 28)
  ... step 30/93  loss=0.2589  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.097e+00 (batch 30)
  ... step 32/93  loss=0.5119  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.781e+00 (batch 32)
  ... step 34/93  loss=0.7989  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.138e+00 (batch 34)
  ... step 36/93  loss=0.5287  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.530e+00 (batch 36)
  ... step 38/93  loss=0.4097  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.909e+00 (batch 38)
  ... step 40/93  loss=0.3820  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.999e+00 (batch 40)
  ... step 42/93  loss=0.3786  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.642e+00 (batch 42)
  ... step 44/93  loss=0.9484  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.436e+00 (batch 44)
  ... step 46/93  loss=0.3415  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.910e+00 (batch 46)
  ... step 48/93  loss=0.1050  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.494e+00 (batch 48)
  ... step 50/93  loss=0.2546  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.099e+00 (batch 50)
  ... step 52/93  loss=0.2768  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.147e+00 (batch 52)
  ... step 54/93  loss=0.2972  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.179e+01 (batch 54)
  ... step 56/93  loss=0.6068  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.040e+01 (batch 56)
  ... step 58/93  loss=0.1099  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.087e+00 (batch 58)
  ... step 60/93  loss=0.3614  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.251e+00 (batch 60)
  ... step 62/93  loss=0.1695  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.897e+00 (batch 62)
  ... step 64/93  loss=0.4969  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.446e+00 (batch 64)
  ... step 66/93  loss=0.3023  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.996e+00 (batch 66)
  ... step 68/93  loss=0.4121  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.929e+00 (batch 68)
  ... step 70/93  loss=0.6092  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.080e+01 (batch 70)
  ... step 72/93  loss=0.4691  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.925e+00 (batch 72)
  ... step 74/93  loss=0.7426  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.324e+01 (batch 74)
  ... step 76/93  loss=0.6670  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.005e+01 (batch 76)
  ... step 78/93  loss=0.3961  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.620e+00 (batch 78)
  ... step 80/93  loss=0.2782  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.856e+00 (batch 80)
  ... step 82/93  loss=0.5142  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.434e+00 (batch 82)
  ... step 84/93  loss=0.3342  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.345e+00 (batch 84)
  ... step 86/93  loss=0.1626  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.178e+00 (batch 86)
  ... step 88/93  loss=0.6486  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.823e+00 (batch 88)
  ... step 90/93  loss=0.3744  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.260e+00 (batch 90)
  ... step 92/93  loss=0.1689  (0.3s since last print)
  â†˜ grad L2 norm â‰ˆ 2.746e+01 (batch 92)
âœ… epoch 18 forward/backward done in 26.4s
  ðŸ”Ž val step 0: batch (4, 32, 2, 324, 1, 16) loss=0.5382
[18] train=0.4488  val=0.8392  RMSE(std)=[Qi:0.893, Qe:0.939, Î“:0.916]  RMSE(phys)=[Qi:64.859, Qe:86.898, Î“:40.432]  (1.6s)
âœ… Saved unified NN â†’ ./mnt/data/bin.cgyro.nn
ðŸŸ¦ Starting epoch 19/30 (train steps â‰ˆ 93)
  ... step 0/93  loss=0.3259  (0.1s since last print)
  â†˜ grad L2 norm â‰ˆ 5.006e+00 (batch 0)
  ... step 2/93  loss=0.4370  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.653e+00 (batch 2)
  ... step 4/93  loss=0.9970  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.226e+01 (batch 4)
  ... step 6/93  loss=0.3220  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.922e+00 (batch 6)
  ... step 8/93  loss=0.4590  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.110e+00 (batch 8)
  ... step 10/93  loss=0.2624  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.165e+00 (batch 10)
  ... step 12/93  loss=0.3937  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.733e+00 (batch 12)
  ... step 14/93  loss=0.3749  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.147e+00 (batch 14)
  ... step 16/93  loss=0.4535  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.303e+00 (batch 16)
  ... step 18/93  loss=0.2250  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.552e+00 (batch 18)
  ... step 20/93  loss=0.4257  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.127e+00 (batch 20)
  ... step 22/93  loss=0.7546  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.338e+01 (batch 22)
  ... step 24/93  loss=0.4046  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.095e+01 (batch 24)
  ... step 26/93  loss=0.4366  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.613e+00 (batch 26)
  ... step 28/93  loss=0.4004  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.080e+01 (batch 28)
  ... step 30/93  loss=0.3297  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.853e+00 (batch 30)
  ... step 32/93  loss=0.3100  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.333e+00 (batch 32)
  ... step 34/93  loss=0.2482  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.422e+00 (batch 34)
  ... step 36/93  loss=0.5680  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.025e+01 (batch 36)
  ... step 38/93  loss=0.7857  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.056e+01 (batch 38)
  ... step 40/93  loss=0.2273  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.317e+00 (batch 40)
  ... step 42/93  loss=0.2772  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.481e+00 (batch 42)
  ... step 44/93  loss=0.4611  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.345e+00 (batch 44)
  ... step 46/93  loss=0.8457  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.047e+01 (batch 46)
  ... step 48/93  loss=0.4469  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.468e+00 (batch 48)
  ... step 50/93  loss=0.5907  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.280e+00 (batch 50)
  ... step 52/93  loss=0.6066  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.379e+00 (batch 52)
  ... step 54/93  loss=0.3299  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.249e+00 (batch 54)
  ... step 56/93  loss=0.3949  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.997e+00 (batch 56)
  ... step 58/93  loss=0.2507  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.699e+00 (batch 58)
  ... step 60/93  loss=0.3056  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.802e+00 (batch 60)
  ... step 62/93  loss=0.4305  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.265e+01 (batch 62)
  ... step 64/93  loss=0.5529  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.491e+00 (batch 64)
  ... step 66/93  loss=0.6805  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.108e+01 (batch 66)
  ... step 68/93  loss=0.3498  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.073e+00 (batch 68)
  ... step 70/93  loss=0.2784  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.234e+00 (batch 70)
  ... step 72/93  loss=0.7702  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.135e+01 (batch 72)
  ... step 74/93  loss=0.7214  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.249e+00 (batch 74)
  ... step 76/93  loss=0.3627  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.897e+00 (batch 76)
  ... step 78/93  loss=0.2908  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.574e+00 (batch 78)
  ... step 80/93  loss=0.3238  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.898e+00 (batch 80)
  ... step 82/93  loss=1.5941  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.184e+01 (batch 82)
  ... step 84/93  loss=0.2819  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.011e+00 (batch 84)
  ... step 86/93  loss=0.2249  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.634e+00 (batch 86)
  ... step 88/93  loss=0.5516  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.531e+00 (batch 88)
  ... step 90/93  loss=0.1943  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.398e+00 (batch 90)
  ... step 92/93  loss=0.6986  (0.3s since last print)
  â†˜ grad L2 norm â‰ˆ 2.813e+01 (batch 92)
âœ… epoch 19 forward/backward done in 26.4s
  ðŸ”Ž val step 0: batch (4, 32, 2, 324, 1, 16) loss=0.4282
[19] train=0.4326  val=0.6903  RMSE(std)=[Qi:0.813, Qe:0.865, Î“:0.813]  RMSE(phys)=[Qi:59.050, Qe:80.096, Î“:35.893]  (1.6s)
âœ… Saved unified NN â†’ ./mnt/data/bin.cgyro.nn
ðŸŸ¦ Starting epoch 20/30 (train steps â‰ˆ 93)
  ... step 0/93  loss=0.3882  (0.1s since last print)
  â†˜ grad L2 norm â‰ˆ 7.957e+00 (batch 0)
  ... step 2/93  loss=0.3880  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.889e+00 (batch 2)
  ... step 4/93  loss=0.2848  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.552e+00 (batch 4)
  ... step 6/93  loss=0.4149  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.458e+00 (batch 6)
  ... step 8/93  loss=0.2972  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.531e+00 (batch 8)
  ... step 10/93  loss=0.9119  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.991e+00 (batch 10)
  ... step 12/93  loss=0.2477  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.970e+00 (batch 12)
  ... step 14/93  loss=0.8090  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.715e+01 (batch 14)
  ... step 16/93  loss=0.3630  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.603e+00 (batch 16)
  ... step 18/93  loss=0.4096  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.157e+01 (batch 18)
  ... step 20/93  loss=0.4520  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.581e+00 (batch 20)
  ... step 22/93  loss=0.5343  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.550e+00 (batch 22)
  ... step 24/93  loss=0.9074  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.091e+01 (batch 24)
  ... step 26/93  loss=1.2123  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.054e+01 (batch 26)
  ... step 28/93  loss=0.1298  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.569e+00 (batch 28)
  ... step 30/93  loss=0.4207  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.967e+00 (batch 30)
  ... step 32/93  loss=0.5568  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.973e+00 (batch 32)
  ... step 34/93  loss=0.2958  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.871e+00 (batch 34)
  ... step 36/93  loss=0.2370  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.446e+00 (batch 36)
  ... step 38/93  loss=0.2464  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.942e+00 (batch 38)
  ... step 40/93  loss=0.1769  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.834e+00 (batch 40)
  ... step 42/93  loss=0.3785  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.074e+01 (batch 42)
  ... step 44/93  loss=0.2863  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.281e+00 (batch 44)
  ... step 46/93  loss=0.1467  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.604e+00 (batch 46)
  ... step 48/93  loss=0.3802  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.227e+01 (batch 48)
  ... step 50/93  loss=0.9341  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.508e+01 (batch 50)
  ... step 52/93  loss=0.2084  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.465e+00 (batch 52)
  ... step 54/93  loss=0.2388  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.310e+00 (batch 54)
  ... step 56/93  loss=0.4658  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.531e+00 (batch 56)
  ... step 58/93  loss=0.4405  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.014e+00 (batch 58)
  ... step 60/93  loss=0.5117  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.030e+00 (batch 60)
  ... step 62/93  loss=0.7047  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.694e+00 (batch 62)
  ... step 64/93  loss=0.4891  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.833e+00 (batch 64)
  ... step 66/93  loss=0.6188  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.249e+00 (batch 66)
  ... step 68/93  loss=0.2295  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.292e+00 (batch 68)
  ... step 70/93  loss=0.3926  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.710e+00 (batch 70)
  ... step 72/93  loss=0.3744  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.133e+01 (batch 72)
  ... step 74/93  loss=0.5421  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.801e+00 (batch 74)
  ... step 76/93  loss=0.4127  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.230e+00 (batch 76)
  ... step 78/93  loss=0.4481  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.094e+01 (batch 78)
  ... step 80/93  loss=0.6094  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.255e+00 (batch 80)
  ... step 82/93  loss=0.3820  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.674e+00 (batch 82)
  ... step 84/93  loss=0.2891  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.307e+00 (batch 84)
  ... step 86/93  loss=0.4969  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.942e+00 (batch 86)
  ... step 88/93  loss=0.0585  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.827e+00 (batch 88)
  ... step 90/93  loss=0.3548  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.242e+00 (batch 90)
  ... step 92/93  loss=0.0492  (0.3s since last print)
  â†˜ grad L2 norm â‰ˆ 1.149e+01 (batch 92)
âœ… epoch 20 forward/backward done in 26.4s
  ðŸ”Ž val step 0: batch (4, 32, 2, 324, 1, 16) loss=0.4739
[20] train=0.4257  val=0.8359  RMSE(std)=[Qi:0.890, Qe:0.950, Î“:0.902]  RMSE(phys)=[Qi:64.609, Qe:87.910, Î“:39.837]  (1.6s)
âœ… Saved unified NN â†’ ./mnt/data/bin.cgyro.nn
ðŸŸ¦ Starting epoch 21/30 (train steps â‰ˆ 93)
  ... step 0/93  loss=0.4162  (0.1s since last print)
  â†˜ grad L2 norm â‰ˆ 7.609e+00 (batch 0)
  ... step 2/93  loss=0.5456  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.445e+00 (batch 2)
  ... step 4/93  loss=0.3851  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.054e+00 (batch 4)
  ... step 6/93  loss=0.4596  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.373e+01 (batch 6)
  ... step 8/93  loss=0.6696  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.117e+01 (batch 8)
  ... step 10/93  loss=0.2423  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.367e+00 (batch 10)
  ... step 12/93  loss=0.5176  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.219e+01 (batch 12)
  ... step 14/93  loss=0.5984  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.871e+00 (batch 14)
  ... step 16/93  loss=0.1525  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.958e+00 (batch 16)
  ... step 18/93  loss=0.2579  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.554e+00 (batch 18)
  ... step 20/93  loss=0.3639  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.856e+00 (batch 20)
  ... step 22/93  loss=0.3303  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.714e+00 (batch 22)
  ... step 24/93  loss=0.4808  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.768e+00 (batch 24)
  ... step 26/93  loss=0.6986  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.472e+01 (batch 26)
  ... step 28/93  loss=0.2677  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.202e+00 (batch 28)
  ... step 30/93  loss=0.3049  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.790e+00 (batch 30)
  ... step 32/93  loss=0.4247  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.257e+00 (batch 32)
  ... step 34/93  loss=0.3831  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.804e+00 (batch 34)
  ... step 36/93  loss=0.3726  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.849e+00 (batch 36)
  ... step 38/93  loss=0.5055  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.504e+00 (batch 38)
  ... step 40/93  loss=0.4125  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.830e+00 (batch 40)
  ... step 42/93  loss=0.3382  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.623e+00 (batch 42)
  ... step 44/93  loss=0.5728  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.888e+00 (batch 44)
  ... step 46/93  loss=0.3843  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.682e+00 (batch 46)
  ... step 48/93  loss=0.3307  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.777e+00 (batch 48)
  ... step 50/93  loss=0.4636  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.074e+01 (batch 50)
  ... step 52/93  loss=0.1028  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.922e+00 (batch 52)
  ... step 54/93  loss=0.4900  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.245e+01 (batch 54)
  ... step 56/93  loss=0.2234  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.808e+00 (batch 56)
  ... step 58/93  loss=0.2602  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.111e+00 (batch 58)
  ... step 60/93  loss=0.6070  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.427e+01 (batch 60)
  ... step 62/93  loss=0.3137  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.322e+00 (batch 62)
  ... step 64/93  loss=0.6565  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.194e+01 (batch 64)
  ... step 66/93  loss=0.8419  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.536e+01 (batch 66)
  ... step 68/93  loss=0.8055  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.321e+01 (batch 68)
  ... step 70/93  loss=0.7034  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.087e+01 (batch 70)
  ... step 72/93  loss=0.1492  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.962e+00 (batch 72)
  ... step 74/93  loss=0.8190  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.340e+01 (batch 74)
  ... step 76/93  loss=0.6234  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.530e+01 (batch 76)
  ... step 78/93  loss=0.2086  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.037e+00 (batch 78)
  ... step 80/93  loss=0.5338  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.026e+00 (batch 80)
  ... step 82/93  loss=0.2668  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.860e+00 (batch 82)
  ... step 84/93  loss=0.4430  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.218e+01 (batch 84)
  ... step 86/93  loss=0.2001  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.737e+00 (batch 86)
  ... step 88/93  loss=0.5202  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.471e+00 (batch 88)
  ... step 90/93  loss=0.4620  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.139e+00 (batch 90)
  ... step 92/93  loss=1.5559  (0.3s since last print)
  â†˜ grad L2 norm â‰ˆ 2.761e+01 (batch 92)
âœ… epoch 21 forward/backward done in 26.4s
  ðŸ”Ž val step 0: batch (4, 32, 2, 324, 1, 16) loss=0.4521
[21] train=0.4201  val=0.8358  RMSE(std)=[Qi:0.889, Qe:0.957, Î“:0.895]  RMSE(phys)=[Qi:64.562, Qe:88.560, Î“:39.533]  (1.6s)
âœ… Saved unified NN â†’ ./mnt/data/bin.cgyro.nn
ðŸŸ¦ Starting epoch 22/30 (train steps â‰ˆ 93)
  ... step 0/93  loss=0.8494  (0.1s since last print)
  â†˜ grad L2 norm â‰ˆ 1.197e+01 (batch 0)
  ... step 2/93  loss=0.4272  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.100e+00 (batch 2)
  ... step 4/93  loss=0.0945  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.501e+00 (batch 4)
  ... step 6/93  loss=0.6892  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.134e+00 (batch 6)
  ... step 8/93  loss=0.1602  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.158e+00 (batch 8)
  ... step 10/93  loss=0.2657  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.508e+00 (batch 10)
  ... step 12/93  loss=0.1649  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.614e+00 (batch 12)
  ... step 14/93  loss=0.2407  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.777e+00 (batch 14)
  ... step 16/93  loss=0.4336  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.703e+00 (batch 16)
  ... step 18/93  loss=0.3710  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.004e+00 (batch 18)
  ... step 20/93  loss=0.7411  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.078e+01 (batch 20)
  ... step 22/93  loss=0.2732  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.020e+00 (batch 22)
  ... step 24/93  loss=0.2151  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.721e+00 (batch 24)
  ... step 26/93  loss=0.2978  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.876e+00 (batch 26)
  ... step 28/93  loss=0.3577  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.947e+00 (batch 28)
  ... step 30/93  loss=0.2867  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.289e+00 (batch 30)
  ... step 32/93  loss=0.1656  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.256e+00 (batch 32)
  ... step 34/93  loss=0.1650  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.598e+00 (batch 34)
  ... step 36/93  loss=0.2831  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.165e+00 (batch 36)
  ... step 38/93  loss=0.3829  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.656e+00 (batch 38)
  ... step 40/93  loss=0.3165  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.807e+00 (batch 40)
  ... step 42/93  loss=0.4262  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.039e+01 (batch 42)
  ... step 44/93  loss=0.1279  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.742e+00 (batch 44)
  ... step 46/93  loss=0.2293  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.750e+00 (batch 46)
  ... step 48/93  loss=0.2440  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.967e+00 (batch 48)
  ... step 50/93  loss=0.2231  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.177e+00 (batch 50)
  ... step 52/93  loss=0.1979  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.825e+00 (batch 52)
  ... step 54/93  loss=0.8979  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.257e+01 (batch 54)
  ... step 56/93  loss=0.6181  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.445e+01 (batch 56)
  ... step 58/93  loss=0.1259  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.268e+00 (batch 58)
  ... step 60/93  loss=0.2798  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.540e+00 (batch 60)
  ... step 62/93  loss=0.3553  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.361e+00 (batch 62)
  ... step 64/93  loss=0.3068  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.974e+00 (batch 64)
  ... step 66/93  loss=0.6914  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.910e+00 (batch 66)
  ... step 68/93  loss=0.3786  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.199e+00 (batch 68)
  ... step 70/93  loss=0.3017  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.907e+00 (batch 70)
  ... step 72/93  loss=0.2122  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.851e+00 (batch 72)
  ... step 74/93  loss=0.6720  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.720e+00 (batch 74)
  ... step 76/93  loss=0.2740  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.212e+00 (batch 76)
  ... step 78/93  loss=0.6647  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.293e+01 (batch 78)
  ... step 80/93  loss=0.3795  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.745e+00 (batch 80)
  ... step 82/93  loss=0.6148  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.245e+00 (batch 82)
  ... step 84/93  loss=0.1059  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.703e+00 (batch 84)
  ... step 86/93  loss=0.3874  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.992e+00 (batch 86)
  ... step 88/93  loss=0.3428  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.253e+01 (batch 88)
  ... step 90/93  loss=0.3521  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.792e+00 (batch 90)
  ... step 92/93  loss=0.9658  (0.3s since last print)
  â†˜ grad L2 norm â‰ˆ 3.720e+01 (batch 92)
âœ… epoch 22 forward/backward done in 26.4s
  ðŸ”Ž val step 0: batch (4, 32, 2, 324, 1, 16) loss=0.4580
[22] train=0.3802  val=0.7931  RMSE(std)=[Qi:0.875, Qe:0.918, Î“:0.878]  RMSE(phys)=[Qi:63.542, Qe:84.920, Î“:38.792]  (1.6s)
âœ… Saved unified NN â†’ ./mnt/data/bin.cgyro.nn
ðŸŸ¦ Starting epoch 23/30 (train steps â‰ˆ 93)
  ... step 0/93  loss=0.2971  (0.1s since last print)
  â†˜ grad L2 norm â‰ˆ 6.112e+00 (batch 0)
  ... step 2/93  loss=0.8192  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.185e+01 (batch 2)
  ... step 4/93  loss=0.2734  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.541e+00 (batch 4)
  ... step 6/93  loss=0.2345  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.877e+00 (batch 6)
  ... step 8/93  loss=0.1676  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.121e+00 (batch 8)
  ... step 10/93  loss=1.1885  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.485e+01 (batch 10)
  ... step 12/93  loss=0.2058  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.100e+00 (batch 12)
  ... step 14/93  loss=0.5008  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.142e+01 (batch 14)
  ... step 16/93  loss=0.4836  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.136e+00 (batch 16)
  ... step 18/93  loss=0.2624  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.028e+00 (batch 18)
  ... step 20/93  loss=0.1335  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.117e+00 (batch 20)
  ... step 22/93  loss=0.1292  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.009e+00 (batch 22)
  ... step 24/93  loss=0.2596  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.045e+01 (batch 24)
  ... step 26/93  loss=0.1387  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.114e+00 (batch 26)
  ... step 28/93  loss=0.2184  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.778e+00 (batch 28)
  ... step 30/93  loss=0.2942  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.316e+00 (batch 30)
  ... step 32/93  loss=0.2921  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.911e+00 (batch 32)
  ... step 34/93  loss=0.3419  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.886e+00 (batch 34)
  ... step 36/93  loss=0.5980  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.158e+01 (batch 36)
  ... step 38/93  loss=0.2943  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.452e+00 (batch 38)
  ... step 40/93  loss=0.1876  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.849e+00 (batch 40)
  ... step 42/93  loss=0.3083  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.369e+00 (batch 42)
  ... step 44/93  loss=0.4133  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.134e+00 (batch 44)
  ... step 46/93  loss=0.6031  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.169e+01 (batch 46)
  ... step 48/93  loss=0.6013  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.425e+01 (batch 48)
  ... step 50/93  loss=0.3070  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.993e+00 (batch 50)
  ... step 52/93  loss=0.7598  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.447e+01 (batch 52)
  ... step 54/93  loss=0.1426  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.045e+00 (batch 54)
  ... step 56/93  loss=0.4983  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.770e+00 (batch 56)
  ... step 58/93  loss=0.5981  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.950e+00 (batch 58)
  ... step 60/93  loss=0.3855  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.878e+00 (batch 60)
  ... step 62/93  loss=0.3761  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.456e+01 (batch 62)
  ... step 64/93  loss=0.7466  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.053e+01 (batch 64)
  ... step 66/93  loss=0.3837  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.880e+00 (batch 66)
  ... step 68/93  loss=0.4170  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.326e+00 (batch 68)
  ... step 70/93  loss=0.1556  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.008e+00 (batch 70)
  ... step 72/93  loss=0.4168  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.176e+00 (batch 72)
  ... step 74/93  loss=0.7262  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.858e+00 (batch 74)
  ... step 76/93  loss=0.3548  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.542e+00 (batch 76)
  ... step 78/93  loss=0.2652  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.528e+00 (batch 78)
  ... step 80/93  loss=0.1577  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.227e+00 (batch 80)
  ... step 82/93  loss=0.1707  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.592e+00 (batch 82)
  ... step 84/93  loss=0.1224  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.079e+00 (batch 84)
  ... step 86/93  loss=0.3939  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.777e+00 (batch 86)
  ... step 88/93  loss=0.3760  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.607e+00 (batch 88)
  ... step 90/93  loss=0.4912  (0.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.016e+01 (batch 90)
  ... step 92/93  loss=3.2223  (0.3s since last print)
  â†˜ grad L2 norm â‰ˆ 6.405e+01 (batch 92)
âœ… epoch 23 forward/backward done in 26.4s
  ðŸ”Ž val step 0: batch (4, 32, 2, 324, 1, 16) loss=1.5973
[23] train=0.3791  val=0.8052  RMSE(std)=[Qi:0.876, Qe:0.931, Î“:0.884]  RMSE(phys)=[Qi:63.648, Qe:86.167, Î“:39.018]  (1.6s)
âœ… Saved unified NN â†’ ./mnt/data/bin.cgyro.nn
â¹ Early stopping after 23 epochs (no val improvement).
Saved metrics â†’ ./mnt/data/myrun_logs_ot2/metrics.csv
âœ… Final unified NN â†’ ./mnt/data/bin.cgyro.nn
âœ… Training complete. Artifacts in: ./mnt/data/myrun_logs_ot2
