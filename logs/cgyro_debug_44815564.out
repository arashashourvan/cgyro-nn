[build_datasets] windows: total=783  train=469  val=156  test=158

[train] dataset:
  y_mean: [[[139.28694 210.00957  82.88394]]]
  y_std : [[[ 81.20469 104.45502  49.48185]]]
  sample Y: min=-1.652e+00, max=2.854e+00, mean=-1.949e-02, std=9.796e-01

[val] dataset:
  y_mean: [[[139.28694 210.00957  82.88394]]]
  y_std : [[[ 81.20469 104.45502  49.48185]]]
  sample Y: min=-1.618e+00, max=2.854e+00, mean=5.182e-03, std=1.023e+00

[test] dataset:
  y_mean: [[[139.28694 210.00957  82.88394]]]
  y_std : [[[ 81.20469 104.45502  49.48185]]]
  sample Y: min=-1.676e+00, max=2.660e+00, mean=-8.993e-02, std=1.021e+00
âœ… Saved flux histograms in ./mnt/data/myrun_logs_deep_debug
ðŸŸ¦ Starting epoch 1/40 (train steps â‰ˆ 59)
[90429] Î¦2FluxDeep forward: input (8, 64, 2, 324, 1, 16)
  ... step 0/59  loss=1.7197  (3.6s since last print)
  â†˜ grad L2 norm â‰ˆ 1.456e+01 (batch 0)
  ... step 2/59  loss=1.3948  (1.5s since last print)
  â†˜ grad L2 norm â‰ˆ 1.589e+01 (batch 2)
  ... step 4/59  loss=1.1944  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.056e+01 (batch 4)
  ... step 6/59  loss=0.9197  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.905e+00 (batch 6)
  ... step 8/59  loss=0.9633  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.834e+00 (batch 8)
  ... step 10/59  loss=1.0945  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.843e+00 (batch 10)
  ... step 12/59  loss=0.9116  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.258e+00 (batch 12)
  ... step 14/59  loss=0.8112  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.985e+00 (batch 14)
  ... step 16/59  loss=0.9450  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.161e+00 (batch 16)
  ... step 18/59  loss=1.3419  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.268e+01 (batch 18)
  ... step 20/59  loss=1.7172  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.470e+01 (batch 20)
  ... step 22/59  loss=0.3709  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.879e+00 (batch 22)
  ... step 24/59  loss=1.2164  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.245e+00 (batch 24)
  ... step 26/59  loss=0.7542  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.235e+00 (batch 26)
  ... step 28/59  loss=0.8581  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.974e+00 (batch 28)
  ... step 30/59  loss=1.5953  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.784e+00 (batch 30)
  ... step 32/59  loss=0.7428  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.148e+00 (batch 32)
  ... step 34/59  loss=0.4204  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.245e+00 (batch 34)
  ... step 36/59  loss=1.1621  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.235e+00 (batch 36)
  ... step 38/59  loss=0.9361  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.014e+01 (batch 38)
  ... step 40/59  loss=1.1437  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.592e+00 (batch 40)
  ... step 42/59  loss=0.5245  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.304e+00 (batch 42)
  ... step 44/59  loss=0.5293  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.555e+00 (batch 44)
  ... step 46/59  loss=1.5671  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.096e+00 (batch 46)
  ... step 48/59  loss=0.7577  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.027e+01 (batch 48)
  ... step 50/59  loss=1.3535  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.029e+01 (batch 50)
  ... step 52/59  loss=1.2132  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.324e+01 (batch 52)
  ... step 54/59  loss=0.4460  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.062e+00 (batch 54)
  ... step 56/59  loss=1.3883  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.110e+00 (batch 56)
  ... step 58/59  loss=0.4750  (1.3s since last print)
  â†˜ grad L2 norm â‰ˆ 4.564e+00 (batch 58)
âœ… epoch 1 forward/backward done in 69.1s
  ðŸ”Ž val step 0: batch (8, 64, 2, 324, 1, 16) loss=1.4591
[01] train=0.9734  val=1.1799  RMSE(std)=[Qi:1.090, Qe:1.088, Î“:1.080]  RMSE(phys)=[Qi:88.520, Qe:113.669, Î“:53.462]  (4.8s)
âœ… Saved unified NN â†’ ./mnt/data/bin.cgyro.nn
ðŸŸ¦ Starting epoch 2/40 (train steps â‰ˆ 59)
  ... step 0/59  loss=1.2645  (0.3s since last print)
  â†˜ grad L2 norm â‰ˆ 6.654e+00 (batch 0)
  ... step 2/59  loss=0.7743  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.612e+00 (batch 2)
  ... step 4/59  loss=0.5967  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.661e+00 (batch 4)
  ... step 6/59  loss=1.1069  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.199e+00 (batch 6)
  ... step 8/59  loss=0.9858  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.713e+00 (batch 8)
  ... step 10/59  loss=1.3305  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.811e+00 (batch 10)
  ... step 12/59  loss=0.6237  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.507e+00 (batch 12)
  ... step 14/59  loss=0.4979  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.949e+00 (batch 14)
  ... step 16/59  loss=1.0192  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.887e+00 (batch 16)
  ... step 18/59  loss=1.0273  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 9.114e+00 (batch 18)
  ... step 20/59  loss=0.3647  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.239e+00 (batch 20)
  ... step 22/59  loss=1.0602  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.918e+00 (batch 22)
  ... step 24/59  loss=0.5590  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.008e+00 (batch 24)
  ... step 26/59  loss=0.6282  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.849e+00 (batch 26)
  ... step 28/59  loss=0.3633  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.817e+00 (batch 28)
  ... step 30/59  loss=0.8230  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.769e+00 (batch 30)
  ... step 32/59  loss=0.6618  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.974e+00 (batch 32)
  ... step 34/59  loss=0.9769  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.080e+01 (batch 34)
  ... step 36/59  loss=1.3656  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.559e+00 (batch 36)
  ... step 38/59  loss=0.6520  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.949e+00 (batch 38)
  ... step 40/59  loss=0.9508  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.984e+00 (batch 40)
  ... step 42/59  loss=0.7523  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.698e+00 (batch 42)
  ... step 44/59  loss=1.2772  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.850e+00 (batch 44)
  ... step 46/59  loss=0.6462  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.638e+00 (batch 46)
  ... step 48/59  loss=0.4167  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.963e+00 (batch 48)
  ... step 50/59  loss=0.6337  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.945e+00 (batch 50)
  ... step 52/59  loss=0.7675  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.577e+00 (batch 52)
  ... step 54/59  loss=0.5142  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.692e+00 (batch 54)
  ... step 56/59  loss=0.7805  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.138e+00 (batch 56)
  ... step 58/59  loss=0.4157  (1.3s since last print)
  â†˜ grad L2 norm â‰ˆ 7.236e+00 (batch 58)
âœ… epoch 2 forward/backward done in 66.5s
  ðŸ”Ž val step 0: batch (8, 64, 2, 324, 1, 16) loss=10.7674
[02] train=0.7565  val=9.2953  RMSE(std)=[Qi:3.270, Qe:3.003, Î“:2.860]  RMSE(phys)=[Qi:265.500, Qe:313.658, Î“:141.516]  (4.2s)
âœ… Saved unified NN â†’ ./mnt/data/bin.cgyro.nn
ðŸŸ¦ Starting epoch 3/40 (train steps â‰ˆ 59)
  ... step 0/59  loss=0.9042  (0.3s since last print)
  â†˜ grad L2 norm â‰ˆ 8.099e+00 (batch 0)
  ... step 2/59  loss=0.7636  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.819e+00 (batch 2)
  ... step 4/59  loss=0.4910  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.931e+00 (batch 4)
  ... step 6/59  loss=0.6657  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.031e+00 (batch 6)
  ... step 8/59  loss=0.9479  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.746e+00 (batch 8)
  ... step 10/59  loss=0.9651  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.460e+00 (batch 10)
  ... step 12/59  loss=1.0378  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.025e+01 (batch 12)
  ... step 14/59  loss=1.0758  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.558e+00 (batch 14)
  ... step 16/59  loss=0.5146  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.712e+00 (batch 16)
  ... step 18/59  loss=0.5982  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.890e+00 (batch 18)
  ... step 20/59  loss=0.4716  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.894e+00 (batch 20)
  ... step 22/59  loss=0.4072  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.724e+00 (batch 22)
  ... step 24/59  loss=0.5377  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.685e+00 (batch 24)
  ... step 26/59  loss=0.7746  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.816e+00 (batch 26)
  ... step 28/59  loss=0.2929  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.319e+00 (batch 28)
  ... step 30/59  loss=0.5786  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.737e+00 (batch 30)
  ... step 32/59  loss=0.6566  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.214e+00 (batch 32)
  ... step 34/59  loss=0.4597  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.819e+00 (batch 34)
  ... step 36/59  loss=0.5663  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.414e+00 (batch 36)
  ... step 38/59  loss=0.7780  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.650e+00 (batch 38)
  ... step 40/59  loss=0.6686  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.880e+00 (batch 40)
  ... step 42/59  loss=1.2314  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.034e+00 (batch 42)
  ... step 44/59  loss=0.5390  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.117e+00 (batch 44)
  ... step 46/59  loss=0.6123  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.241e+00 (batch 46)
  ... step 48/59  loss=0.8499  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.858e+00 (batch 48)
  ... step 50/59  loss=0.8318  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.656e+00 (batch 50)
  ... step 52/59  loss=0.8694  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.706e+00 (batch 52)
  ... step 54/59  loss=0.4079  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.422e+00 (batch 54)
  ... step 56/59  loss=0.6363  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.041e+00 (batch 56)
  ... step 58/59  loss=1.0641  (1.3s since last print)
  â†˜ grad L2 norm â‰ˆ 9.221e+00 (batch 58)
âœ… epoch 3 forward/backward done in 66.5s
  ðŸ”Ž val step 0: batch (8, 64, 2, 324, 1, 16) loss=7.2136
[03] train=0.6590  val=5.9969  RMSE(std)=[Qi:2.573, Qe:2.489, Î“:2.275]  RMSE(phys)=[Qi:208.928, Qe:260.038, Î“:112.551]  (4.2s)
âœ… Saved unified NN â†’ ./mnt/data/bin.cgyro.nn
ðŸŸ¦ Starting epoch 4/40 (train steps â‰ˆ 59)
  ... step 0/59  loss=1.0385  (0.3s since last print)
  â†˜ grad L2 norm â‰ˆ 7.919e+00 (batch 0)
  ... step 2/59  loss=0.5452  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.405e+00 (batch 2)
  ... step 4/59  loss=0.8533  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.498e+00 (batch 4)
  ... step 6/59  loss=0.9120  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.682e+00 (batch 6)
  ... step 8/59  loss=0.3614  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.441e+00 (batch 8)
  ... step 10/59  loss=0.2023  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 1.983e+00 (batch 10)
  ... step 12/59  loss=0.4071  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.596e+00 (batch 12)
  ... step 14/59  loss=0.5538  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.579e+00 (batch 14)
  ... step 16/59  loss=0.2810  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.284e+00 (batch 16)
  ... step 18/59  loss=0.2731  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.121e+00 (batch 18)
  ... step 20/59  loss=0.3874  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.003e+00 (batch 20)
  ... step 22/59  loss=0.6697  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.747e+00 (batch 22)
  ... step 24/59  loss=0.4156  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.044e+00 (batch 24)
  ... step 26/59  loss=0.6639  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.831e+00 (batch 26)
  ... step 28/59  loss=0.2889  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.094e+00 (batch 28)
  ... step 30/59  loss=0.3501  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.659e+00 (batch 30)
  ... step 32/59  loss=0.7846  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.658e+00 (batch 32)
  ... step 34/59  loss=0.8518  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.326e+00 (batch 34)
  ... step 36/59  loss=1.0674  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.570e+00 (batch 36)
  ... step 38/59  loss=0.5876  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.056e+00 (batch 38)
  ... step 40/59  loss=0.3177  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.457e+00 (batch 40)
  ... step 42/59  loss=0.6080  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.526e+00 (batch 42)
  ... step 44/59  loss=0.4674  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.844e+00 (batch 44)
  ... step 46/59  loss=0.6346  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.057e+00 (batch 46)
  ... step 48/59  loss=0.4715  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.136e+00 (batch 48)
  ... step 50/59  loss=0.4713  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.266e+00 (batch 50)
  ... step 52/59  loss=0.5288  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.045e+00 (batch 52)
  ... step 54/59  loss=0.5031  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.991e+00 (batch 54)
  ... step 56/59  loss=0.7036  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.276e+00 (batch 56)
  ... step 58/59  loss=0.7612  (1.3s since last print)
  â†˜ grad L2 norm â‰ˆ 1.034e+01 (batch 58)
âœ… epoch 4 forward/backward done in 66.5s
  ðŸ”Ž val step 0: batch (8, 64, 2, 324, 1, 16) loss=7.6924
[04] train=0.5778  val=6.5128  RMSE(std)=[Qi:2.671, Qe:2.617, Î“:2.357]  RMSE(phys)=[Qi:216.875, Qe:273.359, Î“:116.646]  (4.2s)
âœ… Saved unified NN â†’ ./mnt/data/bin.cgyro.nn
ðŸŸ¦ Starting epoch 5/40 (train steps â‰ˆ 59)
  ... step 0/59  loss=0.3717  (0.3s since last print)
  â†˜ grad L2 norm â‰ˆ 3.207e+00 (batch 0)
  ... step 2/59  loss=0.3356  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.507e+00 (batch 2)
  ... step 4/59  loss=0.5707  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.552e+00 (batch 4)
  ... step 6/59  loss=0.6783  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.614e+00 (batch 6)
  ... step 8/59  loss=0.4870  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.927e+00 (batch 8)
  ... step 10/59  loss=0.6410  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.026e+00 (batch 10)
  ... step 12/59  loss=0.3809  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.055e+00 (batch 12)
  ... step 14/59  loss=0.5749  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.003e+00 (batch 14)
  ... step 16/59  loss=1.1705  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.155e+00 (batch 16)
  ... step 18/59  loss=0.6058  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.188e+00 (batch 18)
  ... step 20/59  loss=0.7762  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.775e+00 (batch 20)
  ... step 22/59  loss=0.4979  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.430e+00 (batch 22)
  ... step 24/59  loss=0.2880  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.122e+00 (batch 24)
  ... step 26/59  loss=0.2769  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.018e+00 (batch 26)
  ... step 28/59  loss=0.3753  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.011e+00 (batch 28)
  ... step 30/59  loss=0.6524  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.645e+00 (batch 30)
  ... step 32/59  loss=0.3847  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.222e+00 (batch 32)
  ... step 34/59  loss=0.3683  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.282e+00 (batch 34)
  ... step 36/59  loss=0.3409  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.975e+00 (batch 36)
  ... step 38/59  loss=0.3809  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.620e+00 (batch 38)
  ... step 40/59  loss=0.8605  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.134e+00 (batch 40)
  ... step 42/59  loss=1.1671  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 7.253e+00 (batch 42)
  ... step 44/59  loss=0.4855  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.143e+00 (batch 44)
  ... step 46/59  loss=0.3624  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.839e+00 (batch 46)
  ... step 48/59  loss=0.3749  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.215e+00 (batch 48)
  ... step 50/59  loss=0.3777  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.847e+00 (batch 50)
  ... step 52/59  loss=0.5474  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.040e+00 (batch 52)
  ... step 54/59  loss=0.4655  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.990e+00 (batch 54)
  ... step 56/59  loss=0.4560  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.242e+00 (batch 56)
  ... step 58/59  loss=0.6455  (1.3s since last print)
  â†˜ grad L2 norm â‰ˆ 4.817e+00 (batch 58)
âœ… epoch 5 forward/backward done in 66.5s
  ðŸ”Ž val step 0: batch (8, 64, 2, 324, 1, 16) loss=12.1643
[05] train=0.5464  val=10.4935  RMSE(std)=[Qi:3.442, Qe:3.155, Î“:3.111]  RMSE(phys)=[Qi:279.518, Qe:329.523, Î“:153.952]  (4.2s)
âœ… Saved unified NN â†’ ./mnt/data/bin.cgyro.nn
ðŸŸ¦ Starting epoch 6/40 (train steps â‰ˆ 59)
  ... step 0/59  loss=0.2061  (0.3s since last print)
  â†˜ grad L2 norm â‰ˆ 2.276e+00 (batch 0)
  ... step 2/59  loss=0.6836  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.087e+00 (batch 2)
  ... step 4/59  loss=0.3421  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.742e+00 (batch 4)
  ... step 6/59  loss=0.2383  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.422e+00 (batch 6)
  ... step 8/59  loss=0.3266  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.192e+00 (batch 8)
  ... step 10/59  loss=0.3399  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.676e+00 (batch 10)
  ... step 12/59  loss=0.6269  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.245e+00 (batch 12)
  ... step 14/59  loss=0.5145  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.669e+00 (batch 14)
  ... step 16/59  loss=0.5419  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.108e+00 (batch 16)
  ... step 18/59  loss=0.3056  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.731e+00 (batch 18)
  ... step 20/59  loss=0.3699  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.368e+00 (batch 20)
  ... step 22/59  loss=0.9322  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.949e+00 (batch 22)
  ... step 24/59  loss=0.2918  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.444e+00 (batch 24)
  ... step 26/59  loss=0.7236  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.161e+00 (batch 26)
  ... step 28/59  loss=0.7647  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 8.040e+00 (batch 28)
  ... step 30/59  loss=0.3059  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.566e+00 (batch 30)
  ... step 32/59  loss=0.2066  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.403e+00 (batch 32)
  ... step 34/59  loss=0.7927  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.902e+00 (batch 34)
  ... step 36/59  loss=0.2167  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.742e+00 (batch 36)
  ... step 38/59  loss=0.5453  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.924e+00 (batch 38)
  ... step 40/59  loss=0.6980  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.775e+00 (batch 40)
  ... step 42/59  loss=0.6214  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.420e+00 (batch 42)
  ... step 44/59  loss=1.2665  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.900e+00 (batch 44)
  ... step 46/59  loss=0.3766  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.485e+00 (batch 46)
  ... step 48/59  loss=0.4451  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.373e+00 (batch 48)
  ... step 50/59  loss=0.3729  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.759e+00 (batch 50)
  ... step 52/59  loss=0.6664  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.919e+00 (batch 52)
  ... step 54/59  loss=0.6167  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.960e+00 (batch 54)
  ... step 56/59  loss=0.5848  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.478e+00 (batch 56)
  ... step 58/59  loss=0.3526  (1.3s since last print)
  â†˜ grad L2 norm â‰ˆ 3.047e+00 (batch 58)
âœ… epoch 6 forward/backward done in 66.5s
  ðŸ”Ž val step 0: batch (8, 64, 2, 324, 1, 16) loss=5.0491
[06] train=0.5323  val=4.0245  RMSE(std)=[Qi:2.127, Qe:1.998, Î“:1.886]  RMSE(phys)=[Qi:172.761, Qe:208.669, Î“:93.317]  (4.2s)
âœ… Saved unified NN â†’ ./mnt/data/bin.cgyro.nn
ðŸŸ¦ Starting epoch 7/40 (train steps â‰ˆ 59)
  ... step 0/59  loss=0.7741  (0.3s since last print)
  â†˜ grad L2 norm â‰ˆ 3.539e+00 (batch 0)
  ... step 2/59  loss=0.6143  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.080e+00 (batch 2)
  ... step 4/59  loss=0.3348  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.934e+00 (batch 4)
  ... step 6/59  loss=0.6431  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.706e+00 (batch 6)
  ... step 8/59  loss=0.3797  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.443e+00 (batch 8)
  ... step 10/59  loss=0.3285  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.420e+00 (batch 10)
  ... step 12/59  loss=0.3261  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.412e+00 (batch 12)
  ... step 14/59  loss=0.6441  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.320e+00 (batch 14)
  ... step 16/59  loss=0.2631  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.258e+00 (batch 16)
  ... step 18/59  loss=0.4095  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.257e+00 (batch 18)
  ... step 20/59  loss=0.4731  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.100e+00 (batch 20)
  ... step 22/59  loss=0.8702  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.678e+00 (batch 22)
  ... step 24/59  loss=1.2380  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 6.628e+00 (batch 24)
  ... step 26/59  loss=0.2616  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.439e+00 (batch 26)
  ... step 28/59  loss=0.5776  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.253e+00 (batch 28)
  ... step 30/59  loss=0.9727  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.161e+00 (batch 30)
  ... step 32/59  loss=0.3267  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.748e+00 (batch 32)
  ... step 34/59  loss=0.5393  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.769e+00 (batch 34)
  ... step 36/59  loss=0.4919  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.382e+00 (batch 36)
  ... step 38/59  loss=0.8908  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 5.147e+00 (batch 38)
  ... step 40/59  loss=0.2200  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.360e+00 (batch 40)
  ... step 42/59  loss=0.5184  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.052e+00 (batch 42)
  ... step 44/59  loss=0.2322  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.398e+00 (batch 44)
  ... step 46/59  loss=0.6737  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 4.136e+00 (batch 46)
  ... step 48/59  loss=0.4512  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.276e+00 (batch 48)
  ... step 50/59  loss=0.6356  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.113e+00 (batch 50)
  ... step 52/59  loss=0.4905  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.418e+00 (batch 52)
  ... step 54/59  loss=0.3691  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 2.783e+00 (batch 54)
  ... step 56/59  loss=0.5520  (1.4s since last print)
  â†˜ grad L2 norm â‰ˆ 3.843e+00 (batch 56)
  ... step 58/59  loss=0.2187  (1.3s since last print)
  â†˜ grad L2 norm â‰ˆ 2.835e+00 (batch 58)
âœ… epoch 7 forward/backward done in 66.5s
  ðŸ”Ž val step 0: batch (8, 64, 2, 324, 1, 16) loss=5.8147
